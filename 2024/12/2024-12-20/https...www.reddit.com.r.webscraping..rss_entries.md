# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Passive scraping with custom web extension
 - [https://www.reddit.com/r/webscraping/comments/1hivxwo/passive_scraping_with_custom_web_extension](https://www.reddit.com/r/webscraping/comments/1hivxwo/passive_scraping_with_custom_web_extension)
 - RSS feed: $source
 - date published: 2024-12-20T22:49:19+00:00

<!-- SC_OFF --><div class="md"><p>I have some questions about architecture and available tooling.</p> <p>I previously wrote a web extension to extract information from a site into an external database I could query. I actually built a nextjs app with shadcn components so I could have a nice UI. It&#39;s currently a separate application, but I&#39;m looking into combining it with the extension, so it can run in the browser.</p> <p>I am not trying to scrape the whole site, more like archive a copy of the data I&#39;ve come across so far. My thinking is that by lifting data off the page I&#39;m browsing, or repeating API calls to retrieve data from the cache, I won&#39;t raise any red flags. I am also thinking of a paradigm where other people install the extension and everyone sends scraped data to a shared repository, for a more complete collection that is updated organically.</p> <p>The extension can do things like highlight pages that I already have saved, or enhance pages with addit

## HELP I AM LOSING MY MIND
 - [https://www.reddit.com/r/webscraping/comments/1hippr1/help_i_am_losing_my_mind](https://www.reddit.com/r/webscraping/comments/1hippr1/help_i_am_losing_my_mind)
 - RSS feed: $source
 - date published: 2024-12-20T18:03:40+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1hippr1/help_i_am_losing_my_mind/"> <img src="https://external-preview.redd.it/yTaTwvlvZdUepgWCkzrH0n-4fuf0fHEb9-x0iYVEaE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=18463f5e9e74ff1c0c13ef375323ad6ac8b36956" alt="HELP I AM LOSING MY MIND" title="HELP I AM LOSING MY MIND" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>I am scraping this website to try and go througgh each job page and extract info:</p> <p><a href="https://wuzzuf.net/jobs/p/6eXds09F3XuO-Sr-Presales-engineer-Light-Current-Itechs-Group-Cairo-Egypt?o=1&amp;l=bp&amp;t=bj&amp;bpv=np&amp;a=IT-Software-Development-Jobs-in-Egypt">https://wuzzuf.net/jobs/p/6eXds09F3XuO-Sr-Presales-engineer-Light-Current-Itechs-Group-Cairo-Egypt?o=1&amp;l=bp&amp;t=bj&amp;bpv=np&amp;a=IT-Software-Development-Jobs-in-Egypt</a></p> <p>now I am not able to scrape anything from the job details and skills and tools sections. </p> <p>I tried selecting the element in mult

## Zillow Scraping Blocked
 - [https://www.reddit.com/r/webscraping/comments/1hin9vs/zillow_scraping_blocked](https://www.reddit.com/r/webscraping/comments/1hin9vs/zillow_scraping_blocked)
 - RSS feed: $source
 - date published: 2024-12-20T16:16:42+00:00

<!-- SC_OFF --><div class="md"><p>So I’ve been scraping zillow through a proxy api service, which I won’t mention the name since for some reason my last post got taken down for promotion? Anyways, it was working fine for a couple of weeks(I didn’t do a lot I was just testing around small amounts), but now it is failing for every request. After some testing I found out that now Zillow is requiring all headers to be present when making a request to this specific page. </p> <p>I’m worried that I might get in trouble? I don’t think there is anything identifying though since I was using that proxy which was me making the request to the proxy api which then made the request to the actual url. </p> <p>I’m also able to google the url fine and I can even make a request to it through python requests if I include all headers. Before it was able to just do a request with no headers…</p> <p>UPDATE: so it actually seems as though I just need to include a user agent header. Is there any generic one

## Is a Twltter API rotating proxy legal or suitable for a MSc Thesis?
 - [https://www.reddit.com/r/webscraping/comments/1hig3c9/is_a_twltter_api_rotating_proxy_legal_or_suitable](https://www.reddit.com/r/webscraping/comments/1hig3c9/is_a_twltter_api_rotating_proxy_legal_or_suitable)
 - RSS feed: $source
 - date published: 2024-12-20T09:21:37+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m quite new to data scraping and I wanted to get some data from twltter using an API but just now it crossed my mind that using a rotating proxy might be problematic as a way of scraping data for an MSc thesis. Do you know if it&#39;s legitimate to use it? And if not, is there any other way of getting the data without the extremelly low ammount of tweets allowed by twltter? I also thought about making a form asking for permission for friends and family to use their accounts to scrape data and enchance my browsing by that ammount (I don&#39;t know if that would make it legit), any help would be appreciated, thanks :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/jsandi99"> /u/jsandi99 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1hig3c9/is_a_twltter_api_rotating_proxy_legal_or_suitable/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1hig3c9

## Scraping Speed?? Goodreads
 - [https://www.reddit.com/r/webscraping/comments/1hiaca6/scraping_speed_goodreads](https://www.reddit.com/r/webscraping/comments/1hiaca6/scraping_speed_goodreads)
 - RSS feed: $source
 - date published: 2024-12-20T03:02:22+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I am working on an AI book recommender and am scraping data from GoodReads. Anything I should do so I don&#39;t get kick off? I am waiting 2 seconds between every retrieval, but that is adding a lot of time. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/PurpleMermaid16"> /u/PurpleMermaid16 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1hiaca6/scraping_speed_goodreads/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1hiaca6/scraping_speed_goodreads/">[comments]</a></span>

## Queue-it problem
 - [https://www.reddit.com/r/webscraping/comments/1hia76m/queueit_problem](https://www.reddit.com/r/webscraping/comments/1hia76m/queueit_problem)
 - RSS feed: $source
 - date published: 2024-12-20T02:54:44+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1hia76m/queueit_problem/"> <img src="https://b.thumbs.redditmedia.com/bQcYj1PDTIP8cF6qalejpd5W4MFHyQ4cbCB_9IEUFRg.jpg" alt="Queue-it problem" title="Queue-it problem" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hello, currently I&#39;m working with axios in a &quot;queue-it&quot; project.</p> <p>The problem I&#39;m facing is with the proofofwork &quot;captcha&quot; that is requiered.</p> <p>Example: I&#39;m testing with <a href="https://footlocker.queue-it.net/?c=footlocker&amp;e=cxcdtest02">https://footlocker.queue-it.net/?c=footlocker&amp;e=cxcdtest02</a> and in the network of google chrome I see that the url make a POST request to &quot;<a href="https://footlocker.queue-it.net/challengeapi/pow/challenge/6b058235-0cb7-4ed0-b9b8-138cfa0dfd24">https://footlocker.queue-it.net/challengeapi/pow/challenge/6b058235-0cb7-4ed0-b9b8-138cfa0dfd24</a>&quot; for example which gives the &quot;challenge&quot; which is

