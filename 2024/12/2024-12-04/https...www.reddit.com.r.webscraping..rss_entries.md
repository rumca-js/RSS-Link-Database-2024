# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Scraping using undetected_chromedriver on rdp
 - [https://www.reddit.com/r/webscraping/comments/1h6sssl/scraping_using_undetected_chromedriver_on_rdp](https://www.reddit.com/r/webscraping/comments/1h6sssl/scraping_using_undetected_chromedriver_on_rdp)
 - RSS feed: $source
 - date published: 2024-12-04T22:12:18+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone.<br/> Recently wrote a small scraper that automates my routine tasks. I am using undetected_chromedriver with pyautogui for mouse movement and lots of other stuff.<br/> In order to run this script continuously i am using RDP windows server machine. There is a catch, whenever i log off from rdp it breaks with message bellow. Other parts of script run fine. Does anyone have encountered anything similar or maybe could suggest other approaches to that problem?<br/> HTTPConnectionPool(host=&#39;localhost&#39;, port=50538): Max retries exceeded with url: /session/fa85f499fbe802b9f1445e520ab31ca1/elements (</p> <p>Caused by NewConnectionError(&#39;&lt;urllib3.connection.HTTPConnection object at 0x00000206AB7E56D0&gt;: Failed to establish a new connection: [WinError 10061] </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/PreviousAfternoon544"> /u/PreviousAfternoon544 </a> <br/> <span><a href="https://www.re

## Keyword Alerts from FB Groups
 - [https://www.reddit.com/r/webscraping/comments/1h6ltgq/keyword_alerts_from_fb_groups](https://www.reddit.com/r/webscraping/comments/1h6ltgq/keyword_alerts_from_fb_groups)
 - RSS feed: $source
 - date published: 2024-12-04T17:31:51+00:00

<!-- SC_OFF --><div class="md"><p>New to all this so not sure I&#39;m even asking in the right place, but is there a scraping tool that I can set up to search various groups for certain keywords and alert me as soon as a post or comment with those keywords is posted?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Blaze_07"> /u/Blaze_07 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h6ltgq/keyword_alerts_from_fb_groups/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h6ltgq/keyword_alerts_from_fb_groups/">[comments]</a></span>

## Webscraping academic websites
 - [https://www.reddit.com/r/webscraping/comments/1h6f499/webscraping_academic_websites](https://www.reddit.com/r/webscraping/comments/1h6f499/webscraping_academic_websites)
 - RSS feed: $source
 - date published: 2024-12-04T12:44:40+00:00

<!-- SC_OFF --><div class="md"><p>I am responsible for creating email campaigns for an academic conference that my company is hosting. I want some website suggestions, the topic is Urban Physics and Environmental Resilience.</p> <p>I&#39;ve been using Science Direct for the most part as well as university websites but I&#39;m wondering is there is somewhere else I can look.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Firm-Atmosphere7085"> /u/Firm-Atmosphere7085 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h6f499/webscraping_academic_websites/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h6f499/webscraping_academic_websites/">[comments]</a></span>

## Strategy for large-scale scraping and dual data saving
 - [https://www.reddit.com/r/webscraping/comments/1h6d0l1/strategy_for_largescale_scraping_and_dual_data](https://www.reddit.com/r/webscraping/comments/1h6d0l1/strategy_for_largescale_scraping_and_dual_data)
 - RSS feed: $source
 - date published: 2024-12-04T10:29:49+00:00

<!-- SC_OFF --><div class="md"><p>Hi Everyone,</p> <p>One of my ongoing webscraping projects is based on Crawlee and Playwright and scrapes millions of pages and extracts tens of millions of data points. The current scraping portion of the script works fine, but I need to modify it to include programmatic dual saving of the scraped data. Iâ€™ve been scraping to JSON files so far, but dealing with millions of files is slow and inefficient to say the least. I want to add direct database saving while still at the same time saving and keeping JSON backups for redundancy. Since I need to rescrape one of the main sites soon due to new selector logic, this felt like the right time to scale and optimize for future updates.</p> <p>The project requires frequent rescraping (e.g., weekly) and the database will overwrite outdated data. The final data will be uploaded to a separate site that supports JSON or CSV imports. My server specs include 96 GB RAM and an 8-core CPU. My primary goals are relia

