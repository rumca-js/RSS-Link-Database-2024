[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T22:52:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there.</p> <p>I created a python script using playwright that scrapes a site just fine using my own IP. I then signed up to a premium service to get access to tonnes of residential proxies. However when I use these proxies (I use the rotating ones) they keep meeting the cloudflare bot detection page when I try to scrape the same url.</p> <p>I have tried different configurations from the service but all of them hit the cloudflare bot detection page.</p> <p>What am I doing wrong? Are all purchased proxies like this?</p> <p>I&#39;m using playwright with playwright stealth too. I&#39;m using a headless browser but even setting headless=false shows cloudflare.</p> <p>It makes me think that cloudflare could just sign up to these premium proxy services, find out all the IPs and then block them. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LordOfTheDips\"> /u/LordOfTheDips </a> <br/> <span><a href=\"https://www.red",
        "id": 1684640,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hbeqz2/premium_proxies_keep_getting_caught_by_cloudflare",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Premium proxies keep getting caught by cloudflare",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T22:17:29+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MeetYourCreator\"> /u/MeetYourCreator </a> <br/> <span><a href=\"https://www.hyperbrowser.ai/blog/nodriver_:_The_next_step_in_web_automation\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hbdyl6/nodriver_the_next_step_in_web_scraping/\">[comments]</a></span>",
        "id": 1684195,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hbdyl6/nodriver_the_next_step_in_web_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Nodriver - The next step in web scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T21:02:14+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Does anyone know of a good tool to scrape builtwith.com? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fundingsecurer\"> /u/fundingsecurer </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hbc7ds/builtwithcom_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hbc7ds/builtwithcom_scraping/\">[comments]</a></span>",
        "id": 1684641,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hbc7ds/builtwithcom_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Builtwith.com Scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T20:05:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there!</p> <p>I am making a python project with a code that will authenticate to some application, and then scrape data while being logged in. The thing is that every user that will use my project will create separate session on my server, so session should be really lightweight like around 5mb or even fewer. </p> <p>Right now I am using selenium as a webscraping tool, but it consumes too much ram on my server (around 20mb per session using headless mode). </p> <p>Are there any other webscraping tools that would be even less ram consuming? Heard about playwright and requests, but I think requests can\u2019t handle javascript and such things that I do. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Excellent-Product230\"> /u/Excellent-Product230 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hbauh3/the_lightest_tool_for_webscraping/\">[link]</a></span> &#32; <span><a href=\"https://www.redd",
        "id": 1684642,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hbauh3/the_lightest_tool_for_webscraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The lightest tool for webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T20:01:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1hbaqwb/font_scraping/\"> <img src=\"https://b.thumbs.redditmedia.com/Fs2gcWVQqtds7QQC5EaREmCSwIEPwaEpUPywDpxx04I.jpg\" alt=\"Font scraping\" title=\"Font scraping\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/bq00htphu26e1.png?width=1197&amp;format=png&amp;auto=webp&amp;s=d8bff77d83e78274cda65a7f5d69946500b30960\">https://preview.redd.it/bq00htphu26e1.png?width=1197&amp;format=png&amp;auto=webp&amp;s=d8bff77d83e78274cda65a7f5d69946500b30960</a></p> <p>Hi, I was trying to scrape web fonts. I found this network request of type font/opentype.</p> <p>I downloaded it and called it &quot;test.otf&quot; but it doesn&#39;t get recognized as a font. I also tried changing the format and using various online converters, but nothing...</p> <p>It&#39;s a binary file so no chances of understanding the content. If any of you have any idea, here&#39;s the link to the file <a href=\"https://files.",
        "id": 1683434,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hbaqwb/font_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Fs2gcWVQqtds7QQC5EaREmCSwIEPwaEpUPywDpxx04I.jpg",
        "title": "Font scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T17:54:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys ! Had a question I needed help with. Was chatting with someone today about their idea for a project to aggregate happy hours locally.</p> <p>They would want to be able to get some data on them so that maybe they can query them by day or by drink type etc </p> <p>My initial thought was to find an app that already did this, or some kind of user aggregated data that is up to date on a site like Yelp or Reddit </p> <p>My second thought would be I&#39;d have to go to Google maps and search for bars near [city] , visit each website, then visit each link within each page internally and search for happy hour or drink special and hope there&#39;s a time and date listed and drink types </p> <p>Does anyone have any thoughts for how I might go about this ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/0sergio-hash\"> /u/0sergio-hash </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hb7plz/lo",
        "id": 1682398,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hb7plz/local_happy_hour_scraping",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Local happy hour scraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T12:21:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey fellow data enthusiasts!I&#39;ve developed a robust public data collection engine that&#39;s been quietly amassing an impressive dataset, and I&#39;m curious about its potential applications and market demand. Here&#39;s what I&#39;m working with:</p> <h1>The Dataset</h1> <ul> <li><strong>Scale</strong>: Over 2 billion data points, with 10 million added per day</li> <li><strong>Sources</strong>: Diverse and challenging public social media sources (X, Reddit, BlueSky, <a href=\"http://YT.com\">YT.com</a>, etc.) All of it really</li> <li><strong>Collection</strong>: Near real-time capture</li> <li><strong>Rich</strong>: Structured, annotated</li> <li><strong>Content</strong>: Strictly public data, no private or user information</li> </ul> <h1>Potential Applications</h1> <p>I initially envisioned this as a goldmine for NLP tasks, offering insights into:</p> <ul> <li>Emerging trends</li> <li>Public discussions</li> <li>Topic evolution</li> <li>Sentimen",
        "id": 1680076,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hb0j22/is_there_a_market_for_realtimepublic_data_really",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is there a market for realtime/public data, really?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T12:19:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1hb0hml/get_v02_page_links_template_optionals/\"> <img src=\"https://external-preview.redd.it/3uUDdxmTTe5a-jS1vKikBA3SubVqlQBBkDrqUq1a-pQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d2c6cb24f304cdc2ba23ff5349f9d1b5222f8d3\" alt=\"GET v0.2 / Page Links / Template Optionals\" title=\"GET v0.2 / Page Links / Template Optionals\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/matty_fu\"> /u/matty_fu </a> <br/> <span><a href=\"https://getlang.dev/blog/get-v0.2\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hb0hml/get_v02_page_links_template_optionals/\">[comments]</a></span> </td></tr></table>",
        "id": 1680077,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hb0hml/get_v02_page_links_template_optionals",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/3uUDdxmTTe5a-jS1vKikBA3SubVqlQBBkDrqUq1a-pQ.jpg?width=640&crop=smart&auto=webp&s=1d2c6cb24f304cdc2ba23ff5349f9d1b5222f8d3",
        "title": "GET v0.2 / Page Links / Template Optionals",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T09:09:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all </p> <p>I am glad to present the result of my work that allows you to bypass Yandex captcha (Puzzle type): <a href=\"https://github.com/yoori/yandex-captcha-puzzle-solver\">https://github.com/yoori/yandex-captcha-puzzle-solver</a> </p> <p>I will be glad if this helps someone)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/yoori111\"> /u/yoori111 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haxt9o/yandex_captcha_puzzle_free_solver/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haxt9o/yandex_captcha_puzzle_free_solver/\">[comments]</a></span>",
        "id": 1680078,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1haxt9o/yandex_captcha_puzzle_free_solver",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Yandex Captcha (Puzzle) Free Solver",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T08:33:24+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>I just finished creating a small <a href=\"https://github.com/sergioparamo/blog-crawler\">website</a> (not hosted yet) to scrape (limited format yet) blogs. Based on React and Python</p> <p>feel free to leave comments and/or suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/s3ktor_13\"> /u/s3ktor_13 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haxcev/blog_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haxcev/blog_scraper/\">[comments]</a></span>",
        "id": 1679036,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1haxcev/blog_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Blog Scraper",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T08:15:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>We are planning on training an ML model on the screenshots of website GUIs- for a commercial use product.</p> <p>This scraping is fully automated- we have our own AI computer controller that visits sites to get a diverse dataset. We can get millions of images of GUIs.</p> <p>Is it risky or would we be fine with public data?</p> <p>If it is risky, is there any way we can create an automation to determine what&#39;s safe (eg. getting AI to locate the site&#39;s TOS and analyze it, and output the sites which are safe)? What should the AI look for?</p> <p>Thought I&#39;d ask here along to get a general opinion.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ComfortableDivide640\"> /u/ComfortableDivide640 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hax47s/legality_of_getting_images_of_websites_guis_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/",
        "id": 1678809,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hax47s/legality_of_getting_images_of_websites_guis_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Legality of getting images of website's GUIs for ML training",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T05:30:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently took on a project involving web scraping, spent a lot of effort, and was almost done when Upwork closed the contract, citing TOS violations of another website.</p> <p>I know scraping can be tricky legally, but there are so many scraping jobs on Upwork, often from big clients, and established freelancers seem to do fine with it. This contract wasn\u2019t even solely about scraping, it was just part of the work.</p> <p>How do you handle this? Do you avoid scraping jobs on Upwork altogether, or is there a better way to approach them?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Nearby_Category2596\"> /u/Nearby_Category2596 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haupq4/scraping_jobs_on_upwork/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haupq4/scraping_jobs_on_upwork/\">[comments]</a></span>",
        "id": 1678601,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1haupq4/scraping_jobs_on_upwork",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping Jobs on Upwork",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T00:32:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey,</p> <p>I was working on simple scraper past few days, and now it&#39;s time to scrape all offers. I never got in to 429 or anything, scraper is not as fast as it could be, but i can wait few days to finish everything (it does not matter, and will run once). However I tried: Hetzner (ips blocked, cloudfront), Contabo (slow asf, and losing connection - losing offers, would take a month after some calculations xdd). I know i could use RPI, but would like to try cloud first. Any advice?</p> <p>Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Spare-Repeat-8820\"> /u/Spare-Repeat-8820 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hap24i/vps_to_keep_scraper_alive/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hap24i/vps_to_keep_scraper_alive/\">[comments]</a></span>",
        "id": 1677971,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hap24i/vps_to_keep_scraper_alive",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "VPS to keep scraper alive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-10T00:18:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>I just finished creating a small <a href=\"https://github.com/sergioparamo/blog-crawler\">website</a> (not hosted yet) to scrape (limited format yet) blogs. Based on React and Python</p> <p>feel free to leave comments and/or suggestions.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/s3ktor_13\"> /u/s3ktor_13 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haor7u/blog_scraper/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1haor7u/blog_scraper/\">[comments]</a></span>",
        "id": 1677292,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1haor7u/blog_scraper",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Blog Scraper",
        "vote": 0
    }
]