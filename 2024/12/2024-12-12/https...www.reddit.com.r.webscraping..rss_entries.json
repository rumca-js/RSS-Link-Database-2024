[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T20:44:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve to build a scraper that scraps 10 millions request per day, I have to keep project low budget, can afford like 50 to 100 USD a month for hosting. Is it duable?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Resiakvrases\"> /u/Resiakvrases </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcuuws/to_scrape_10_millions_requests_per_day/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcuuws/to_scrape_10_millions_requests_per_day/\">[comments]</a></span>",
        "id": 1699319,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcuuws/to_scrape_10_millions_requests_per_day",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "To scrape 10 millions requests per day",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T20:02:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, </p> <p>I&#39;m getting started here for a friend. Basically, we want to extract her book collection from SensCritique to import it on Goodreads. We already have all the book information, but Senscritique doesn&#39;t give out the ISBN when asked for personal data (we have asked).</p> <p>From what I&#39;ve seen, other script exist for SensCritique but it&#39;s mainly for getting movies to Letterbox which doesn&#39;t really help us. Other script to get SensCritique books to Goodreads seems outdated.</p> <p>So, because we already have everything except the ISBN, we&#39;d like to get each of the ISBN from her book list into a cvs so we can match them after :</p> <ul> <li>Scrape the book list from her collection (<a href=\"https://www.senscritique.com/spif/collection?universe=2\">https://www.senscritique.com/spif/collection?universe=2</a>) random collection used here</li> <li>Go to the detail page of each book (I figured that if you use the ",
        "id": 1698947,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hctwq8/extract_isbn_from_book_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Extract ISBN from book collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T18:28:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m facing what&#39;s for me a HUGE problem. I was given the task of scraping certain data from a website with recaptcha v3 on it. No problem until here, i was using 2captcha to solve the token and then inject it on the site and i thought the problem was solved. What happens now it is that this recaptcha it is not in charge of &quot;hiding&quot; the info to users as i just checked but something else i can&#39;t decipher.</p> <p>The flow: Go into the website, click the &quot;Busqueda Avanzada&quot; and insert a number on the &quot;<strong>Identificador Convocatoria</strong>&quot; input, then click on search. </p> <p>What this should do is to return an answer on the table below with some information. When i run this little shit on Production no matter what i search on the site it always returns &quot;No se encontraron resultados&quot; (no results at all) and i can&#39;t do shit. But, locally, it works flawlessly even if i do not resolve the recaptc",
        "id": 1698484,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcrpln/is_the_website_tricky_or_am_i_ignorant",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the website tricky or am I ignorant?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T13:05:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everybody. We open-sourced <a href=\"https://github.com/getmaxun/maxun\">Maxun</a> last month. We have decided to post our new features here, just in case someone needs it.<br/> Now, it is possible to scrape data behind logins with username and password. We are actively working on cookie sessions for the same - and will roll it out this week.</p> <p>Here is an example video: <a href=\"https://www.youtube.com/watch?v=_-n6OFQI_x4\">https://www.youtube.com/watch?v=_-n6OFQI_x4</a></p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/carishmaa\"> /u/carishmaa </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcko2s/opensource_nocode_web_data_extraction_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcko2s/opensource_nocode_web_data_extraction_with/\">[comments]</a></span>",
        "id": 1696047,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcko2s/opensource_nocode_web_data_extraction_with",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Open-Source No-Code Web Data Extraction With Support For Logins!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T11:49:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am scraping a website using R, not Python, as I do not have much experience with Python. Whenever I start scraping, the website blocks my attempts. After some research, I found two potential solutions: purchasing IPs to use IP rotation or using AWS to change the IP address. I chose the second option, and I learned how to change the IP address from a YouTube video <a href=\"https://www.youtube.com/watch?v=8BbOVhHs950&amp;t=786s\">Change the IP address every time you run a scraper for FREE</a>.</p> <p>However, most examples and tutorials use Python. Can we use <a href=\"/r/RStudio\">r/RStudio</a> in AWS to change the IP address after each run of the R code? I think it might be difficult to use R in an AWS Lambda function.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/No_Painting6076\"> /u/No_Painting6076 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcjeid/scraping_with_r_using_aws_to_cha",
        "id": 1698948,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcjeid/scraping_with_r_using_aws_to_change_ip_address",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scraping with R: Using AWS to Change IP Address After Each Run",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T06:56:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I&#39;ve recently seen a few posts about Cloudflare Turnstile and I&#39;m a bit confused. Could someone explain it to me like I&#39;m 5?</p> <ul> <li>How do you know if a website is protected by Cloudflare Turnstile or similar mechanisms?</li> <li>What does it mean when people talk about bypassing Cloudflare Turnstile?</li> <li>If I wanted to learn or research more about how Cloudflare works, where would be a good place to start?</li> </ul> <p>Thanks for any help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LocalConversation850\"> /u/LocalConversation850 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcffp9/discussion_cloudflare_turnstile/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcffp9/discussion_cloudflare_turnstile/\">[comments]</a></span>",
        "id": 1694821,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcffp9/discussion_cloudflare_turnstile",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Discussion: Cloudflare Turnstile",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T06:11:14+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1hcescp/amazon_scraping_beyond_page_7/\"> <img src=\"https://preview.redd.it/l0loxyoa0d6e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e3caeffc9f58e6f5e2ef74c2b1e6ae19cc8d060\" alt=\"Amazon Scraping Beyond Page 7\" title=\"Amazon Scraping Beyond Page 7\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Amazon India limits the search results to 7 pages only. But there are more than 40,000 products listed in the category. To maximize the number of scraped products data I use different combinations of the pricing filter and other filters available to get all the different ASINs (Amazon&#39;s unique ID for each product). So, it&#39;s like performing 200 different search queries to scrape 40,000 products. I want to know what are other ways that one can use to scrape Amazon at scale? Is this the most efficient approach for covering the range of products, or are there better options?</p> </div><!-- SC_ON --> &#32; sub",
        "id": 1694822,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcescp/amazon_scraping_beyond_page_7",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/l0loxyoa0d6e1.png?width=640&crop=smart&auto=webp&s=2e3caeffc9f58e6f5e2ef74c2b1e6ae19cc8d060",
        "title": "Amazon Scraping Beyond Page 7",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T02:50:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I have a marketplace with different sellers who are businesses. I wanted to somehow import their Google Reviews into each of their seller profiles. Is there a way to do this? My marketplace is built on a backend platfrom by Tangram and Webflow.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mundane-Fold-2017\"> /u/Mundane-Fold-2017 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcbdbz/adding_google_reviews_to_seller_profiles_on_my/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hcbdbz/adding_google_reviews_to_seller_profiles_on_my/\">[comments]</a></span>",
        "id": 1693321,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcbdbz/adding_google_reviews_to_seller_profiles_on_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Adding Google Reviews to seller profiles on my Marketplace app",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T02:30:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a huge Grateful Dead fan and want to create a spreadsheet for tracking my own listening of live show recordings. One of the greatest resources out there is <a href=\"https://jerrybase.com/\">Jerrybase</a>, which lists information about every known show Jerry Garcia ever played. </p> <p>On the site, if you go to a year (<a href=\"https://jerrybase.com/events?year=1977\">ex. 1977</a>) you get a list of shows from that year. If you click on an individual show, such as <a href=\"https://jerrybase.com/events/19770319-01\">this one</a>, you get a ton of information including the setlist, notes, links to torrent downloads, and more. </p> <p>Of course I could go through show by show and copy/paste all of this data into my spreadsheet, but there are literally thousands of individual shows. </p> <p>Is there a way to scrape the data from each individual show into a spreadsheet all at once? I don&#39;t know anything about scraping, so any help is appreciated! ",
        "id": 1693322,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hcazhv/help_scraping_concert_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help Scraping Concert Data",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T01:51:56+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1hca8kt/should_i_publish_this_turnstile_bypass_or_make_it/\"> <img src=\"https://external-preview.redd.it/eGV0b3FrMzFxYjZlMbi7e4zKXJMdrXGdn-zlMbYaKv3CveKE2UQs1MTAiCui.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=838ccd3b78ae9d8476b45cce2cde50cfcaf29cdd\" alt=\"Should I publish this turnstile bypass or make it paid? (not browser)\" title=\"Should I publish this turnstile bypass or make it paid? (not browser)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have been programming this Cloudflare turnstile bypass for 1 month. </p> <p>I&#39;m thinking about whether to make it public or paid, because the Cloudflare developers will probably improve their turnstile and patch this. What do you think?</p> <p>I&#39;m almost done with this bypass. If anyone wants to try the unfinished BETA version, here it is: <a href=\"https://github.com/LOBYXLYX/Cloudflare-Bypass\">https://github.com/LOBYXLYX/Cloudflare-Bypass</a></p> </",
        "id": 1693126,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hca8kt/should_i_publish_this_turnstile_bypass_or_make_it",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/eGV0b3FrMzFxYjZlMbi7e4zKXJMdrXGdn-zlMbYaKv3CveKE2UQs1MTAiCui.png?width=640&crop=smart&auto=webp&s=838ccd3b78ae9d8476b45cce2cde50cfcaf29cdd",
        "title": "Should I publish this turnstile bypass or make it paid? (not browser)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_dead_since": null,
        "date_published": "2024-12-12T00:57:39+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I wanna do a real project with Python and webscraping. Doesn&#39;t have to be big or fancy, nor bring me money, I just wanna take it seriously, reason being I wanna learn both python and webscraping.</p> <p>In case you wonder, I do have some serious projects on my bag, like a Unity game, an Api in Dotnet core, flutter and React apps. But with python, and webscraping, it&#39;ll be my first</p> <p>What is one of the easiest, simplest projects on the matter that&#39;ll give me valid experience on these subjects? Maybe not an exact idea, but types of ideas to do. Something that once complete someone can ask me &quot;hey can you work with this&quot; and I can say &quot;yes&quot;?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Blender-Fan\"> /u/Blender-Fan </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1hc95eq/can_ya_suggest_serious_ideas_for_a_portfolio/\">[link]</a></span> &#32; <span><a href",
        "id": 1693125,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1hc95eq/can_ya_suggest_serious_ideas_for_a_portfolio",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can ya suggest serious ideas for a portfolio project?",
        "vote": 0
    }
]