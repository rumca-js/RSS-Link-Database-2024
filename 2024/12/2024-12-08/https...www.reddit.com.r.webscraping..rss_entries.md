# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## How to scape a dictionary without a list of search params/words?
 - [https://www.reddit.com/r/webscraping/comments/1h9tye2/how_to_scape_a_dictionary_without_a_list_of](https://www.reddit.com/r/webscraping/comments/1h9tye2/how_to_scape_a_dictionary_without_a_list_of)
 - RSS feed: $source
 - date published: 2024-12-08T21:56:52+00:00

<!-- SC_OFF --><div class="md"><p>Hi! </p> <p>Just a little question, this is something that I had in my mind from years already but cant figure it out (nor find an answer on Stackoverflow or Google, etc) - Maybe with your knowledge and expertise some of you can point me in the right direction: a book, an article, etc.</p> <p>This is it:</p> <p>I know how to scrape the content (I don&#39;t need to &#39;clean&#39; it, I want it with the HTML and all - to style it with CSS), so that&#39;s coverered, but **how** can I scrape a site (exactly this, actually: <a href="https://dle.rae.es/amor?m=form">https://dle.rae.es/amor?m=form</a>) that serves 1 result at a time (a dictionary: languages, not the Data Structure) **without** having the list of existent search terms??</p> <p>To clarify, there&#39;s a search box, after input a search the result is returned and that&#39;s the data that I want; so how to &#39;exhaust&#39; the list of possible results without having a list to made queries from

## Any Way to bypass the Cloudflare's Turnstile ?
 - [https://www.reddit.com/r/webscraping/comments/1h9l7u9/any_way_to_bypass_the_cloudflares_turnstile](https://www.reddit.com/r/webscraping/comments/1h9l7u9/any_way_to_bypass_the_cloudflares_turnstile)
 - RSS feed: $source
 - date published: 2024-12-08T15:29:09+00:00

<!-- SC_OFF --><div class="md"><p>Tried seleniumbase, puppeteer and flaresolver. Nothing wokred. , Just want the script automation, can click the checkbox.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/rospubogne"> /u/rospubogne </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h9l7u9/any_way_to_bypass_the_cloudflares_turnstile/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h9l7u9/any_way_to_bypass_the_cloudflares_turnstile/">[comments]</a></span>

## Is this a good approach to scrape data for 1 million movies?
 - [https://www.reddit.com/r/webscraping/comments/1h9kytb/is_this_a_good_approach_to_scrape_data_for_1](https://www.reddit.com/r/webscraping/comments/1h9kytb/is_this_a_good_approach_to_scrape_data_for_1)
 - RSS feed: $source
 - date published: 2024-12-08T15:17:18+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m building a webapp for movie and tv show discovery.</p> <p>The data pipeline is implemented via python scripts and orchestrated with Windmill. I&#39;m using multiple Hetzner VPS to get better rate limiting results.</p> <p>In another post I got a comment about using paid proxies instead to save money. Would you agree with that? If yes, which proxies would you recommend?</p> <p>To learn more about my scraping pipeline, I wrote a blog post recently. I can share it if you&#39;re interested.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/alp82"> /u/alp82 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h9kytb/is_this_a_good_approach_to_scrape_data_for_1/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h9kytb/is_this_a_good_approach_to_scrape_data_for_1/">[comments]</a></span>

## What are the best practices to prevent my website from being scraped?
 - [https://www.reddit.com/r/webscraping/comments/1h9j9jq/what_are_the_best_practices_to_prevent_my_website](https://www.reddit.com/r/webscraping/comments/1h9j9jq/what_are_the_best_practices_to_prevent_my_website)
 - RSS feed: $source
 - date published: 2024-12-08T13:51:42+00:00

<!-- SC_OFF --><div class="md"><p>I‚Äôm looking for practical tips or tools to protect my site‚Äôs content from bots and scrapers. Any advice on balancing security measures without negatively impacting legitimate users would be greatly appreciated!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/metaplaton"> /u/metaplaton </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h9j9jq/what_are_the_best_practices_to_prevent_my_website/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h9j9jq/what_are_the_best_practices_to_prevent_my_website/">[comments]</a></span>

## Having an hard time scraping GMAPS for free.
 - [https://www.reddit.com/r/webscraping/comments/1h9ehw8/having_an_hard_time_scraping_gmaps_for_free](https://www.reddit.com/r/webscraping/comments/1h9ehw8/having_an_hard_time_scraping_gmaps_for_free)
 - RSS feed: $source
 - date published: 2024-12-08T08:33:19+00:00

<!-- SC_OFF --><div class="md"><p>I need to scrape email, phone, website, and business names from Google Maps! For instance, if I search for ‚Äúcleaning service in San Diego,‚Äù all the cleaning services listed on Google Maps should be saved in a CSV file. I‚Äôm working with a lot of AI tools to accomplish this task, but I‚Äôm new to web scraping. It would be helpful if someone could guide me through the process.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/6UwO9"> /u/6UwO9 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h9ehw8/having_an_hard_time_scraping_gmaps_for_free/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h9ehw8/having_an_hard_time_scraping_gmaps_for_free/">[comments]</a></span>

## ESPN data a day behind
 - [https://www.reddit.com/r/webscraping/comments/1h9dptv/espn_data_a_day_behind](https://www.reddit.com/r/webscraping/comments/1h9dptv/espn_data_a_day_behind)
 - RSS feed: $source
 - date published: 2024-12-08T07:35:47+00:00

<!-- SC_OFF --><div class="md"><p>When I go to ESPN to scrape live soccer scores, I use the API, e.g.:<br/> SOCCER_URL = &#39;https://<strong>site.web.api.espn.com</strong>/apis/v2/scoreboard/header?sport=soccer&amp;region=us&amp;lang=en&amp;contentorigin=espn&amp;tz=America%2FNew_York&#39;<br/> However it doesn&#39;t give me live scores in Australia - I think perhaps of timezones and Australia being a day ahead of USA.<br/> I&#39;ve tried using different regions in the URL (region=au) and different timezones (tz=Australia%2Sydney), even tried <a href="http://site.web.api.espn.com.au">site.web.api.espn.com.au</a> but nothing works (either get page not found error, or the same data as before).<br/> Does anybody know how to get live soccer json data for Australia?<br/> (note: if I go to normal HTML page (<a href="https://www.espn.com/football/scoreboard">https://www.espn.com/football/scoreboard</a>) I do see live Australian scores but cannot scrape them.)</p> </div><!-- SC_ON --> &#32;

## First time scraping data
 - [https://www.reddit.com/r/webscraping/comments/1h9dkdy/first_time_scraping_data](https://www.reddit.com/r/webscraping/comments/1h9dkdy/first_time_scraping_data)
 - RSS feed: $source
 - date published: 2024-12-08T07:25:03+00:00

<!-- SC_OFF --><div class="md"><p>I have never done Scraping, but I am trying to understand how it works. I had a first test in mind, extract all the times (per Runnings &amp; Stations) of the participants in a Hyrox (here Paris 2024) on the website <a href="https://results.hyrox.com/season-7/">https://results.hyrox.com/season-7/</a>.</p> <p>Having no skills I use ChatGPT to write in Python. The problem I am facing is the URL : there is no notion of filter in the URL. So once the filter is done, I have a list of participants : the program clicks on each participant to have their time per station (click on participant 1, return to the previous page, participant 2 etc.) But the list of participants is not filtered in the URL so the program gives me all the participants‚Ä¶ üò≠ (too long to execute the program)</p> <p>Maybe the cookies are the solution, but I don‚Äôt know how</p> <p>If someone can help me on this, that would be great üòä </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href

## Has anyone managed to scrape Ticketmaster with headless browser ?
 - [https://www.reddit.com/r/webscraping/comments/1h9cysd/has_anyone_managed_to_scrape_ticketmaster_with](https://www.reddit.com/r/webscraping/comments/1h9cysd/has_anyone_managed_to_scrape_ticketmaster_with)
 - RSS feed: $source
 - date published: 2024-12-08T06:42:56+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve tried playwright (python and node) normally, and with rebrowser as well. It can pass bot detection on browserscan.net/bot-detection, but Ticketmaster detects it still as a bot.</p> <p>Playwright-stealth also did nothing.</p> <p>I&#39;ve also tried setting executable path and even tried brave (both while using rebrowser) but nothing. </p> <p>Finally I tired headless=False and it&#39;s still the same issue. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/HoaxOfLife"> /u/HoaxOfLife </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1h9cysd/has_anyone_managed_to_scrape_ticketmaster_with/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1h9cysd/has_anyone_managed_to_scrape_ticketmaster_with/">[comments]</a></span>

## How to run AI webscrapers ?
 - [https://www.reddit.com/r/webscraping/comments/1h96rim/how_to_run_ai_webscrapers](https://www.reddit.com/r/webscraping/comments/1h96rim/how_to_run_ai_webscrapers)
 - RSS feed: $source
 - date published: 2024-12-08T00:44:42+00:00

<!-- SC_OFF --><div class="md"><p>Legit question , im a new starter , but i have been able to produce multiple python BS4 webscrapers that constantly need updating ,,, its for my personal use , so I&#39;m happy to be slower and use AI , if I don&#39;t have to constantly rebuild the webscrapers. </p> <p>Ive gotten : <a href="https://www.automation-campus.com/downloads/scrapemaster-4-0">https://www.automation-campus.com/downloads/scrapemaster-4-0</a> working with Gemini but it doesn&#39;t quite do what I want it to do. </p> <p>I think a python scraper that uses AI would be better for me , but for the life of me I cant get it working. </p> <p>Ive tried <a href="https://github.com/unclecode/crawl4ai">https://github.com/unclecode/crawl4ai</a> &amp; <a href="https://github.com/ScrapeGraphAI/Scrapegraph-ai">https://github.com/ScrapeGraphAI/Scrapegraph-ai</a> </p> <p>but no luck , I would prefer to use Gemini/Mistral API as they&#39;re free .... Any suggestions or good discord channels or Yo

