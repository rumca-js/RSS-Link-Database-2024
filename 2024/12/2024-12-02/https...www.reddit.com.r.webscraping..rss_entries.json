[
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T22:13:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1h57fai/unable_to_click_a_button_on_simple_website_using/\"> <img src=\"https://b.thumbs.redditmedia.com/7WsT-t5_0AdjPuF4MS1tK6cw0gCQA_Qr925KXPqiwnM.jpg\" alt=\"Unable to click a button on simple website using selenium\" title=\"Unable to click a button on simple website using selenium\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I am very new to this and still learning but I have encountered this problem while trying to click a button using selenium&#39;s click() function. This is the code that I have written:</p> <p><a href=\"https://preview.redd.it/uvp3dn3gei4e1.png?width=561&amp;format=png&amp;auto=webp&amp;s=c5002cbed16617b5e83a92b7df2b175a5ee96140\">https://preview.redd.it/uvp3dn3gei4e1.png?width=561&amp;format=png&amp;auto=webp&amp;s=c5002cbed16617b5e83a92b7df2b175a5ee96140</a></p> <p>It loads up the google homepage as normal but throws this error when trying to click on the button:</p> <p><a href=\"https:/",
        "id": 1629644,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h57fai/unable_to_click_a_button_on_simple_website_using",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/7WsT-t5_0AdjPuF4MS1tK6cw0gCQA_Qr925KXPqiwnM.jpg",
        "title": "Unable to click a button on simple website using selenium",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T20:09:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to figure out how I can scrape data to get the current listing / market status of properties<br/> Preferably from Zillow, or a MLS</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ZenBuddhism\"> /u/ZenBuddhism </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1h54e3o/how_to_scrape_property_data_listing_status/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1h54e3o/how_to_scrape_property_data_listing_status/\">[comments]</a></span>",
        "id": 1629218,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h54e3o/how_to_scrape_property_data_listing_status",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How to scrape property data / listing status?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T18:14:53+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>noob scraper here so sorry in advance if this is a stupid question. I&#39;m trying to scrape one of the big social media platforms (last time I said which one my post got taken down so won&#39;t say which one...). I want to search a keyword and scrape the first 50 results. I need to be logged in to do this</p> <p>I can log in with pretty much no problem, and when logged in, when entering &quot;network&quot; on chrome, I can find the correct request with all the info i need. I copy this and call it from my own terminal and it returns all the info</p> <p>So far so good. But this website only returns the first 10 results, I have to scroll down to eventually reach 50 results. There is an &quot;offset&quot; param in the request I&#39;ve copied, which will return the next 10 results if changed, but if i change it, it returns an empty 200 response</p> <p>So my question is: is there any way I can make this request to get all 50 results? ",
        "id": 1627193,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h51k79/noob_question_how_to_call_the_websites_api",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Noob question: how to call the websites API directly when logged in?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T15:01:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Welcome to the weekly discussion thread! Whether you&#39;re a seasoned web scraper or just starting out, this is the perfect place to discuss topics that might not warrant a dedicated post, such as:</p> <ul> <li>Techniques for extracting data from popular sites like LinkedIn, Facebook, etc.</li> <li>Industry news, trends, and insights on the web scraping job market</li> <li>Challenges and strategies in marketing and monetizing your scraping projects</li> </ul> <p>Like our monthly <a href=\"https://reddit.com/r/webscraping/about/sticky?num=1\">self-promotion</a> thread, mentions of paid services and tools are permitted \ud83e\udd1d. If you&#39;re new to web scraping, be sure to check out the <a href=\"https://webscraping.fyi\">beginners guide</a> \ud83c\udf31</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AutoModerator\"> /u/AutoModerator </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1h4ww0v/weekly_discussion_02_",
        "id": 1627191,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h4ww0v/weekly_discussion_02_dec_2024",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Weekly Discussion - 02 Dec 2024",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T14:49:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey there,</p> <p>I am currently running a business based on web-scraping, and I am currently self hosting it. It is a job that runs for about 4 hours of the day, and the machine stays idle for the rest of the day.</p> <p>My first idea was to run this app (a docker container) inside an AWS EC2 machine, which obviously turned out bad, because IP&#39;s from AWS are all blacklisted by big companies that do not want their data scraped.</p> <p>I would not like to pay for proxies, because the experience I had is that they are very non-reliable. </p> <p>Do you guys know any option I have where I can host this app without having issues with IP? Maybe a smaller provider...</p> <p>I am trying to get out of the self hosting because my internet connection has been lacking lately, and its summertime on the south hemisphere and with rains comes a lot of power outages.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dick_vegan",
        "id": 1627192,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h4wmfv/cloud_providers_that_do_not_have_blacklisted_ips",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Cloud providers that do not have \"blacklisted\" IP's for webscraping",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T10:46:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working on a little side project and need some advice. I want to scrape data for every restaurant in a specific city from Google Maps to run a price analysis on food prices. I\u2019ve looked into the Google Places API, but it seems like it can\u2019t handle this kind of bulk scraping(limit to 20 results per query, and rank by prominence so some restaurants might be missing).</p> <p>Has anyone here managed to do something like this? Are there any tools, scripts, or workarounds you\u2019d recommend? Or is it just not doable with Google Maps due to restrictions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/hello_motherfuckers_\"> /u/hello_motherfuckers_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1h4s942/scrape_every_restaurants_in_a_city_from_google_map/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1h4s942/scrape_every_restaurants_in_a_city_from_google_ma",
        "id": 1625344,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h4s942/scrape_every_restaurants_in_a_city_from_google_map",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "scrape every restaurants in a city from google map",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T10:30:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Heyho</p> <p>So, I&#39;ve had a new job for a while at a small company and my boss wants me to build a kind of search engine which searches a ( fixed) number of job boards for what the user wants and I was wondering if you guys might have insight in how to best approach this. </p> <p><strong>Prerequisites:</strong><br/> - My boss has a list of roughly 2000 job boards, all directly on the websites of the institutions themselves. So no Indeed or other big boards.</p> <p>- The important thing is the user should be able to search through these websites either through freetext or specific job titles (doesn&#39;t have to be both, either one is fine )</p> <p>- The company is very small and I&#39;m the only developer.</p> <p>- Filtering by location with &quot;X km radius&quot; is necessary</p> <p>At first I&#39;ve been thinking this might be way too much work and take too long - but since talking about the requirements I&#39;m thinking with existing solution",
        "id": 1625345,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h4s1nv/scrape_thousands_of_small_websites_for_job",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scrape thousands of small websites for job postings?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": null,
        "bookmarked": false,
        "comments": [],
        "date_published": "2024-12-02T06:16:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1h4ok3i/getting_empty_lists_back_despite_using_selenium/\"> <img src=\"https://a.thumbs.redditmedia.com/sghDm3RZdaosEPfgH-Nc9q-Wsc8CcaSNu4oPdHKfac4.jpg\" alt=\"Getting empty lists back despite using Selenium\" title=\"Getting empty lists back despite using Selenium\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>hi guys. need some help. this is the site - <a href=\"https://www.livemint.com/market/market-stats/stocks-zomato-share-price-nse-bse-s0005111\">https://www.livemint.com/market/market-stats/stocks-zomato-share-price-nse-bse-s0005111</a></p> <p>I want to scrape the Strengths, Weakness, Opportunities and Threat &lt;li&gt; tags. Despite using Selenium, I&#39;m being returned empty lists. Please help. Thanks a lot. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Open-Zookeepergame90\"> /u/Open-Zookeepergame90 </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1h4ok3i\">",
        "id": 1624356,
        "language": null,
        "link": "https://www.reddit.com/r/webscraping/comments/1h4ok3i/getting_empty_lists_back_despite_using_selenium",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/sghDm3RZdaosEPfgH-Nc9q-Wsc8CcaSNu4oPdHKfac4.jpg",
        "title": "Getting empty lists back despite using Selenium",
        "vote": 0
    }
]