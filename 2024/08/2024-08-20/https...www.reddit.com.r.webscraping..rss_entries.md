# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Scraping USNews Health - Requests timing out
 - [https://www.reddit.com/r/webscraping/comments/1ex3ddk/scraping_usnews_health_requests_timing_out](https://www.reddit.com/r/webscraping/comments/1ex3ddk/scraping_usnews_health_requests_timing_out)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T18:39:23+00:00

<!-- SC_OFF --><div class="md"><p>I'm trying to scrape <a href="https://health.usnews.com/">https://health.usnews.com/</a>. Unfortunately, any request I send to their search API, &quot;<a href="https://health.usnews.com/search/api/results">https://health.usnews.com/search/api/results</a>&quot;, results in a time-out -- not even an error. I suppose that they detect my request as botting and block it. What can I do to get through?</p> <p>I tried setting my user agent to the same one used in the request sent by the browser, as well as the &quot;referer&quot; header, but that doesn't seem to be enough. How do I find out which property exactly is responsible for blocking/allowing my request?</p> <p>Could it have something to do with the request in my browser containing HTTP2 stream headers? (<a href="https://stackoverflow.com/questions/57484393/http-2-requests-and-headers-starting-with-colon">https://stackoverflow.com/questions/57484393/http-2-requests-and-headers-starting-with-colon</a>)<

## TLS fingerprinting with Playwright/Puppeteer
 - [https://www.reddit.com/r/webscraping/comments/1ex1z1e/tls_fingerprinting_with_playwrightpuppeteer](https://www.reddit.com/r/webscraping/comments/1ex1z1e/tls_fingerprinting_with_playwrightpuppeteer)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T17:43:27+00:00

<!-- SC_OFF --><div class="md"><p>If I am dealing with a website that uses TLS fingerprinting, how should I deal with this while using playwright or puppeteer at scale? As I understand, these solutions do not provide any way to change their cipher suites.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/happyotaku35"> /u/happyotaku35 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1ex1z1e/tls_fingerprinting_with_playwrightpuppeteer/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ex1z1e/tls_fingerprinting_with_playwrightpuppeteer/">[comments]</a></span>

## Scraping data behind login, transforming and referencing source legal?
 - [https://www.reddit.com/r/webscraping/comments/1ewwft5/scraping_data_behind_login_transforming_and](https://www.reddit.com/r/webscraping/comments/1ewwft5/scraping_data_behind_login_transforming_and)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T14:04:34+00:00

<!-- SC_OFF --><div class="md"><p>pretty much the above. </p> <p>If I scrape data that is behind a login, then make it an aggregate and throw in my own categorization (completeley new interpretation/numbers) and reference the data source, am I in the clear? Is that considered fair use? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Nokita_is_Back"> /u/Nokita_is_Back </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1ewwft5/scraping_data_behind_login_transforming_and/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ewwft5/scraping_data_behind_login_transforming_and/">[comments]</a></span>

## Scraping a "simple" PoE-Switch dynamically loaded internal site
 - [https://www.reddit.com/r/webscraping/comments/1ewuvzj/scraping_a_simple_poeswitch_dynamically_loaded](https://www.reddit.com/r/webscraping/comments/1ewuvzj/scraping_a_simple_poeswitch_dynamically_loaded)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T12:59:21+00:00

<!-- SC_OFF --><div class="md"><p>Hi there,</p> <p>I am somewhat familiar with Python but I do not have any clue about html structure and JS code...</p> <p>I'm trying to scrape a single number from my TP-Link Switch internal site. When I log in with Selenium...I get to this page:</p> <p><a href="https://preview.redd.it/0putnt59atjd1.png?width=2394&amp;format=png&amp;auto=webp&amp;s=af91c392622ed96d305f7b7e61108709521d964d">https://preview.redd.it/0putnt59atjd1.png?width=2394&amp;format=png&amp;auto=webp&amp;s=af91c392622ed96d305f7b7e61108709521d964d</a></p> <p>What I need to do to get to my desired number is clicking on the &quot;PoE&quot;-Button. But there is no button I can address with Selenium. It is generated after a click event. But how do I initiate this click event with Selenium.</p> <p>This is the html structure where my desired number lives in:</p> <p><a href="https://preview.redd.it/qhnlj1jratjd1.png?width=1766&amp;format=png&amp;auto=webp&amp;s=6fa8fa176d6fd73300f758e7f125

## Project Ideas
 - [https://www.reddit.com/r/webscraping/comments/1ewlk3l/project_ideas](https://www.reddit.com/r/webscraping/comments/1ewlk3l/project_ideas)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T03:26:30+00:00

<!-- SC_OFF --><div class="md"><p>What applications can I build with a web crawling that won't cause me problems? I am thinking of ideas for a Senior Design project class. We have 7 people. No experience with web crawling but we can learn for one semester and implement the next. Any ideas in mind that doesn't have any problems with legality. Any ideas will be helpful. Thanks in advance. I was excited to build an application to scrape job listings but LinkedIn, handshake, and indeed don't allow for it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Hash_003_"> /u/Hash_003_ </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1ewlk3l/project_ideas/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1ewlk3l/project_ideas/">[comments]</a></span>

## Millions of domains- whats the best way to discover their subdomains?
 - [https://www.reddit.com/r/webscraping/comments/1ewik3n/millions_of_domains_whats_the_best_way_to](https://www.reddit.com/r/webscraping/comments/1ewik3n/millions_of_domains_whats_the_best_way_to)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-20T01:00:40+00:00

<!-- SC_OFF --><div class="md"><p>I'm working on a large scale project(300m domains) that is scraping a large portion of the internet </p> <p>Even though I have a solid list of domains to work from I don't have a list of their subdomains or websites in general that include subdomains. I know that there are different tools out there such as amass and reconftw which I started using today, however they all seem to be too slow in returning results.</p> <p>I need either a github repo/code that is performant enough to return results for these 300 million OR a paid tool (couple hundred max) that can return the subdomains for a large amount of active website domains. </p> <p>I'm fortunate enough to have a very large homelab with several servers, two fiber ISP's and proxies which means I can run it out of my house. But I need the throughput to be 10 million per day/ 115 domains per second.</p> <p>I'm looking for suggestions on how to do this or recommendations from people who have done somethi

