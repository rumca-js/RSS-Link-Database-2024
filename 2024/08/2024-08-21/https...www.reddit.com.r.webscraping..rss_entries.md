# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Apple defacto removed open access to their Podcast index. What now?
 - [https://www.reddit.com/r/webscraping/comments/1exrb60/apple_defacto_removed_open_access_to_their](https://www.reddit.com/r/webscraping/comments/1exrb60/apple_defacto_removed_open_access_to_their)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T14:39:26+00:00

<!-- SC_OFF --><div class="md"><p>Until a few days ago Apple had a (pretty) open listing of all the podcasts in their index under <a href="https://podcasts.apple.com/us/genre/podcasts/id26">https://podcasts.apple.com/us/genre/podcasts/id26</a> - AFAIK this was the base for everyone who wanted to build and maintain their own podcast directory.</p> <p>Now Apple removed that listing and replaced it with their new Apple Podcasts web app. All the old listing URLs redirect there. Sadly there doesn't seem to be an equivalent, full index access anymore. Only category pages have just a few podcasts on them. And a search. But no way to systematically access all podcasts. (At least as far as I can see).</p> <p>If this is true I guess this will be problematic for the podcast ecosystem. Everyone who has their own directory can of course crawl the existing RSS feeds. But new Podcasts will probably mostly be submitted to the Apple index (and maybe a few other, bigger services).</p> <p>I stumbled ove

## Which is the most powerfull tool
 - [https://www.reddit.com/r/webscraping/comments/1exqvzn/which_is_the_most_powerfull_tool](https://www.reddit.com/r/webscraping/comments/1exqvzn/which_is_the_most_powerfull_tool)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T14:22:11+00:00

<!-- SC_OFF --><div class="md"><p>I want to know which is the most powerfull and capable tool for webscraping, i already know a lil puppeteer.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1exqvzn/which_is_the_most_powerfull_tool/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1exqvzn/which_is_the_most_powerfull_tool/">[comments]</a></span>

## Botasaurus scraping library
 - [https://www.reddit.com/r/webscraping/comments/1exmb84/botasaurus_scraping_library](https://www.reddit.com/r/webscraping/comments/1exmb84/botasaurus_scraping_library)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T10:40:01+00:00

<!-- SC_OFF --><div class="md"><p>Anyone using <a href="https://github.com/omkarcloud/botasaurus">Botasaurus</a> library? Specifically I'm looking at the <a href="https://github.com/Touventure/google-map-scraper">google-map-scraper</a> that uses the Botasaurus framework.</p> <p>I got it running locally, and it works quite good, but I'd like to host in the cloud as an API scraper server. Anyone had any luck running Botasaurus in the cloud?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/FromAtoZen"> /u/FromAtoZen </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1exmb84/botasaurus_scraping_library/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1exmb84/botasaurus_scraping_library/">[comments]</a></span>

## How to bypass PerimeterX with Puppeteer?
 - [https://www.reddit.com/r/webscraping/comments/1exk8az/how_to_bypass_perimeterx_with_puppeteer](https://www.reddit.com/r/webscraping/comments/1exk8az/how_to_bypass_perimeterx_with_puppeteer)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T08:22:18+00:00

<!-- SC_OFF --><div class="md"><p>Hey folks!</p> <p>I'm trying to scrape <a href="http://Zillow.com">Zillow.com</a> and keep getting the &quot;Press and Hold&quot; by PerimeterX.</p> <p>I implemented the stealth plugin and worked with a proxy that is clean (When I access Zillow from my Google Chrome I'm able to see Zillow right away).</p> <p>Have you had a chance to bypass it? If so - I would appreciate your feedback.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/kobimantzur"> /u/kobimantzur </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1exk8az/how_to_bypass_perimeterx_with_puppeteer/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1exk8az/how_to_bypass_perimeterx_with_puppeteer/">[comments]</a></span>

## Can you scrape Truecaller?
 - [https://www.reddit.com/r/webscraping/comments/1exje6q/can_you_scrape_truecaller](https://www.reddit.com/r/webscraping/comments/1exje6q/can_you_scrape_truecaller)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T07:23:56+00:00

<!-- SC_OFF --><div class="md"><p>Is it possible to scrape the Truecaller database? As annoying their ads and trackers are, their functionality is pretty decent. So I was wondering if it was possible, and how large would their database be? I'm not really that familiar with webscraping but as the data in Truecaller is public, I assume it's possible?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Lazy_Fortune_9409"> /u/Lazy_Fortune_9409 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1exje6q/can_you_scrape_truecaller/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1exje6q/can_you_scrape_truecaller/">[comments]</a></span>

## My team launched an AI web scraper that extracts data from websites
 - [https://www.reddit.com/r/webscraping/comments/1exi39m/my_team_launched_an_ai_web_scraper_that_extracts](https://www.reddit.com/r/webscraping/comments/1exi39m/my_team_launched_an_ai_web_scraper_that_extracts)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T05:58:38+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1exi39m/my_team_launched_an_ai_web_scraper_that_extracts/"> <img alt="My team launched an AI web scraper that extracts data from websites " src="https://external-preview.redd.it/cDg4Y210NnFpeWpkMTq0dSm3FmqjaXNM9mC8T9e6EcxbBr8uRYWGEyiYpiBy.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6db98acbcc9820f9a5b4c4acd988f722de09c67" title="My team launched an AI web scraper that extracts data from websites " /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Legitimate-Adagio662"> /u/Legitimate-Adagio662 </a> <br /> <span><a href="https://v.redd.it/jksbcm6qiyjd1">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1exi39m/my_team_launched_an_ai_web_scraper_that_extracts/">[comments]</a></span> </td></tr></table>

## Why should one ever use requests after learning about curl cffi?
 - [https://www.reddit.com/r/webscraping/comments/1exbqyi/why_should_one_ever_use_requests_after_learning](https://www.reddit.com/r/webscraping/comments/1exbqyi/why_should_one_ever_use_requests_after_learning)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-21T00:28:35+00:00

<!-- SC_OFF --><div class="md"><p>I recently discovered that curl cffi can be used for evading anti bot measures.<br /> My question is, why do people still use the simple requests library? I mean it looks really simple to use as well (with the added benefit of browser fingerprinting). I found this code snippet to fetch a URL online. Looks just like using the requests library with the only difference being an extra &quot;impersonate&quot; paramater being passed to get() ```py</p> <h1>import the required libraries</h1> <p>from curl_cffi import requests</p> <h1>add an impersonate parameter</h1> <p>response = requests.get( &quot;<a href="https://www.scrapingcourse.com/ecommerce/">https://www.scrapingcourse.com/ecommerce/</a>&quot;, impersonate=&quot;safari_ios&quot; ) ``` Can anyone please help me understand the specific situations where each of these libraries should be used? Note: It's a beginner question. Sorry if it is a bit basic.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a 

