# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Question about scraping edimax AP web page
 - [https://www.reddit.com/r/webscraping/comments/1elqo7u/question_about_scraping_edimax_ap_web_page](https://www.reddit.com/r/webscraping/comments/1elqo7u/question_about_scraping_edimax_ap_web_page)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T19:23:47+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1elqo7u/question_about_scraping_edimax_ap_web_page/"> <img alt="Question about scraping edimax AP web page" src="https://b.thumbs.redditmedia.com/oVhlAFkx09iqMVLnQqtyoIuCKEi_66_0xVdr4XMFLlg.jpg" title="Question about scraping edimax AP web page" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hi</p> <p>Maybe this is stupid / novice question but I'm new at this </p> <p>I'm trying to automate in Home Assistant (with HA browserless addon, which I believe uses puppeteer under the hood ) activation of edimax AP web interface. See example picture </p> <p>Eventually I want to browse to a advanced configuration page sub menu and 'press' restart when something the automation is triggered </p> <p>I'm having trouble reading this simple web page The edimax web page is accessed via url like this <a href="http://192.168.0.52/index.asp">http://192.168.0.52/index.asp</a></p> <p>Let's say for start I want to use curl for scrap

## Data analysis on aviation safety database
 - [https://www.reddit.com/r/webscraping/comments/1ellzbg/data_analysis_on_aviation_safety_database](https://www.reddit.com/r/webscraping/comments/1ellzbg/data_analysis_on_aviation_safety_database)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T16:17:48+00:00

<!-- SC_OFF --><div class="md"><p>Link: <a href="https://asn.flightsafety.org/asndb/types/CJ">https://asn.flightsafety.org/asndb/types/CJ</a></p> <p>Project description: Hi guys,</p> <p>I’m an undergrad CS/Data science student trying to do an exploratory data analysis project to add to my resume and GitHub. I have a fascination with plane crashes and aviation in general, so I found the most comprehensive database on the internet on this topic (more comprehensive than NTSB which doesn’t include international data). </p> <p>The link I provided is the database for the Aviation Safety Network database for commercial jetliner incidents. Essentially, I’m trying to scrape all of the data for each entry of each airliner type (Airbus A320, Boeing 757-300, etc) and put all this data into one .csv or .json file. </p> <p>The reason I’m reaching out is because I contacted the site PR team for access to the full structured database in the form of a .csv or .json but they said I need to pay a fee of

## Google Maps scraping best way
 - [https://www.reddit.com/r/webscraping/comments/1elle7j/google_maps_scraping_best_way](https://www.reddit.com/r/webscraping/comments/1elle7j/google_maps_scraping_best_way)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T15:54:59+00:00

<!-- SC_OFF --><div class="md"><p>Hi guys, I have a list of businesses(20k) and i want to scrape info from the google Maps. What would be the best approach regarding to the money and time. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/albundyhdd"> /u/albundyhdd </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elle7j/google_maps_scraping_best_way/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elle7j/google_maps_scraping_best_way/">[comments]</a></span>

## Request limit on Amazon per proxy?
 - [https://www.reddit.com/r/webscraping/comments/1elksr2/request_limit_on_amazon_per_proxy](https://www.reddit.com/r/webscraping/comments/1elksr2/request_limit_on_amazon_per_proxy)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T15:31:04+00:00

<!-- SC_OFF --><div class="md"><p>I am building a scraper for Amazon Reviews and I am curious about the limit of requests per day per proxy.<br /> I already have residential proxies, which work great but I now want to implement a rotation system with limits.<br /> But I haven't found any resources on how many requests per Ip(proxy) I can make max per day?<br /> Is there a formula or base guide I can follow or what do you recommend?</p> <p>Basically how many requests can one proxy handle per day or per month if that is better to be on the safe side?</p> <p>Also would this be for all of Amazon or could I do the limit for <a href="http://amazon.com">amazon.com</a> and then again for <a href="http://amazon.co.uk">amazon.co.uk</a> again or will it track for the whole site, indepented of the country domain? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Intrepid_Traffic9100"> /u/Intrepid_Traffic9100 </a> <br /> <span><a href="https://www.reddit.com/r

## Review my web scraper code
 - [https://www.reddit.com/r/webscraping/comments/1elie1g/review_my_web_scraper_code](https://www.reddit.com/r/webscraping/comments/1elie1g/review_my_web_scraper_code)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T13:53:52+00:00

<!-- SC_OFF --><div class="md"><p>Hello guys I just finished this <a href="https://pastebin.com/KLQhrQPV">web scraper</a> which takes client's company name then gets the data for their yearly turnover from a public registry. What do you think and how can I improve it.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/No_Bus8709"> /u/No_Bus8709 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elie1g/review_my_web_scraper_code/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elie1g/review_my_web_scraper_code/">[comments]</a></span>

## Scraping all companies using greenhouse.io in a city.
 - [https://www.reddit.com/r/webscraping/comments/1elhy3g/scraping_all_companies_using_greenhouseio_in_a](https://www.reddit.com/r/webscraping/comments/1elhy3g/scraping_all_companies_using_greenhouseio_in_a)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T13:34:25+00:00

<!-- SC_OFF --><div class="md"><p>I've been trying to scrape jobs in my city on greenhouse.io. Actually, first I'm trying to determine which companies are using greenhouse, because then I can just type <a href="https://boards.greenhouse.io/companynamehere">https://boards.greenhouse.io/companynamehere</a> and just loop through all these companies and get listings that way. The main attempts so far are using google dorking (site:boards.greenhouse.io &quot;cityname&quot;) and trying apyfy's greenhouse scraper (wasn't very good). Greenhouse doesn't really have a way to let you search for their clients lol, from a quick glance at their API it doesn't look like its a good use for what I'm trying to scrape. I keep trying to think further outside the box but i'm open to any recommendations</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Impressive_Safety_26"> /u/Impressive_Safety_26 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments

## Selenium button click not updating content when browsing manually
 - [https://www.reddit.com/r/webscraping/comments/1elgf12/selenium_button_click_not_updating_content_when](https://www.reddit.com/r/webscraping/comments/1elgf12/selenium_button_click_not_updating_content_when)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T12:26:08+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1elgf12/selenium_button_click_not_updating_content_when/"> <img alt="Selenium button click not updating content when browsing manually " src="https://a.thumbs.redditmedia.com/IYS2oPdNFAbo4HRCVH3Nard7ou-h7x1p0_x0x6Mihh0.jpg" title="Selenium button click not updating content when browsing manually " /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hi! I'm trying to get content from a website using Selenium, Python, but manual interaction doesn't work as expected (and code doesn't do its job, either).</p> <p><strong>Context</strong>: the website contains multiple products, and each product has its own page with product info, a section for reviews and a section for questions and answers. </p> <p>The reviews section is similar tot the questions one. They both only show 10 items at a time, and to see more you have to press a button with a page number that updates only that specific section with content from that sect

## HELP! Scraping from Clinicaltrials.gov
 - [https://www.reddit.com/r/webscraping/comments/1elfxid/help_scraping_from_clinicaltrialsgov](https://www.reddit.com/r/webscraping/comments/1elfxid/help_scraping_from_clinicaltrialsgov)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T12:01:37+00:00

<!-- SC_OFF --><div class="md"><p>For a research project, I need to extract around 50 variables from pre-registered research protocolls on clinicaltrials.gov. 70% of the variables are rather easy to find, the rest requires some kind of absrtaction. Would be happy if someone could help me with the first part, even better when also more abstract variables could be extracted. </p> <p>&lt;3 </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AboutPsychedelics"> /u/AboutPsychedelics </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elfxid/help_scraping_from_clinicaltrialsgov/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elfxid/help_scraping_from_clinicaltrialsgov/">[comments]</a></span>

## Automation!!
 - [https://www.reddit.com/r/webscraping/comments/1elf4x5/automation](https://www.reddit.com/r/webscraping/comments/1elf4x5/automation)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T11:19:24+00:00

<!-- SC_OFF --><div class="md"><p>Hi guys! I am new to this and I have a doubt<br /> My manager assigned me a task to fetch all the data from a website. I know to write a simple web scraper. But, first we have to log into their website. So, they suggested me to do automating website. I tried to write selenium in python. But, this has to be hosted on server and the data from the website should be accessed via an API call. Every time I run selenium code, It opens an instance of browser and performing automation. Is there any other way to handle this without opening a browser? I think this should be hosted on EC2 instance??</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Plane_Past129"> /u/Plane_Past129 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elf4x5/automation/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elf4x5/automation/">[comments]</a></span>

## Scrape names from a website?
 - [https://www.reddit.com/r/webscraping/comments/1eldzdh/scrape_names_from_a_website](https://www.reddit.com/r/webscraping/comments/1eldzdh/scrape_names_from_a_website)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T10:09:56+00:00

<!-- SC_OFF --><div class="md"><p>I have a trade news website that I want to scrape all the names of people and companies from.</p> <p>They get mentioned all the time in news articles.</p> <p>I want the output to be a column with the name of the person, the name of the company (they are not always mentioned together) and a column with a short extract of the text before and after they were mentioned </p> <p>Is this possible? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Shhhh_for_now"> /u/Shhhh_for_now </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1eldzdh/scrape_names_from_a_website/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1eldzdh/scrape_names_from_a_website/">[comments]</a></span>

## I'm sure there are a million posts with this question but not sure what to search for: capturing xhr requests triggered client side with backend tool?
 - [https://www.reddit.com/r/webscraping/comments/1elcx0l/im_sure_there_are_a_million_posts_with_this](https://www.reddit.com/r/webscraping/comments/1elcx0l/im_sure_there_are_a_million_posts_with_this)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T08:59:55+00:00

<!-- SC_OFF --><div class="md"><p>Hello!</p> <p>I have a use case in which I'd like to be able to monitor response times as well as the request bodies of XHR requests triggered from a few pages.</p> <p>For example: I have <code>mystore.com/t-shirts</code> which is a React application. The React app bootstraps and will go out and fetch a bunch of data from various services (a commerce API for product data, a CMS for marketing content, various analytics endpoints, CRM, etc, etc). </p> <p>I want to build a <em>backend</em> service (i.e. headless / not running in a web browser) that can load this website, &quot;run&quot; the web application and capture / intercept / proxy all of the fetch requests that page loads.</p> <p>I don't actually care what the final rendered page looks like, but I do need the React application to run so I can see which XHR requests are triggered. I care about capturing the XHR requests more than the website itself.</p> <p>Is there a clear or obvious tool for doing

## May I check if anyone here knows if it's possible to do the following webscraping?
 - [https://www.reddit.com/r/webscraping/comments/1elcjb5/may_i_check_if_anyone_here_knows_if_its_possible](https://www.reddit.com/r/webscraping/comments/1elcjb5/may_i_check_if_anyone_here_knows_if_its_possible)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T08:33:11+00:00

<!-- SC_OFF --><div class="md"><p>Hi guys, I'm relatively new to webscraping and have only tried scraping twitter info with R programming (I've never worked with python). </p> <p>I asked ChatGPT and it provided me with a solution but I would like to double check with a real human that it's possible/relatively easy to perform before I go into in-depth researching how to do it. </p> <p>Basically my task is go into this webpage (<a href="https://proff.no/laglister?sort=revenueDesc&amp;proffIndustryCode=4468">https://proff.no/laglister?sort=revenueDesc&amp;proffIndustryCode=4468</a>) --&gt; click into the individual URLs of the 20 companies listed on the page --&gt; print each page as a pdf separately --&gt; zip the files into one folder. </p> <p>Chat GPT tells me I can do this with the rvest package. </p> <p>Thanks for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Manekitty"> /u/Manekitty </a> <br /> <span><a href="https://www.reddit.co

## Google reviews scraping
 - [https://www.reddit.com/r/webscraping/comments/1elc0g5/google_reviews_scraping](https://www.reddit.com/r/webscraping/comments/1elc0g5/google_reviews_scraping)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T07:57:07+00:00

<!-- SC_OFF --><div class="md"><p>Best library or 3rd party solution for this?</p> <p>The Google API only allows for fetching 5 reviews. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/FromAtoZen"> /u/FromAtoZen </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elc0g5/google_reviews_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elc0g5/google_reviews_scraping/">[comments]</a></span>

## Scraping public Facebook pages
 - [https://www.reddit.com/r/webscraping/comments/1elbpst/scraping_public_facebook_pages](https://www.reddit.com/r/webscraping/comments/1elbpst/scraping_public_facebook_pages)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T07:37:20+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I'm trying to help a friend that needs to collect like, follower, and event dates from public Facebook pages of local metal bands.</p> <p>Any idea how to start ? API route is not an option, Facebook doesn't want him to get verified even though he is promoting those bands on his music festivals.</p> <p>Do I need to use creds for scraping or is public access is enough ? Any tips? I have python skills but not in this specific task.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Anovion"> /u/Anovion </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1elbpst/scraping_public_facebook_pages/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1elbpst/scraping_public_facebook_pages/">[comments]</a></span>

## Scraping for a CS/Data science project
 - [https://www.reddit.com/r/webscraping/comments/1ela9kf/scraping_for_a_csdata_science_project](https://www.reddit.com/r/webscraping/comments/1ela9kf/scraping_for_a_csdata_science_project)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T06:02:39+00:00

<!-- SC_OFF --><div class="md"><p>I’m a college student trying to do some exploratory data analysis as a personal project (I also want to add it to GitHub) but as a first time web scraper, I’m scared of getting sued. </p> <p>So the database I’m looking at is publicly available on the front-end for free for all users, but there just isn’t a .json or .csv file containing all of the structured data. All the data is literally there, but I’d have to go entry by entry clicking each link and inputting data manually if I want to do analysis. </p> <p>I reached out to the guy running the database by email, telling them what I planned to do and asking for access, but he said that it’s only available for fee of $1200… </p> <p>I did inspect element and there was a comment on the HTML saying “do not scrape it is copyright infringement” which is frustrating.</p> <p>Any tips on this? Should I proceed, if so, is there a possibility of scraping blockers on the site (and how to circumvent) and should I 

## Noob in need of some advice: Python, requests, bs4, and selenium
 - [https://www.reddit.com/r/webscraping/comments/1el9jnc/noob_in_need_of_some_advice_python_requests_bs4](https://www.reddit.com/r/webscraping/comments/1el9jnc/noob_in_need_of_some_advice_python_requests_bs4)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T05:17:24+00:00

<!-- SC_OFF --><div class="md"><p>I used to scrap a page and successfully retrieved JSON data using Python with requests and beautiful soup 4. However, they changed something. Now, I can’t find that specific div containing data using requests and bs4. The div is there if I open the page in a browser and view the source code. I used selenium with a waiting time of up to 60 seconds for the page/js to load completely. Still no luck.</p> <p>Using selenium and bs4 to dump the source code… it shows all the html/js, excluding that specific div.<br /> This is the URL in question:<br /> <a href="https://tabs.ultimate-guitar.com/tab/pink-floyd/wish-you-were-here-guitar-pro-574167">https://tabs.ultimate-guitar.com/tab/pink-floyd/wish-you-were-here-guitar-pro-574167</a></p> <p>this is what I'm trying to get:<br /> &lt;div class=&quot;js-store&quot; data-content=&quot;data to scrap”&gt;&lt;/div&gt;</p> <p>Can anyone advise me on how to approach this?</p> </div><!-- SC_ON --> &#32; submitted by &#3

## Help Digging up an Audio File
 - [https://www.reddit.com/r/webscraping/comments/1el7s1a/help_digging_up_an_audio_file](https://www.reddit.com/r/webscraping/comments/1el7s1a/help_digging_up_an_audio_file)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T03:36:41+00:00

<!-- SC_OFF --><div class="md"><p>Trying my hand at reviving a Song on Spotify that was made unavailable by the Artist. I'm not sure how exactly it works, but seeing that the song itself is still on Spotify, I'm holding out some hope as its not erased from the website completely.</p> <p>Tried using both Screaming Frog and httrack, both of which I tried my best to understand and use. Also even tried using The Wayback Machine lol. Parsed through the Website Console for the song too and found no luck, but im mainly just suspicious that the mp3 file is still tucked deep away in there somewhere and Spotify is only preventing me from reaching it.</p> <p>This is somewhat new to me, so if im in the wrong anywhere please correct me. Otherwise, any better tools, methods or tips would be greatly appreciated :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Mimicewoo"> /u/Mimicewoo </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1el

## How to Efficiently Scrape News Pages from 1000 Company Websites?
 - [https://www.reddit.com/r/webscraping/comments/1el3ns4/how_to_efficiently_scrape_news_pages_from_1000](https://www.reddit.com/r/webscraping/comments/1el3ns4/how_to_efficiently_scrape_news_pages_from_1000)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-08-06T00:16:20+00:00

<!-- SC_OFF --><div class="md"><p>I am currently working on a project where I need to scrape the news pages from 10 to at most 2000 different company websites. The project is divided into two parts: the initial run to initialize a database and subsequent weekly (or other periodic) updates.</p> <p>I am stuck on the first step, initializing the database. My boss wants a “write-once, generalizable” solution, essentially mimicking the behavior of search engines. However, even if I can access the content of the first page, handling pagination during the initial database population is a significant challenge. My boss understands Python but is not deeply familiar with the intricacies of web scraping. He suggested researching how search engines handle this task to understand our limitations. While search engines have vastly more resources, our target is relatively small. The primary issue seems to be the complexity of the code required to handle pagination robustly. For a small team, implemen

