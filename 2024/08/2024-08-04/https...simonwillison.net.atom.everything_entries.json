[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-08-04T19:11:13+00:00", "description": "<p><strong><a href=\"https://www.wsj.com/tech/ai/openai-tool-chatgpt-cheating-writing-135b755a?st=830dm1b5txdsqx4\">There\u2019s a Tool to Catch Students Cheating With ChatGPT. OpenAI Hasn\u2019t Released It.</a></strong></p>\nThis attention-grabbing headline from the Wall Street Journal makes the underlying issue here sound less complex, but there's a lot more depth to it.</p>\n<p>The story is actually about watermarking: embedding hidden patterns in generated text that allow that text to be identified as having come out of a specific LLM.</p>\n<p>OpenAI evidently have had working prototypes of this for a couple of years now, but they haven't shipped it as a feature. I think this is the key section for understanding why:</p>\n<blockquote>\n<p>In April 2023, OpenAI commissioned a survey that showed people worldwide supported the idea of an AI detection tool by a margin of four to one, the internal documents show. </p>\n<p>That same month, OpenAI surveyed ChatGPT users and found 69% believe cheating det", "id": 973990, "language": "en-us", "link": "https://simonwillison.net/2024/Aug/4/watermarking/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "There\u2019s a Tool to Catch Students Cheating With ChatGPT. OpenAI Hasn\u2019t Released It.", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-08-04T16:55:33+00:00", "description": "<p><strong><a href=\"https://nicholas.carlini.com/writing/2024/how-i-use-ai.html\">How I Use &quot;AI&quot; by Nicholas Carlini</a></strong></p>\nNicholas is an author on <a href=\"https://arxiv.org/abs/2307.15043\">Universal and Transferable Adversarial Attacks on Aligned Language Models</a>, one of my favorite LLM security papers from last year. He understands the flaws in this class of technology at a deeper level than most people.</p>\n<p>Despite that, this article describes several of the many ways he still finds utility in these models in his own work:</p>\n<blockquote>\n<p>But the reason I think that the recent advances we've made aren't just hype is that, over the past year, I have spent at least a few hours every week interacting with various large language models, and have been consistently impressed by their ability to solve increasingly difficult tasks I give them. And as a result of this, I would say I'm at least 50% faster at writing code for both my research projects and my sid", "id": 973991, "language": "en-us", "link": "https://simonwillison.net/2024/Aug/4/how-i-use-ai-by-nicholas-carlini/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "How I Use \"AI\" by Nicholas Carlini", "user": null, "vote": 0}]