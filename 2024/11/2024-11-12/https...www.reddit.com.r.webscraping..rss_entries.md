# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## I can't be the only one who felt this way, right?
 - [https://www.reddit.com/r/webscraping/comments/1gpujui/i_cant_be_the_only_one_who_felt_this_way_right](https://www.reddit.com/r/webscraping/comments/1gpujui/i_cant_be_the_only_one_who_felt_this_way_right)
 - RSS feed: $source
 - date published: 2024-11-12T20:27:33+00:00

<!-- SC_OFF --><div class="md"><p>I have been beating myself over the head for not knowing about web scraping for the past couple of weeks. </p> <p>I am like there is no way it has been this simple this whole time. </p> <p>I had always assumed I had to buy ads, build an audience, or buy leads, or information. Seeing I can get thousands of free emails, phone numbers, and data for free (small cost) then automate the whole process seems stupidly simple to me. </p> <p>I guess it is one of those things, you don&#39;t know what you don&#39;t know.</p> <p>I was curious if you felt the same way, or if you have sold data you have scrapped to others.</p> <p>To me this seems simple to do, so why wouldn&#39;t others do the same?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Cin_anime"> /u/Cin_anime </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gpujui/i_cant_be_the_only_one_who_felt_this_way_right/">[link]</a></span> &#32; <span>

## Take cookies from valid session and inject them into Selenium
 - [https://www.reddit.com/r/webscraping/comments/1gps555/take_cookies_from_valid_session_and_inject_them](https://www.reddit.com/r/webscraping/comments/1gps555/take_cookies_from_valid_session_and_inject_them)
 - RSS feed: $source
 - date published: 2024-11-12T18:48:53+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m having trouble scraping a particular webpage, where I find that no matter what I try to do, I get stuck in the &quot;Verifying you&#39;re human&quot; loop. I&#39;m using Python, Selenium, and a Geckodriver for Firefox. I can&#39;t use undetected-chromedriver or derivates since they won&#39;t run on a Raspberry Pi. Faking the user-agent doesn&#39;t seem to work either.</p> <p>I&#39;m looking into how this works, and some <a href="https://community.cloudflare.com/t/verifying-you-are-human-this-may-take-a-few-seconds-couldnt-remove-it-in-any-way/718905/2">Cloudfare Team Member</a> said that they set a cookie when you pass a challenge, so I&#39;m wondering if it is worth the hassle to try and get it from a valid session, and import it into Selenium.</p> <p>Has anyone ever done something like that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/yzzidyzzid"> /u/yzzidyzzid </a> <br/> <span><a href="https://www

## Scraping a blog and converting it into a PDF
 - [https://www.reddit.com/r/webscraping/comments/1gprxhr/scraping_a_blog_and_converting_it_into_a_pdf](https://www.reddit.com/r/webscraping/comments/1gprxhr/scraping_a_blog_and_converting_it_into_a_pdf)
 - RSS feed: $source
 - date published: 2024-11-12T18:40:16+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone,</p> <p>I&#39;ve recently started exploring web scraping and am working on a project to convert a blog I really enjoy into a single PDF file, so I can read it more conveniently.</p> <p>I&#39;m using Python for this project. So far, I’ve managed to scrape the content (posts, dates, etc.) with Beautiful Soup and stored it in a JSON structure organized by year -&gt; month -&gt; post1. I then tried using ReportLab to generate a PDF from this structured content.</p> <p>It&#39;s worth noting that the blog is quite large, with over 700 posts, so there’s a significant amount of HTML content involved.</p> <p>The main issue I’m facing now is handling the HTML tags and formatting them properly in the PDF. Basic text works fine, but other tags either aren’t displaying correctly, are misplaced, or end up duplicated.</p> <p>I also tried using WeasyPrint to convert the HTML directly to PDF, but this approach isn&#39;t displaying images correctly.</p>

## 403 Error while trying to web scrape TripAdvisor reviews
 - [https://www.reddit.com/r/webscraping/comments/1gpq10v/403_error_while_trying_to_web_scrape_tripadvisor](https://www.reddit.com/r/webscraping/comments/1gpq10v/403_error_while_trying_to_web_scrape_tripadvisor)
 - RSS feed: $source
 - date published: 2024-11-12T17:23:09+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone,</p> <p>I need to scrape reviews from TripAdvisor for a personal project. I managed to get reviews for the first two or three hotels, and then I&#39;ve been running into consistent <strong>403 Forbidden errors</strong>. I&#39;ve already included switching up <strong>user agents</strong> using the <code>fake_useragent</code> library and also I made it to cycle through a list of proxies.<br/> I&#39;ll include the <strong>collab</strong> link in this post, please let me know how to fix this.</p> <p><a href="https://colab.research.google.com/drive/11JdNfOL_NTg3lpstQ7qxd1XzjoxN30Lc?usp=sharing">https://colab.research.google.com/drive/11JdNfOL_NTg3lpstQ7qxd1XzjoxN30Lc?usp=sharing</a></p> <p>TIA</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/fenaz99"> /u/fenaz99 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gpq10v/403_error_while_trying_to_web_scrape_tripadvisor/">[link]</a></sp

## Have a Shopify specific question
 - [https://www.reddit.com/r/webscraping/comments/1gpmyr5/have_a_shopify_specific_question](https://www.reddit.com/r/webscraping/comments/1gpmyr5/have_a_shopify_specific_question)
 - RSS feed: $source
 - date published: 2024-11-12T15:14:41+00:00

<!-- SC_OFF --><div class="md"><p>Not entirely sure it fits here so, excuse me if this is the wrong spot. Anyhow, a friend of mine on discord has an associate who can pull inventory numbers on items that are in the products.json file but don’t show stock in the json file. I’m curious where one would go to learn how to do that or some terms I could use to get started on the path to THAT enlightenment. Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/SoulDoubt7491"> /u/SoulDoubt7491 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gpmyr5/have_a_shopify_specific_question/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gpmyr5/have_a_shopify_specific_question/">[comments]</a></span>

## A non WS needing advice
 - [https://www.reddit.com/r/webscraping/comments/1gplati/a_non_ws_needing_advice](https://www.reddit.com/r/webscraping/comments/1gplati/a_non_ws_needing_advice)
 - RSS feed: $source
 - date published: 2024-11-12T13:57:48+00:00

<!-- SC_OFF --><div class="md"><p>Hi all,</p> <p><em>disclaimer</em> I am not an IT literate person at all. I can barely switch a pc on but I do have a lot of ideas. Any help that’s dumbed down as possible would be great</p> <p>I recently paid someone to create a tool that scrapes from one site and posts to another. Great, took them a while but he has showed me it works. Only issue is now I don’t have a clue on how to actually use the tool.</p> <p>He says he created a GUI which makes it easier for me to use but I don’t have a clue how to run the script or anything.</p> <p>He sent me a video with doing something on cmd but he’s almost non responsive at this point. Can someone tell me how I get this thing to run? It’s a python script if that helps</p> <p>I don’t know if that’s enough information but do let me know if you need any further info </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/NUFC199103"> /u/NUFC199103 </a> <br/> <span><a href="http

## For webscraping, what do i need to consider before buying a laptop?
 - [https://www.reddit.com/r/webscraping/comments/1gpkeiv/for_webscraping_what_do_i_need_to_consider_before](https://www.reddit.com/r/webscraping/comments/1gpkeiv/for_webscraping_what_do_i_need_to_consider_before)
 - RSS feed: $source
 - date published: 2024-11-12T13:12:50+00:00

<!-- SC_OFF --><div class="md"><p>Hey guys already have one which is HP probook 16GB Ram But i need another for some personal reasons. So now i was looking to buy one, please let me know what to consider or be more concerned.</p> <p>I guess for developing scripts we don need very big specs. Please suggest me. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gpkeiv/for_webscraping_what_do_i_need_to_consider_before/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gpkeiv/for_webscraping_what_do_i_need_to_consider_before/">[comments]</a></span>

## how to make headless selenium act like non-headless?
 - [https://www.reddit.com/r/webscraping/comments/1gpk52r/how_to_make_headless_selenium_act_like_nonheadless](https://www.reddit.com/r/webscraping/comments/1gpk52r/how_to_make_headless_selenium_act_like_nonheadless)
 - RSS feed: $source
 - date published: 2024-11-12T12:59:23+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m trying to scrape a couple websites using selenium (Meijer.com to start) for some various product prices to build historical data for a school project. I&#39;ve figured out how navigate to Meijer, search their page and locate the prices on the page. the problem is, I want this to just run once a day on a server and write the info to a .csv for me. So, I need to use headless.. Problem is, when I do this, Meijer.com returns a different page, and it doesn&#39;t seem to have the search bar in it. Any suggestions to get selenium to act like non-headless, but still run on my server?</p> <p>I&#39;m not doing this un-ethically, It will be one search per day for several products, no different than me doing it myself, just a computer doing it so I don&#39;t forget or waste time. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Agreeable-Bug-4901"> /u/Agreeable-Bug-4901 </a> <br/> <span><a href="https://www.reddit.c

## Feasible to scrape 500,000 different ebay products each week?
 - [https://www.reddit.com/r/webscraping/comments/1gpcmi5/feasible_to_scrape_500000_different_ebay_products](https://www.reddit.com/r/webscraping/comments/1gpcmi5/feasible_to_scrape_500000_different_ebay_products)
 - RSS feed: $source
 - date published: 2024-11-12T04:29:13+00:00

<!-- SC_OFF --><div class="md"><p>I’m relatively new to web scraping but have done small projects with Python before</p> <p>I’m currently working on an app idea that catalogs various products and retrieves average last solds. I’m estimating I’ll have about 500,000 products in my catalog</p> <p>The prices of these products are always changing so I’m want to get last sold data of each product within the past week</p> <p>Would it be feasible to have a bot set up to scrape eBay 24/7 cycling through each of the 500,000 products? If my bot cycles every week, that would mean I would need to scrape 3,000 products/hr 24/7. Is that even within the realm of possibility?</p> <p>Ideally I would like to use an API, but eBay has restricted this within their Marketplace Insights API and it seems unlikely they would give me access to it (although it would be great if they did)</p> <p>Thoughts? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/DogtorPepper"> /u/Do

## Scrape wordpress websites
 - [https://www.reddit.com/r/webscraping/comments/1gp8rhp/scrape_wordpress_websites](https://www.reddit.com/r/webscraping/comments/1gp8rhp/scrape_wordpress_websites)
 - RSS feed: $source
 - date published: 2024-11-12T01:09:41+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone, </p> <p>i want to scrape as much as possible of websites that uses wordpress as CMS, how can i do that ? </p> <p>i tried dorks but i&#39;m not getting a lot (maximum: 200 per search and i don&#39;t have a lot of dorks)</p> <p>any suggestions ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Individual_Web_7011"> /u/Individual_Web_7011 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gp8rhp/scrape_wordpress_websites/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gp8rhp/scrape_wordpress_websites/">[comments]</a></span>

