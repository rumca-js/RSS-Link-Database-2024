[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T23:21:18+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p> <p>I have a list of several hundreds of millions of different websites that I want to scrape (basically just collect the raw html as a string or whatever).</p> <p>I currently have a Python script using the simple request libraries and I just a multiprocess scrape. With 32 cores, it can scrape about 10000 websites in 20 minutes. When I monitor network, I/O and CPU usage, none seem to be a bottleneck, so I tend to think it is just the response time of each request that is capping.</p> <p>I have read somewhere that asynchronous calls could make it much faster as I don&#39;t have to wait to get a response from the request to call another website, but I find it so tricky to set up on Python, and it never seem to work (it basically hangs even with a very small amount of website).</p> <p>Is it worth digging deeper on async calls, is it really going to dramatically give me faster results? If yes, is there some Python library that makes it easier t", "id": 1431721, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ghiupt/scrape_hundreds_of_millions_of_different_websites", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Scrape hundreds of millions of different websites efficiently", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T20:34:36+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>hey everyone, i need residential proxy but most of I&#39;ve tried triggered cloudflare human verification. only brightdata pass cloudflare but it has long verification journey lol. so is there any alternative like brightdata that I can quickly start use.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Laurentjee\"> /u/Laurentjee </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghf7le/i_need_residential_proxy_that_undetected_by/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghf7le/i_need_residential_proxy_that_undetected_by/\">[comments]</a></span>", "id": 1430944, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ghf7le/i_need_residential_proxy_that_undetected_by", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "i need residential proxy that undetected by cloudflare", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T18:46:34+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I need to webscrape texts from this site <a href=\"https://codes.iccsafe.org/content/WAFC2021P1\">https://codes.iccsafe.org/content/WAFC2021P1</a>. It has lots of chapters and I need to analyze this site to get info with the text pathway or setback majorly looking for pathway and setback measurements. I am having trouble getting that. The page is accessible since i was able to extract when selecting a specific chapter. Any help?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Proud_Insurance1401\"> /u/Proud_Insurance1401 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghcr3g/webscraping_an_interactive_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghcr3g/webscraping_an_interactive_website/\">[comments]</a></span>", "id": 1431461, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ghcr3g/webscraping_an_interactive_website", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Webscraping an interactive website", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T18:31:00+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I am curious how sites like <a href=\"https://newsapi.org/\">https://newsapi.org/</a> gather all their news. I assume it&#39;s from scraping. This site claims to have <a href=\"https://newsapi.org/docs/get-started#:%7E:text=150%2C000%20news%20sources%20and%20blogs\">150,000 news sources and blogs</a>. </p> <p>My question is how tf do they even come close to this? Are they maintaining tens of thousands of unique parsing methods each slightly different to get the responses they want? In my mind, this seems impossible but I am curious to hear from people who know a lot more about this than me. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zorkidreams\"> /u/zorkidreams </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghcdsl/newsapiorg_150000_sources/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ghcdsl/newsapiorg_150000_sources/\">[comments]</a></span>", "id": 1430566, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ghcdsl/newsapiorg_150000_sources", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "newsapi.org, 150,000 sources...?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T12:54:00+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Ready pay also</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LocalConversation850\"> /u/LocalConversation850 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1gh4nxj/best_website_automation_tutorial_series/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1gh4nxj/best_website_automation_tutorial_series/\">[comments]</a></span>", "id": 1428448, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1gh4nxj/best_website_automation_tutorial_series", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Best website automation tutorial series?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T12:36:35+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m working on an automation that requires logging in with credentials to a certain site. </p> <p>The site is protected by reCAPTCHA v2 Enterprise. I&#39;m using browser automation (Selenium, undetected ChromeDriver) and have an extension for solving captcha challenges by clicking on pictures.</p> <p>I&#39;ve noticed that sometimes the extension solves the challenges correctly, and I see on the client side that there is a blue checkmark saying &quot;I&#39;m not a robot,&quot; which allows me to log in. However, when I try to log in, the site prompts a message that it wasn&#39;t able to validate the captcha solution, and if it happens again, my account will be blocked.</p> <p>When I try to log in with the same credentials manually, it succeeds. It seems like the site is performing some kind of additional server-side validation or detecting that the challenge was solved by an automated extension.</p> <p>Is anyone familiar with this kind of behavior", "id": 1431232, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1gh4c1v/passing_recaptcha_v2_enterprise", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "passing reCAPTCHA v2 Enterprise", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T04:26:24+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there any way to gets text data from Threads comments and replies?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Snoo-50498\"> /u/Snoo-50498 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggxi0g/threads_public_omments_and_reply/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggxi0g/threads_public_omments_and_reply/\">[comments]</a></span>", "id": 1426476, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggxi0g/threads_public_omments_and_reply", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Threads public omments and reply", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T03:01:11+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello and howdy, digital miners of r/webscraping!</p> <p>The moment you&#39;ve all been waiting for has arrived - it&#39;s our once-a-month, no-holds-barred, show-and-tell thread!</p> <ul> <li>Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you&#39;ve just unleashed on the world?</li> <li>Maybe you&#39;ve got a ground-breaking product in need of some intrepid testers?</li> <li>Got a secret discount code burning a hole in your pocket that you&#39;re just itching to share with our talented tribe of data extractors?</li> <li>Looking to make sure your post doesn&#39;t fall foul of the community rules and get ousted by the spam filter?</li> </ul> <p>Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!</p> <p>Just a friendly reminder, we like to keep all our self-promotion in one handy place, so any promotional posts will be kindly redirected here. Now, let&#39;s get", "id": 1426375, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggw2na/monthly_selfpromotion_november_2024", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Monthly Self-Promotion - November 2024", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-11-01T00:31:28+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I\u2019m using Playwright with Python to stay signed into Nextdoor by saving auth data to a .json file. However, I\u2019m running into an issue where one of my accounts expires every few days, while another can stay signed in for up to a month.</p> <p>The account that logs out frequently is accessed from Canada but is set to a New Jersey neighborhood, whereas the account that stays signed in is based in Canada and linked to a Canadian city. I\u2019m not sure if this location difference is causing the problem, but it seems possible.</p> <p>If anyone has any insights or suggestions, I\u2019d really appreciate it. Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/RandomFactChecker_\"> /u/RandomFactChecker_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggtbzq/cant_stay_sign_in_with_auth_json_file/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/", "id": 1425908, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggtbzq/cant_stay_sign_in_with_auth_json_file", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Can\u2019t stay sign in with auth .json file", "vote": 0}]