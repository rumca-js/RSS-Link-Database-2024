# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Advice for scraping from airline sites
 - [https://www.reddit.com/r/webscraping/comments/1gm3bn7/advice_for_scraping_from_airline_sites](https://www.reddit.com/r/webscraping/comments/1gm3bn7/advice_for_scraping_from_airline_sites)
 - RSS feed: $source
 - date published: 2024-11-07T22:39:55+00:00

<!-- SC_OFF --><div class="md"><p>Hey all,</p> <p>I am new to webscraping, not new to webdev. I have been trying to complete a project to replicate a google flights price checker for a specific airlines website. I have slowly worked my way through various anti-scraping measures they have put in place, using puppeteer with a simulated real browser package and a bunch of http interception / masking configs, stealth plugins, residential proxies, and trying to mimic human behavior for all of my parameters on inputs. </p> <p>As of now, I can search a flight successfully from the homepage about 50% of the time without getting errored out due to bot detection. I am trying to figure out if I can get this to be consistent and was looking for insight on common detection methods they use or if anybody has advice on tools to aid me in this project.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AStanfordRunner"> /u/AStanfordRunner </a> <br/> <span><a href=

## Advice for web scraping airline sites
 - [https://www.reddit.com/r/webscraping/comments/1gm3a3p/advice_for_web_scraping_airline_sites](https://www.reddit.com/r/webscraping/comments/1gm3a3p/advice_for_web_scraping_airline_sites)
 - RSS feed: $source
 - date published: 2024-11-07T22:37:59+00:00

<!-- SC_OFF --><div class="md"><p>Hey all,</p> <p>I am new to webscraping, not new to webdev. I have been trying to complete a project to replicate a google flights price checker for a specific airlines website. I have slowly worked my way through various anti-scraping measures they have put in place, using puppeteer with a simulated real browser package and a bunch of http interception / masking configs, stealth plugins, residential proxies, and trying to mimic human behavior for all of my parameters on inputs. </p> <p>As of now, I can search a flight successfully from the homepage about 50% of the time without getting errored out due to bot detection. I am trying to figure out if I can get this to be consistent and was looking for insight on common detection methods they use or if anybody has advice on tools to aid me in this project.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Many-General2688"> /u/Many-General2688 </a> <br/> <span><a hre

## We've built a browser API for web automation powered by AI
 - [https://www.reddit.com/r/webscraping/comments/1gm11x8/weve_built_a_browser_api_for_web_automation](https://www.reddit.com/r/webscraping/comments/1gm11x8/weve_built_a_browser_api_for_web_automation)
 - RSS feed: $source
 - date published: 2024-11-07T21:02:58+00:00

<!-- SC_OFF --><div class="md"><p>Hello Redditors! It’s Marcos.</p> <p>I’m thrilled to introduce <a href="https://airtop.ai">Airtop</a>, an AI-powered browser controlled by an API that is designed to automate web tasks at scale. Perfect for teams building AI agents that need to interact with websites requiring authentication like MFA or have human-in-the-loop scenarios. Airtop simplifies the process of navigating and controlling sites using natural language commands.</p> <p><strong>Key Features:</strong></p> <ul> <li><strong>Web Automation for AI:</strong> Control any website, scrape data, or automate tasks with ease.</li> <li><strong>Handles Complex Authentication:</strong> From MFA to OAuth, Airtop handles it all.</li> <li><strong>Scalable Cloud Browsers:</strong> Whether you need one browser or thousands, Airtop has you covered.</li> <li><strong>Compliant</strong>: Airtop ensures your data is handled with the highest security, availability, and privacy standards, giving you peace 

## Ideas for Projects Ideas
 - [https://www.reddit.com/r/webscraping/comments/1gm0f3b/ideas_for_projects_ideas](https://www.reddit.com/r/webscraping/comments/1gm0f3b/ideas_for_projects_ideas)
 - RSS feed: $source
 - date published: 2024-11-07T20:36:00+00:00

<!-- SC_OFF --><div class="md"><p>Not long ago, I read through a lot of threads on &quot;project ideas&quot; and &quot;your favorite project&quot;... truth is, they weren&#39;t very inspiring. As a data analyst/engineer, I&#39;m drawn to understanding reality through numbers, but I found these suggestions lacking real spark. So here are some tips from my perspective:</p> <p>When you think about what information to gather, most of us tend to think of e-commerce, price tracking, etc. — but have you thought about why and what specifically you want from this data? Ask yourself:</p> <ul> <li>What are your main interests?</li> <li>What insights are you hoping to gain?</li> <li>What would you genuinely want to know or uncover?</li> </ul> <p>Here are a few project ideas I&#39;ve come up with recently:</p> <ul> <li>I follow several YouTubers who discuss literature, history, politics… some even have their own online bookstores. So, I thought, why not scrape book titles, authors, prices, and ca

## Large scale distributed scraping help.
 - [https://www.reddit.com/r/webscraping/comments/1glvc6x/large_scale_distributed_scraping_help](https://www.reddit.com/r/webscraping/comments/1glvc6x/large_scale_distributed_scraping_help)
 - RSS feed: $source
 - date published: 2024-11-07T17:04:07+00:00

<!-- SC_OFF --><div class="md"><p>I am working on a project where I need to scrape data from government LLC websites. like below:</p> <p><a href="https://esos.nv.gov/EntitySearch/OnlineEntitySearch">https://esos.nv.gov/EntitySearch/OnlineEntitySearch</a></p> <p><a href="https://ecorp.sos.ga.gov/BusinessSearch">https://ecorp.sos.ga.gov/BusinessSearch</a></p> <p>I have bunch of such websites. Client is non-technical so I have to figure out a way how he will input the keyword and based on that keyword I will scrape data from every website and store results somewhere in the database. Almost all websites are build with ASP .Net so that is another issue for me. Making one scraper is okay but how can I manage scraping of this size. I should be able to add new websites as needed and also need some interface like API where my client can input keyword to scrape. I have proxies and captcha solver API. Needed a way or boilerplate how can i proceed with this project. I explored about distributed 

## How to tell if I'd get blocked beforehand?
 - [https://www.reddit.com/r/webscraping/comments/1gltf0t/how_to_tell_if_id_get_blocked_beforehand](https://www.reddit.com/r/webscraping/comments/1gltf0t/how_to_tell_if_id_get_blocked_beforehand)
 - RSS feed: $source
 - date published: 2024-11-07T15:44:13+00:00

<!-- SC_OFF --><div class="md"><p>For example, I&#39;m studying accounting and I&#39;m using a quiz app. On this quiz app, I can setup a pretest with 100 questions. For each question is a different webpage. After I take the quiz, I want to retrieve the question text and answer so I can study and learn it later.</p> <p>Is there anyway to tell if I would get blocked if I attempted to webscrape beforehand? For example, I could probably scrape the websites of local mom-and-pop business without issue but if I tried to scrape the websites we all know and love to try and scrape I&#39;d get blocked instantly. Is there a way to tell if a website is locked down with bot detection, etc?</p> <p>The reason I&#39;m asking this is because I paid for this service and don&#39;t want to somehow lose access to this web app.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/TotalDodd"> /u/TotalDodd </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/commen

## Recommend a Cloud Setup
 - [https://www.reddit.com/r/webscraping/comments/1glphyr/recommend_a_cloud_setup](https://www.reddit.com/r/webscraping/comments/1glphyr/recommend_a_cloud_setup)
 - RSS feed: $source
 - date published: 2024-11-07T12:40:52+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone</p> <p>I&#39;m trying to scrape 500+ websites with an average of 1.5mil total requests Once a day , about a 3rd of those will need JS rendering, didn&#39;t run anything at this scale before although its not that much or complicated either.</p> <p>skipping all the script details just looking for cloud setups best fitting this use case </p> <p>Im assuming a queue like SQS with lambdas for requests along with fargate (or are ec2 spots better) for the headless browser requests<br/> That or would you run this all on one powerful server? </p> <p>Appreciate y&#39;all!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Double-Passage-438"> /u/Double-Passage-438 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1glphyr/recommend_a_cloud_setup/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1glphyr/recommend_a_cloud_setup/">[comments]</a></span>

