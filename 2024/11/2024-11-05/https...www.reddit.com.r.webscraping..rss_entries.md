# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Application-Layer Protocol - weight impact on bot scoring
 - [https://www.reddit.com/r/webscraping/comments/1gkkbbq/applicationlayer_protocol_weight_impact_on_bot](https://www.reddit.com/r/webscraping/comments/1gkkbbq/applicationlayer_protocol_weight_impact_on_bot)
 - RSS feed: $source
 - date published: 2024-11-05T23:16:07+00:00

<!-- SC_OFF --><div class="md"><p>When making requests directly, eg. to &quot;hidden APIs&quot;, how important is the protocol used? e.g. HTTP/1, HTTP/2, HTTP/3 + QUIC</p> <p>And how much does the protocol affect the chance of getting detected or blocked?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/matty_fu"> /u/matty_fu </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gkkbbq/applicationlayer_protocol_weight_impact_on_bot/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gkkbbq/applicationlayer_protocol_weight_impact_on_bot/">[comments]</a></span>

## Visual scrape builder
 - [https://www.reddit.com/r/webscraping/comments/1gkj1j1/visual_scrape_builder](https://www.reddit.com/r/webscraping/comments/1gkj1j1/visual_scrape_builder)
 - RSS feed: $source
 - date published: 2024-11-05T22:18:43+00:00

<!-- SC_OFF --><div class="md"><p>Hi all! I am looking for the tool that will work in a following way. I have 100+ websites, each with different structure. All of them capture same data entities (1 website n entities) and I need convenient way to update the data in my db. Both SaaS and libraries in any language are fine with me. My goal is not to write the code for each website.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/zdzarsky"> /u/zdzarsky </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gkj1j1/visual_scrape_builder/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gkj1j1/visual_scrape_builder/">[comments]</a></span>

## Fake cookie
 - [https://www.reddit.com/r/webscraping/comments/1gkgrxw/fake_cookie](https://www.reddit.com/r/webscraping/comments/1gkgrxw/fake_cookie)
 - RSS feed: $source
 - date published: 2024-11-05T20:42:12+00:00

<!-- SC_OFF --><div class="md"><p>Hey guys I’ve been scraping dozens of websites if not hundreds using node as a side project. And from time to time, I encounter the following issue: - I test a request on postman without a lot of header, it is not working - so I use my browser, go the website and copy past all my header to postman to simulate the exact request (including cookie) - and it is working. </p> <p>Hence, I know I need to put some header on my code to access the website. On my code: I’m using straight Beautifulsoup so SSR or not it does not matter. But when I try to implement the exact same header, including cookie, I got an error such as: « this website uses JS, please turn on JS ». </p> <p>As I’m using headless browser, it should be ok but I still do not understand. My view is that it is linked to a cookie detection and my headless browser is not carrying the cookies to website. Since I know without cookie I got the same error on my postman I have the feeling everything is

## Advice/Help on an idea
 - [https://www.reddit.com/r/webscraping/comments/1gkgclp/advicehelp_on_an_idea](https://www.reddit.com/r/webscraping/comments/1gkgclp/advicehelp_on_an_idea)
 - RSS feed: $source
 - date published: 2024-11-05T20:23:56+00:00

<!-- SC_OFF --><div class="md"><p>Hello!</p> <p>First time poster, medium time lurker here. I&#39;ve been self learning Python for the last couple of years, mainly so I can mess around with some sports data and calculate stats for some sports sim video games I play. I&#39;ve not tried any web scraping before but wanted to get some advice on an idea I&#39;ve had as a way to start. </p> <p>I wanted to scrape the information from the schedule page for the ECHL (Hockey League) and turn it into tabular form in a simple &#39;Date&#39;, &#39;Home-Team&#39;, &#39;Home-Score&#39;, &#39;Away-Team&#39;, &#39;Away-Score&#39;, &#39; Game_Status&#39; style, so that I can calculate an &#39;Excitement Score&#39; and decide which game I want to download/watch from the last set and which I can skip over. The link to the schedule page is here - <a href="https://echl.com/schedule">https://echl.com/schedule</a></p> <p>I&#39;d managed to achieve the same thing via PowerQuery for the AHL, because it&#39;s 

## Is there a way to generate random cookies?
 - [https://www.reddit.com/r/webscraping/comments/1gkfvl8/is_there_a_way_to_generate_random_cookies](https://www.reddit.com/r/webscraping/comments/1gkfvl8/is_there_a_way_to_generate_random_cookies)
 - RSS feed: $source
 - date published: 2024-11-05T20:04:01+00:00

<!-- SC_OFF --><div class="md"><p>Hello. Good day everyone.</p> <p>I&#39;ve been running my automation software, and sometimes it gets detected. I wanna lower the chances of getting detected to 0%, ideally. I thought about a number of things, from mimicking human mouse movemen; which I&#39;m currently working on, to populating the browsing I&#39;m using with dummy data, such as cookies. I looked online and I haven&#39;t found an answer to my question.</p> <p>So I&#39;m reaching out here if anyone does what I&#39;m trying to do, I&#39;d appreciate any input!</p> <p>I can make a software that does this within a couple of days, I just wanna know a few things beforehand. Do cookies store timezone and geo-location data? Because I&#39;m obviously using proxies to change each browser&#39;s location. And I was planning on running my software to generate cookies on my main machine, so I don&#39;t wanna populate browsers on the US with cookies that were harvested in China for example..any inpu

## Web scraping script cant seem to consistently grab information
 - [https://www.reddit.com/r/webscraping/comments/1gkemxh/web_scraping_script_cant_seem_to_consistently](https://www.reddit.com/r/webscraping/comments/1gkemxh/web_scraping_script_cant_seem_to_consistently)
 - RSS feed: $source
 - date published: 2024-11-05T19:11:43+00:00

<!-- SC_OFF --><div class="md"><p>I wrote a script to web scrape my schools dining hall website. It needs to grab a good amount of information and store it in a database. It works mostly beside one hiccup that I cant see to fix. When you click on a button a modal shows up with all the information that&#39;s contained in an ul elements with multiple li elements side.<br/> I try and grab the name and the amount and put it in a dictionary but for some reason it seems to fail to grab the first few macro names. I&#39;ve tried countless different methods to try and get it but even sometimes it will work for one item and not another. It could be because the website is dynamically loaded, but I felt like I accounted for that with the waits. Any help is much appreciated, and I can always add more information if needed.</p> <p>Example of structure:<br/> &lt;ul&gt;<br/> &lt;li&gt;&lt;strong&gt; Calories: &lt;/strong&gt; 180 &lt;/li&gt;<br/> &lt;li&gt;&lt;strong&gt; Protein (g): &lt;/strong&gt; 

## Amazon keeps getting harder to scrape
 - [https://www.reddit.com/r/webscraping/comments/1gkay4j/amazon_keeps_getting_harder_to_scrape](https://www.reddit.com/r/webscraping/comments/1gkay4j/amazon_keeps_getting_harder_to_scrape)
 - RSS feed: $source
 - date published: 2024-11-05T16:38:43+00:00

<!-- SC_OFF --><div class="md"><p>Is it just me, or is Amazon&#39;s bot detection getting way tighter. Even on my actual laptop and browser, I get a captcha if I visit while not logged in. </p> <p>Has anyone found good solutions for getting past?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ZMech"> /u/ZMech </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gkay4j/amazon_keeps_getting_harder_to_scrape/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gkay4j/amazon_keeps_getting_harder_to_scrape/">[comments]</a></span>

## what is the best "AI" web scraping tool that you use?
 - [https://www.reddit.com/r/webscraping/comments/1gkacaa/what_is_the_best_ai_web_scraping_tool_that_you_use](https://www.reddit.com/r/webscraping/comments/1gkacaa/what_is_the_best_ai_web_scraping_tool_that_you_use)
 - RSS feed: $source
 - date published: 2024-11-05T16:13:24+00:00

<!-- SC_OFF --><div class="md"><p>there are so many new AI tools that say they can web scrape but most of them provide subpar result. which ones have you used that have been the best to work with? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/mrnzt"> /u/mrnzt </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gkacaa/what_is_the_best_ai_web_scraping_tool_that_you_use/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gkacaa/what_is_the_best_ai_web_scraping_tool_that_you_use/">[comments]</a></span>

## Rate My Professors Dataset
 - [https://www.reddit.com/r/webscraping/comments/1gka68d/rate_my_professors_dataset](https://www.reddit.com/r/webscraping/comments/1gka68d/rate_my_professors_dataset)
 - RSS feed: $source
 - date published: 2024-11-05T16:06:17+00:00

<!-- SC_OFF --><div class="md"><p>Hi, I&#39;m wondering if anyone&#39;s been able to contact this guy about the 5G RMP dataset:</p> <p><a href="https://data.mendeley.com/datasets/fvtfjyvw7d/2">https://data.mendeley.com/datasets/fvtfjyvw7d/2</a></p> <p>I only sent my email about 10 mins ago but scared I won&#39;t get a response. Anyone have direct access and could help me out?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/nelariddle"> /u/nelariddle </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gka68d/rate_my_professors_dataset/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gka68d/rate_my_professors_dataset/">[comments]</a></span>

## How to export an entire g-contact directory without scripting?
 - [https://www.reddit.com/r/webscraping/comments/1gk7uv6/how_to_export_an_entire_gcontact_directory](https://www.reddit.com/r/webscraping/comments/1gk7uv6/how_to_export_an_entire_gcontact_directory)
 - RSS feed: $source
 - date published: 2024-11-05T14:24:13+00:00

<!-- SC_OFF --><div class="md"><p>I need to extract/export a google contacts directory but without any scripting or coding as I suck at it. I know it&#39;s possible just don&#39;t know how!!!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Cicada_ace"> /u/Cicada_ace </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gk7uv6/how_to_export_an_entire_gcontact_directory/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gk7uv6/how_to_export_an_entire_gcontact_directory/">[comments]</a></span>

## Web scraping in less than 2 minutes.
 - [https://www.reddit.com/r/webscraping/comments/1gk7txz/web_scraping_in_less_than_2_minutes](https://www.reddit.com/r/webscraping/comments/1gk7txz/web_scraping_in_less_than_2_minutes)
 - RSS feed: $source
 - date published: 2024-11-05T14:23:01+00:00

<!-- SC_OFF --><div class="md"><p>Hello, I&#39;m trying to understand the web scraping / data extraction market and you could be of great help.</p> <p>As per my knowledge, the current processes are very manual &amp; daunting for even the simplest data extraction needs out of a simple website.</p> <p>What if you could:</p> <ol> <li>Enter the URL of the website you&#39;d like the data from.</li> <li>Enter the schema of data (describing it in plain English)</li> <li>Get the extracted data within 2 minutes in various different formats (CSV, JSON, etc.)</li> </ol> <p>Is that something you see yourself using?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/nightmayz"> /u/nightmayz </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gk7txz/web_scraping_in_less_than_2_minutes/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gk7txz/web_scraping_in_less_than_2_minutes/">[comments]</a></span>

## Starting a SaaS with Web Scraping and Data Collection. Whats legal?
 - [https://www.reddit.com/r/webscraping/comments/1gjxbaz/starting_a_saas_with_web_scraping_and_data](https://www.reddit.com/r/webscraping/comments/1gjxbaz/starting_a_saas_with_web_scraping_and_data)
 - RSS feed: $source
 - date published: 2024-11-05T03:17:46+00:00

<!-- SC_OFF --><div class="md"><p>Hello, </p> <p>I’m working on building a SaaS product that revolves around web scraping for data collection—aiming to provide valuable insights for businesses and individuals alike. I think there’s a ton of potential in helping people get access to useful data, but I’ve noticed that many user agreements specifically restrict web scraping, for example:</p> <blockquote> </blockquote> <p>It raises some questions for me about the exact boundaries here. Specifically:</p> <ul> <li>What are some ethical ways to gather data while respecting these agreements?</li> <li>Are there alternatives, like public APIs, that allow similar data access without breaching terms?</li> <li>What are the potential risks if a company does decide to ignore these restrictions?</li> </ul> <p>I’d love to hear from others who’ve dealt with these challenges. How do you manage the balance between collecting useful data and staying within ethical and legal limits? And are there ways to 

## Why am I able to do this:
 - [https://www.reddit.com/r/webscraping/comments/1gjuw7m/why_am_i_able_to_do_this](https://www.reddit.com/r/webscraping/comments/1gjuw7m/why_am_i_able_to_do_this)
 - RSS feed: $source
 - date published: 2024-11-05T01:14:42+00:00

<!-- SC_OFF --><div class="md"><p>Interning at a startup. For various reasons we need to scrape this huge website. I found out that I can basically copy the network req headers and make requests through basic Python program with requests library. Why does this work and why does it allow me to do it? I’m able to do it with no tracking info provided(no cookies or anything), so how do they enforce the no scraping thing since this part of the site isn’t allowed by their robots.txt? </p> <p>I’m assuming they can still track the origin of the request (maybe IP?)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/bruhidk123345"> /u/bruhidk123345 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gjuw7m/why_am_i_able_to_do_this/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gjuw7m/why_am_i_able_to_do_this/">[comments]</a></span>

