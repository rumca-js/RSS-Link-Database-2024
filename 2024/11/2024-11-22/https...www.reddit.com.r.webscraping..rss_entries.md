# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Recommend me a course thats related to these…
 - [https://www.reddit.com/r/webscraping/comments/1gxh4vw/recommend_me_a_course_thats_related_to_these](https://www.reddit.com/r/webscraping/comments/1gxh4vw/recommend_me_a_course_thats_related_to_these)
 - RSS feed: $source
 - date published: 2024-11-22T20:12:10+00:00

<!-- SC_OFF --><div class="md"><ol> <li>Anti bot best practices</li> <li>webscrape and automation</li> </ol> <p>I just went through youtube udemy course era I still could not find a good teacher. Im ready pay also.</p> <p>Please help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gxh4vw/recommend_me_a_course_thats_related_to_these/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gxh4vw/recommend_me_a_course_thats_related_to_these/">[comments]</a></span>

## 2 Years of Introducing Web Scraping Service in my Analytics Agency
 - [https://www.reddit.com/r/webscraping/comments/1gxh2mb/2_years_of_introducing_web_scraping_service_in_my](https://www.reddit.com/r/webscraping/comments/1gxh2mb/2_years_of_introducing_web_scraping_service_in_my)
 - RSS feed: $source
 - date published: 2024-11-22T20:09:27+00:00

<!-- SC_OFF --><div class="md"><p>Today here to share some experience of mine! </p> <p>Almost 6 years ago I started my analytics consulting right after graduating from college. I was good with programming and statistics. </p> <p>Slowly I started to collaborate with teams and introduced many services including web scraping in our agency. When I started, I was charging very less like I worked at $75 project which took a full day for scraping 5000 product pages. </p> <p>Slowly clients&#39; requests started to make us more busy and our rates went up. </p> <p>Majority of clients of mine come from <em>real estate</em> and <em>e-commerce</em>. Some academic clients also reached us for collecting data from weird sources! </p> <p>Some months our hourly rate shoot up to $120/hr. But mostly we fix the prices depending on projects. In last year 40% of our projects were around web scraping. </p> <p>Now we expanding and providing end to end solutions from scraping to analysis and then dashboards t

## Making get request from headless browser
 - [https://www.reddit.com/r/webscraping/comments/1gxfhff/making_get_request_from_headless_browser](https://www.reddit.com/r/webscraping/comments/1gxfhff/making_get_request_from_headless_browser)
 - RSS feed: $source
 - date published: 2024-11-22T19:01:50+00:00

<!-- SC_OFF --><div class="md"><p>Hello,</p> <p>Using python, is it possible to pull up a target website, and use the same session to make get requests through the browser? I know how to intercept the network requests, but that&#39;s only what&#39;s loaded when you first load the page. I was wondering if you could make subsequent get requests such that it would look more like normal browser traffic and utilize the same cookies and such. This is a cloudflare protected API (cr_clearance cookies, etc.). </p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/for_dinnerz"> /u/for_dinnerz </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gxfhff/making_get_request_from_headless_browser/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gxfhff/making_get_request_from_headless_browser/">[comments]</a></span>

## Google Maps scraper to get addresses and geolocation?
 - [https://www.reddit.com/r/webscraping/comments/1gxddfh/google_maps_scraper_to_get_addresses_and](https://www.reddit.com/r/webscraping/comments/1gxddfh/google_maps_scraper_to_get_addresses_and)
 - RSS feed: $source
 - date published: 2024-11-22T17:34:07+00:00

<!-- SC_OFF --><div class="md"><p>Hey <a href="/r/webscraping">r/webscraping</a>, </p> <p>I made a python script to return addresses and geolocations for a list of hospitals I am doing research on, but I&#39;m not sure how to scrape maps without a google api? If anyone has any tips please lmk. I&#39;m new to coding so sorry if this seems trivial </p> <pre><code>import csv import requests from bs4 import BeautifulSoup def get_hospital_info(hospital_name): search_url = f&quot;https://www.google.com/maps/search/{hospital_name.replace(&#39; &#39;, &#39;+&#39;)}&quot; headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3&quot; } response = requests.get(search_url, headers=headers) if response.status_code != 200: return &#39;Error fetching data&#39;, &#39;Error fetching data&#39;, &#39;Error fetching data&#39; soup = BeautifulSoup(response.text, &#39;html.parser&#39;) address = soup.find(

## Yelp Scraper
 - [https://www.reddit.com/r/webscraping/comments/1gxdars/yelp_scraper](https://www.reddit.com/r/webscraping/comments/1gxdars/yelp_scraper)
 - RSS feed: $source
 - date published: 2024-11-22T17:31:07+00:00

<!-- SC_OFF --><div class="md"><p>Let&#39;s say i scrape 240 businesses in los angeles from yelp with yelp data scraper, and then i want to continue to scrape from los angeles without duplicates, how do i do it?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Embarrassed-Box-9911"> /u/Embarrassed-Box-9911 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gxdars/yelp_scraper/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gxdars/yelp_scraper/">[comments]</a></span>

## Is there any way to get Binance news before they appear officially?
 - [https://www.reddit.com/r/webscraping/comments/1gxalij/is_there_any_way_to_get_binance_news_before_they](https://www.reddit.com/r/webscraping/comments/1gxalij/is_there_any_way_to_get_binance_news_before_they)
 - RSS feed: $source
 - date published: 2024-11-22T15:37:24+00:00

<!-- SC_OFF --><div class="md"><p>Hello guys!</p> <p>I need some help with getting data on Binance news before they appear officialy. Just like some telegram-channels do.</p> <p>Such as: <a href="https://t.me/coin_listing">https://t.me/coin_listing</a> or <a href="https://t.me/BWEnews">https://t.me/BWEnews</a></p> <p>Apparently Binance publishes cached news on announcement pages of Latest activities and Cryptocurrency listing.</p> <p>Thus there is some delay between appearing on some “invisible” source and the public announcement page.</p> <p>So are there any solutions to get the data early? Maybe API, cache bypass etc?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Impossible-Ad9043"> /u/Impossible-Ad9043 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gxalij/is_there_any_way_to_get_binance_news_before_they/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gxalij/is_there_any_way_t

## I made a Chrome Extension that automates & scrapes data directly in your browser using AI
 - [https://www.reddit.com/r/webscraping/comments/1gx966w/i_made_a_chrome_extension_that_automates_scrapes](https://www.reddit.com/r/webscraping/comments/1gx966w/i_made_a_chrome_extension_that_automates_scrapes)
 - RSS feed: $source
 - date published: 2024-11-22T14:33:23+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1gx966w/i_made_a_chrome_extension_that_automates_scrapes/"> <img src="https://external-preview.redd.it/M2l0eTExNzVwZzJlMavvpx57SWgUtaJe5jvSgzog7v4DGi5sVUqnZuYr5-X6.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2eb73258bcdfec823b1dcfa1a9fea7abedd86cdb" alt="I made a Chrome Extension that automates &amp; scrapes data directly in your browser using AI" title="I made a Chrome Extension that automates &amp; scrapes data directly in your browser using AI" /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/ducanchaocacban"> /u/ducanchaocacban </a> <br/> <span><a href="https://v.redd.it/4j4at855pg2e1">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gx966w/i_made_a_chrome_extension_that_automates_scrapes/">[comments]</a></span> </td></tr></table>

## Someone knows about a chromium build hard to fingerprint?
 - [https://www.reddit.com/r/webscraping/comments/1gx60x2/someone_knows_about_a_chromium_build_hard_to](https://www.reddit.com/r/webscraping/comments/1gx60x2/someone_knows_about_a_chromium_build_hard_to)
 - RSS feed: $source
 - date published: 2024-11-22T11:49:28+00:00

<!-- SC_OFF --><div class="md"><p>sometime ago i saw in some place i forgot a chromium build that can change fingerprint on every reload, i totally forgot where i saw this, and the AI articles bloat the search engines.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Amazing-Exit-1473"> /u/Amazing-Exit-1473 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gx60x2/someone_knows_about_a_chromium_build_hard_to/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gx60x2/someone_knows_about_a_chromium_build_hard_to/">[comments]</a></span>

## Made a declarative library for scraping Transfermarkt
 - [https://www.reddit.com/r/webscraping/comments/1gx4d5s/made_a_declarative_library_for_scraping](https://www.reddit.com/r/webscraping/comments/1gx4d5s/made_a_declarative_library_for_scraping)
 - RSS feed: $source
 - date published: 2024-11-22T09:55:30+00:00

<!-- SC_OFF --><div class="md"><p>Hi there, I have been developing and using this package to speed up a few personal projects involving the extraction of data from Transfermarkt and I thought I could share it. The library provides a declarative interface that eases the search and retrieval of data and allows basic querying of TM&#39;s content, I intend to expand and improve it if there is some interest, all feedback is welcome</p> <p><a href="https://github.com/franz38/tmquery">https://github.com/franz38/tmquery</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/adipiscing_elit"> /u/adipiscing_elit </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gx4d5s/made_a_declarative_library_for_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gx4d5s/made_a_declarative_library_for_scraping/">[comments]</a></span>

## I made a docker image, should I put it on Github?
 - [https://www.reddit.com/r/webscraping/comments/1gx1r9c/i_made_a_docker_image_should_i_put_it_on_github](https://www.reddit.com/r/webscraping/comments/1gx1r9c/i_made_a_docker_image_should_i_put_it_on_github)
 - RSS feed: $source
 - date published: 2024-11-22T06:40:41+00:00

<!-- SC_OFF --><div class="md"><p>Not sure if anyone else finds this useful. Please tell me. </p> <p>What it does:</p> <p>It allows you to programmatically fetch valid cookies that allow you access to sites that are protected by Cloudflare etc. </p> <p>This is how it works:</p> <p>The image only runs briefly. You run it and provide it a URL. </p> <p>A headful normal Chrome browser starts up that opens the URL. Server does not see anything suspicious and return page with normal cookies. </p> <p>After the page has loaded, Playwright connects to the running browser instance. </p> <p>Playwright then loads the same URL again, the browser will send the same valid cookies that it has saved.</p> <p>If this second request is also successful, the cookies are saved in a file so that they can be used to connect to this site from another script/scraper.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/stephan85"> /u/stephan85 </a> <br/> <span><a href="https:/

