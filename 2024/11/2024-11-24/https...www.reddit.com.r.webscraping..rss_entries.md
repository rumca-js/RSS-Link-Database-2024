# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## How to use 2captcha solver
 - [https://www.reddit.com/r/webscraping/comments/1gz0guv/how_to_use_2captcha_solver](https://www.reddit.com/r/webscraping/comments/1gz0guv/how_to_use_2captcha_solver)
 - RSS feed: $source
 - date published: 2024-11-24T20:21:02+00:00

<!-- SC_OFF --><div class="md"><p>I just paid 3$ for i guess 1000 captcha solves, I watch a YT video and then decided to buy this,</p> <p>But it requires site key i dont know where to get it, when i try to find it in source there nothing lik ‚Äòsitekey‚Äô Can anyone please help.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gz0guv/how_to_use_2captcha_solver/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gz0guv/how_to_use_2captcha_solver/">[comments]</a></span>

## How to build a residential proxy network?
 - [https://www.reddit.com/r/webscraping/comments/1gz0c21/how_to_build_a_residential_proxy_network](https://www.reddit.com/r/webscraping/comments/1gz0c21/how_to_build_a_residential_proxy_network)
 - RSS feed: $source
 - date published: 2024-11-24T20:15:25+00:00

<!-- SC_OFF --><div class="md"><p>Can anyone help me understand what tools/software already exist that could help me in building a residential proxy network? I have access to residential nodes (say 10-20) and I want to connect them to some public API/gateway such that a client can make a single HTTP/S request to that gateway and have it route through one of the residential nodes. Things to consider: </p> <p>* Residential nodes are behind routes/NAT so they can&#39;t expose ports publicly. </p> <p>* The gateway would have to be hosted somewhere e.g AWS. Maybe there&#39;s already a commercial service that allows me to connect my own nodes to it? And it just routes traffic to those nodes. </p> <p>* My goal: Looking to significantly reduce the costs of routing traffic through residential proxies ( running/owning the nodes myself is the best way to do that). Also just curious to understand ways to implement this. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.redd

## Hoping to contact every city in the US...
 - [https://www.reddit.com/r/webscraping/comments/1gyuka6/hoping_to_contact_every_city_in_the_us](https://www.reddit.com/r/webscraping/comments/1gyuka6/hoping_to_contact_every_city_in_the_us)
 - RSS feed: $source
 - date published: 2024-11-24T16:14:52+00:00

<!-- SC_OFF --><div class="md"><p>Has anyone tried emailing every city/town in the US? I&#39;m looking for an email list/database I can use, buy, or scrape, to contact cities across the US. Any recommendations?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/UnionizedBee"> /u/UnionizedBee </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gyuka6/hoping_to_contact_every_city_in_the_us/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gyuka6/hoping_to_contact_every_city_in_the_us/">[comments]</a></span>

## Puppeteer unreliable, frequently crashes the process with exception
 - [https://www.reddit.com/r/webscraping/comments/1gymdey/puppeteer_unreliable_frequently_crashes_the](https://www.reddit.com/r/webscraping/comments/1gymdey/puppeteer_unreliable_frequently_crashes_the)
 - RSS feed: $source
 - date published: 2024-11-24T08:31:42+00:00

<!-- SC_OFF --><div class="md"><p>Just wondering if there&#39;s any way to prevent Puppeteer from crashing the node.js process?</p> <p>With all sort of nonsense like &quot;Execution context was destroyed, most likely because of a navigation.&quot; etc. </p> <p>Usually, you can handle errors as <code>try { await somefn() } catch (e) { handle(e) }</code>. Not in Puppeteer, it manages to escape try/catch and crash the whole proceess, I guess with the dangling Promise or something like that.</p> <p>So far I found the only way to make it work reliably is to watch and restart <code>node.js</code> process when it crashes.</p> <p>Wonder, if anyone found a way to handle it better? Prevent puppeteer escaping try/catches and crashing node.js process randomly?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/h234sd"> /u/h234sd </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gymdey/puppeteer_unreliable_frequently_crashes_the/">[link]<

## A little project of mine
 - [https://www.reddit.com/r/webscraping/comments/1gyl0dx/a_little_project_of_mine](https://www.reddit.com/r/webscraping/comments/1gyl0dx/a_little_project_of_mine)
 - RSS feed: $source
 - date published: 2024-11-24T06:54:44+00:00

<!-- SC_OFF --><div class="md"><p>Hii! So I&#39;ve been getting into web scraping and data visualisation I think I want to specialize in this field and do as my freelancing path... So I&#39;m kinda here looking for opinions on the project I&#39;ve been working on it&#39;s called eventHive and it&#39;s a google chrome extension that records events on dom and lets you extract them... Here is a link to the github repo <a href="https://github.com/slavkomirkovic-1/EventHive-Public">https://github.com/slavkomirkovic-1/EventHive-Public</a></p> <p>The question I have is, Can you guys give me a little bit more ideas? I&#39;ve been thinking of making it a standalone thing and not a google chrome extension, like maybe use electron or something? and you know make it more pretty and give more functionality like an ability to make your own plugins and stuff like that so just trying to get your opinions and try to catch potential clients hehe üòÅ anyway looking forward to hearing from yall </p> </div

## Data scraping (not a data analyst)
 - [https://www.reddit.com/r/webscraping/comments/1gyhnj8/data_scraping_not_a_data_analyst](https://www.reddit.com/r/webscraping/comments/1gyhnj8/data_scraping_not_a_data_analyst)
 - RSS feed: $source
 - date published: 2024-11-24T03:27:13+00:00

<!-- SC_OFF --><div class="md"><p>I have this project where I am asked to scrape some data about two companies. Can someone help me with understanding how do I start? I‚Äôm completely clueless about these things- would someone be okay to provide me some guidance? Thank you! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/meditatingpiggy"> /u/meditatingpiggy </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gyhnj8/data_scraping_not_a_data_analyst/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gyhnj8/data_scraping_not_a_data_analyst/">[comments]</a></span>

## curl_cffi - getting exceptions when scraping
 - [https://www.reddit.com/r/webscraping/comments/1gydyg4/curl_cffi_getting_exceptions_when_scraping](https://www.reddit.com/r/webscraping/comments/1gydyg4/curl_cffi_getting_exceptions_when_scraping)
 - RSS feed: $source
 - date published: 2024-11-24T00:14:51+00:00

<!-- SC_OFF --><div class="md"><p>I am scraping a sports website. Previously i was using the basic request library in python, but was recommended to use curl_ciffi by the community. I am following best practices for scraping 1. Mobile rotating proxy 2. random sleeps 3. Avoid pounding server. 4. rotate who i impersonate (i.e diff user agents) 5. implement retries</p> <p>I have also previously already scraped a bunch of data, so my gut is these issues are arising from curl_cffi. Below i have listed 2 of the errors that keep arising. Does anyone have any idea how i can avoid these errors? Part of me is wondering if i should disable SSL cert valiadtion.</p> <pre><code>curl_cffi.requests.exceptions.ProxyError: Failed to perform, curl: (56) CONNECT tunnel failed, response 522. See https://curl.se/libcurl/c/libcurl-errors.html first for more details. curl_cffi.requests.exceptions.SSLError: Failed to perform, curl: (35) BoringSSL: error:1e000065:Cipher functions:OPENSSL_internal:BAD_DECRYPT.

