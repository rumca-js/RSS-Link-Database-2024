# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Why does the Wikidata information of cities not contain its country?
 - [https://www.reddit.com/r/webscraping/comments/1glaoc4/why_does_the_wikidata_information_of_cities_not](https://www.reddit.com/r/webscraping/comments/1glaoc4/why_does_the_wikidata_information_of_cities_not)
 - RSS feed: $source
 - date published: 2024-11-06T22:11:40+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m trying to query the wikidata of certain Canadian cities, and I&#39;ve noticed some strange things. I&#39;m requesting the data in JSON format, and the data extracted contains the various names of the city, and stuff like &quot;citydistrict&quot;.</p> <p>For example, if i try querying &quot;Toronto&quot; (Code: Q919169), I am getting 2 strings:</p> <blockquote> <p>&quot;OldToronto&quot;<br/> &quot;citydistrict&quot;</p> </blockquote> <p>But it nowhere says that its a city of Canada. I cannot find the Canada keyword anywhere.</p> <p>Isn&#39;t that pretty important? For it to mention that Toronto exists within Canada?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/SubzeroCola"> /u/SubzeroCola </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1glaoc4/why_does_the_wikidata_information_of_cities_not/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1g

## Bypassing Queue-It and Akamai
 - [https://www.reddit.com/r/webscraping/comments/1gl94n7/bypassing_queueit_and_akamai](https://www.reddit.com/r/webscraping/comments/1gl94n7/bypassing_queueit_and_akamai)
 - RSS feed: $source
 - date published: 2024-11-06T21:05:59+00:00

<!-- SC_OFF --><div class="md"><p>I am hoping to purchase tickets to an event next week and there will be a Queue-It system implemented.</p> <p>Is there a way to bypass the queue-it and access the website directly without being redirected? Potentially amending the JavaScript? </p> <p>Akamai will also be implemented by the </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Ok-Alarm363"> /u/Ok-Alarm363 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gl94n7/bypassing_queueit_and_akamai/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gl94n7/bypassing_queueit_and_akamai/">[comments]</a></span>

## Defeating Captchas
 - [https://www.reddit.com/r/webscraping/comments/1gl63vl/defeating_captchas](https://www.reddit.com/r/webscraping/comments/1gl63vl/defeating_captchas)
 - RSS feed: $source
 - date published: 2024-11-06T18:59:56+00:00

<!-- SC_OFF --><div class="md"><p>What tools/services/options are there for defeating captchas while scraping?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/dca12345"> /u/dca12345 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gl63vl/defeating_captchas/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gl63vl/defeating_captchas/">[comments]</a></span>

## Old and obscure manga hosting site is shutting down
 - [https://www.reddit.com/r/webscraping/comments/1gl2uik/old_and_obscure_manga_hosting_site_is_shutting](https://www.reddit.com/r/webscraping/comments/1gl2uik/old_and_obscure_manga_hosting_site_is_shutting)
 - RSS feed: $source
 - date published: 2024-11-06T16:44:18+00:00

<!-- SC_OFF --><div class="md"><p>So guys, I made this post on <a href="/r/DataHoarder">r/DataHoarder</a> as well. But I feel like you a people can do something about this issue. Yeah it&#39;s really an issue if not a great emergency for people who are interested in anime and manga. And I believe this sub has a significant number of those people. So this website <a href="https://www.mangaz.com/">https://www.mangaz.com/</a> is closing down on 26th November. The thing which makes this site really important is that it hosts a plethora of manga which are either out of print or really obscure. As a result of which this site has been serving scanlation communities or hardcore otakus for nearly a decade for they can experience non-mainstream titles which are nearly impossible to find even if somebody is ready to buy them. So you guys can imagine that those really cool titles all the way from 70s to 2010s will become lost media forever if this site goes down before someone backing up those. 

## Shopee scraping
 - [https://www.reddit.com/r/webscraping/comments/1gkyag6/shopee_scraping](https://www.reddit.com/r/webscraping/comments/1gkyag6/shopee_scraping)
 - RSS feed: $source
 - date published: 2024-11-06T13:18:33+00:00

<!-- SC_OFF --><div class="md"><p>Shopee wont let me in without login and I want to send 200 requests per second. Does anyone have an idea on how to break shopee?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Healthy-Educator-289"> /u/Healthy-Educator-289 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gkyag6/shopee_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gkyag6/shopee_scraping/">[comments]</a></span>

## Fake cookie
 - [https://www.reddit.com/r/webscraping/comments/1gklohp/fake_cookie](https://www.reddit.com/r/webscraping/comments/1gklohp/fake_cookie)
 - RSS feed: $source
 - date published: 2024-11-06T00:19:53+00:00

<!-- SC_OFF --><div class="md"><p>Hey guys I’ve been scraping dozens of websites if not hundreds using node as a side project. And from time to time, I encounter the following issue: - I test a request on postman without a lot of header, it is not working - so I use my browser, go the website and copy past all my header to postman to simulate the exact request (including cookie) - and it is working. </p> <p>Hence, I know I need to put some header on my code to access the website. On my code: I’m using straight Beautifulsoup so SSR or not it does not matter. But when I try to implement the exact same header, including cookie, I got an error such as: « this website uses JS, please turn on JS ». </p> <p>As I’m using headless browser, it should be ok but I still do not understand. My view is that it is linked to a cookie detection and my headless browser is not carrying the cookies to website. Since I know without cookie I got the same error on my postman I have the feeling everything is

## Can't have too many scrapers.
 - [https://www.reddit.com/r/webscraping/comments/1gklbvk/cant_have_too_many_scrapers](https://www.reddit.com/r/webscraping/comments/1gklbvk/cant_have_too_many_scrapers)
 - RSS feed: $source
 - date published: 2024-11-06T00:02:39+00:00

<!-- SC_OFF --><div class="md"><p>Currently scraping some comments about the election. Wouldn&#39;t mind for some of you to also send a few threads my way:</p> <p><a href="https://github.com/UnrelatedWorks/24comments">https://github.com/UnrelatedWorks/24comments</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/UnrelatedReddit"> /u/UnrelatedReddit </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gklbvk/cant_have_too_many_scrapers/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gklbvk/cant_have_too_many_scrapers/">[comments]</a></span>

