# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Perimeterx againâ€¦
 - [https://www.reddit.com/r/webscraping/comments/1gsqppb/perimeterx_again](https://www.reddit.com/r/webscraping/comments/1gsqppb/perimeterx_again)
 - RSS feed: $source
 - date published: 2024-11-16T16:18:16+00:00

<!-- SC_OFF --><div class="md"><p>How difficult is it to keep bypassing PerimeterX automated? And what is the best way? Iâ€™m so tired of trying, and using a proxy is not enough. I need to scrape 24/7, but I keep getting blocked over and over.</p> <p>Please ðŸ˜•ðŸ˜¥</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Few_Inevitable_7333"> /u/Few_Inevitable_7333 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gsqppb/perimeterx_again/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gsqppb/perimeterx_again/">[comments]</a></span>

## Framework that can screencapture actual browser errors like https://expired.badssl.com/
 - [https://www.reddit.com/r/webscraping/comments/1gspj1t/framework_that_can_screencapture_actual_browser](https://www.reddit.com/r/webscraping/comments/1gspj1t/framework_that_can_screencapture_actual_browser)
 - RSS feed: $source
 - date published: 2024-11-16T15:23:28+00:00

<!-- SC_OFF --><div class="md"><p>Hello. I&#39;m looking for advice how to accomplish ALL of the following:</p> <ol> <li>Open Chrome to: <a href="https://expired.badssl.com/">https://expired.badssl.com/</a></li> <li>Screen-capture the expected browser error, then screencapture the interior of the browser. <ol> <li>also acceptable: the entire browser UI window with the page, OR a desktop rectangle, OR the whole desktop area if we have to.</li> </ol></li> </ol> <p>The use case involves intentionally breaking the browser connection to the website (think: parental controls and HTTPS blocking), <strong>then capturing</strong> <strong><em>the Chrome &quot;SSL error&quot; the end-user sees</em></strong> <strong>as &quot;evidence&quot;.</strong> </p> <p>To be clear, screenshot of the browser error is the actual Requirement. There&#39;s already some test automation that validates the end to end functionality (but w/o screenshotting). </p> <p>Webdriver does not support capturing errors drawn b

## Is it possible to run geckodriver on Google colab?
 - [https://www.reddit.com/r/webscraping/comments/1gsp005/is_it_possible_to_run_geckodriver_on_google_colab](https://www.reddit.com/r/webscraping/comments/1gsp005/is_it_possible_to_run_geckodriver_on_google_colab)
 - RSS feed: $source
 - date published: 2024-11-16T14:58:34+00:00

<!-- SC_OFF --><div class="md"><p>I am trying to run geckodriver with selenium on Google colab but it&#39;s giving me an error. I can run chromedriver without any error but it&#39;s unable to scrape some websites for some reason.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Nikovlod445"> /u/Nikovlod445 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gsp005/is_it_possible_to_run_geckodriver_on_google_colab/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gsp005/is_it_possible_to_run_geckodriver_on_google_colab/">[comments]</a></span>

## Help with selenium webdriver stacktrace error
 - [https://www.reddit.com/r/webscraping/comments/1gsl6sb/help_with_selenium_webdriver_stacktrace_error](https://www.reddit.com/r/webscraping/comments/1gsl6sb/help_with_selenium_webdriver_stacktrace_error)
 - RSS feed: $source
 - date published: 2024-11-16T11:15:05+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1gsl6sb/help_with_selenium_webdriver_stacktrace_error/"> <img src="https://preview.redd.it/xqjxcrity81e1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc3875bb01f98c1193f9e22113c7b92eadc96173" alt="Help with selenium webdriver stacktrace error" title="Help with selenium webdriver stacktrace error" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hi everyone! I&#39;m encountering an error when trying to click on a link using selenium webdriver. The error stacktrace looks something like this. I&#39;ve checked the usual things like webdriver compatibility with the browser, and the path to the webdriver executable, but I&#39;m still getting this error.</p> <p>Has anyone encountered something similar? Any advice on what could be causing this or how to resolve it?</p> <p>Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/mariamt21"> /u/mariamt21 </a> <br/> <

## getting instant 403
 - [https://www.reddit.com/r/webscraping/comments/1gsgp6b/getting_instant_403](https://www.reddit.com/r/webscraping/comments/1gsgp6b/getting_instant_403)
 - RSS feed: $source
 - date published: 2024-11-16T05:51:49+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m trying to access this site (<a href="https://www.musicmagpie.co.uk/store/products/tom-clancy-s-splinter-cell-xbox">https://www.musicmagpie.co.uk/store/products/tom-clancy-s-splinter-cell-xbox</a>) as an example.</p> <p>I copy as many of my real http headers that my library can support except the cookie field as I don&#39;t know what goes there. I included accept, accept lang, user agent, referrer.</p> <p>Could the lack of the coookie request header be the reason of the 403? How can I generate a legitimate cookie to use?</p> <p>I&#39;m a total novice to web stuff in general so learning as I go along. Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Fuarkistani"> /u/Fuarkistani </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gsgp6b/getting_instant_403/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gsgp6b/getting_instant_403/">[comments

## Most effective way to monitor prices of producsts
 - [https://www.reddit.com/r/webscraping/comments/1gsauz2/most_effective_way_to_monitor_prices_of_producsts](https://www.reddit.com/r/webscraping/comments/1gsauz2/most_effective_way_to_monitor_prices_of_producsts)
 - RSS feed: $source
 - date published: 2024-11-16T00:16:51+00:00

<!-- SC_OFF --><div class="md"><p>Im currenty monitoring a site and there products im saving them in a csv file and the code runs every hour. My current setup is its just going through the category urls scraping the data then updating the csv, is there any faster way to monitor products for example monitor every product in the csv at once? instead of going product by product etc</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Organic_Way_3597"> /u/Organic_Way_3597 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gsauz2/most_effective_way_to_monitor_prices_of_producsts/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gsauz2/most_effective_way_to_monitor_prices_of_producsts/">[comments]</a></span>

