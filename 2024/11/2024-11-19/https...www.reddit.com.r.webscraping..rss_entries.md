# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## I scraped 50,000+ remote jobs into a googlesheet - it's free. you're welcome. upvote so everyone sees it <3
 - [https://www.reddit.com/r/webscraping/comments/1gv8fd8/i_scraped_50000_remote_jobs_into_a_googlesheet](https://www.reddit.com/r/webscraping/comments/1gv8fd8/i_scraped_50000_remote_jobs_into_a_googlesheet)
 - RSS feed: $source
 - date published: 2024-11-19T21:25:28+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1gv8fd8/i_scraped_50000_remote_jobs_into_a_googlesheet/"> <img src="https://external-preview.redd.it/--VqND4HOOPxHyFKFKmJA1VHOLnp8MtZIsTZfTj_26k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f59cad34067f02ee6d2470e76bbc96d9b6b230fa" alt="I scraped 50,000+ remote jobs into a googlesheet - it's free. you're welcome. upvote so everyone sees it &lt;3" title="I scraped 50,000+ remote jobs into a googlesheet - it's free. you're welcome. upvote so everyone sees it &lt;3" /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/marvythemantis"> /u/marvythemantis </a> <br/> <span><a href="https://docs.google.com/spreadsheets/d/1uHZQuAFMBOvGLBtJdZA-GXzNk_Ug3EgBgCk7d1Vbrg8/edit?usp=sharing">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gv8fd8/i_scraped_50000_remote_jobs_into_a_googlesheet/">[comments]</a></span> </td></tr></table>

## Scraping PGA Tour Data
 - [https://www.reddit.com/r/webscraping/comments/1gv3ov2/scraping_pga_tour_data](https://www.reddit.com/r/webscraping/comments/1gv3ov2/scraping_pga_tour_data)
 - RSS feed: $source
 - date published: 2024-11-19T18:09:02+00:00

<!-- SC_OFF --><div class="md"><p>I’m new to web scraping and currently working on a reinforcement learning project aimed at predicting outcomes using PGA Tour data. My plan is to collect stats for players over multiple seasons, but I’m running into some issues.<br/> I wrote the following script using <strong>Selenium</strong> to scrape player stats from the PGA Tour website. It seems to work fine when scraping data for a single player or a small gorup when I try to include multiple players in runs into problems.<br/> The script fails to load the dropdown, select the season, or retrieve the table data, which makes it unreliable for large datasets.<br/> Heres the script ive been working on. </p> <pre><code>from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import pandas as pd import time driver = webdriver.Chrome() driver.set_window_size(1920

## HCaptcha bypass? (Effective and free)
 - [https://www.reddit.com/r/webscraping/comments/1gv1dpt/hcaptcha_bypass_effective_and_free](https://www.reddit.com/r/webscraping/comments/1gv1dpt/hcaptcha_bypass_effective_and_free)
 - RSS feed: $source
 - date published: 2024-11-19T16:35:44+00:00

<!-- SC_OFF --><div class="md"><p>Anyone know of a chrome extension or python script that reliably solves HCaptcha for completely free?</p> <p>The site I am scraping has a custom button that, once clicked, a pop up HCaptcha appears. The HCaptcha is configured at the hardest difficulty it seems, and requires two puzzles each time to pass. </p> <p>In Python, I made a script that uses Pixtral VLM API to: - Skip puzzles until you get one of those 3x3 puzzles (because you can simply click or not click the images rather than click on a certain coordinate) - Determine what’s in the reference image - goes through each of the 9 images and determines if they are the same as the reference / solve the prompt. </p> <p>Even with pre-processing the image to minimize the effect of the pattern overlay on the challenge image, I’m only solving them about 10% of the time. Even then, it takes it like 2 minutes per solve. </p> <p>Also, I’ve tried rotating residential proxies, user agents, timeouts, etc. t

## Target website detecting that proxies are originating from same place
 - [https://www.reddit.com/r/webscraping/comments/1guzeki/target_website_detecting_that_proxies_are](https://www.reddit.com/r/webscraping/comments/1guzeki/target_website_detecting_that_proxies_are)
 - RSS feed: $source
 - date published: 2024-11-19T15:13:15+00:00

<!-- SC_OFF --><div class="md"><p>I’m working on a web scraping project using a residential proxy pool and python requests. While I can successfully scrape a few hundred API URLs, I start getting RemoteDisconnected errors on subsequent requests. I’ve verified that I’m getting unique IPs with every request, so that’s not the issue. I&#39;ve tried multiple proxy providers and am running into the same issue. </p> <p>I suspect the target website might be detecting that all proxies are from the same provider, even though the IPs are different, and they&#39;re blocking everything. I’ve tried implementing delays and rotating user agents, but still running into issues.</p> <p>Has anyone dealt with this before? Are there specific proxy services or configurations that work better for avoiding detection? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/for_dinnerz"> /u/for_dinnerz </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1guz

## Do you use data enrichment and your strategies?
 - [https://www.reddit.com/r/webscraping/comments/1gupnw6/do_you_use_data_enrichment_and_your_strategies](https://www.reddit.com/r/webscraping/comments/1gupnw6/do_you_use_data_enrichment_and_your_strategies)
 - RSS feed: $source
 - date published: 2024-11-19T05:07:19+00:00

<!-- SC_OFF --><div class="md"><p>I was wondering what tools are being used for you guys. I’m currently using apollo.io. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/FontesB"> /u/FontesB </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1gupnw6/do_you_use_data_enrichment_and_your_strategies/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1gupnw6/do_you_use_data_enrichment_and_your_strategies/">[comments]</a></span>

