# Source:Simon Willison's Weblog, URL:https://simonwillison.net/atom/everything, language:en-us

## marimo.app
 - [https://simonwillison.net/2024/Jun/29/marimo-app/#atom-everything](https://simonwillison.net/2024/Jun/29/marimo-app/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-29T23:07:42+00:00

<p><a href="https://marimo.app/">marimo.app</a></p>
The Marimo reactive notebook (<a href="https://simonwillison.net/2024/Jan/12/marimo/">previously</a>) - a Python notebook that's effectively a cross between Jupyter and Observable - now also has a version that runs entirely in your browser using WebAssembly and Pyodide. Here's <a href="https://docs.marimo.io/guides/wasm.html">the documentation</a>.

## Quoting Jeremy Howard
 - [https://simonwillison.net/2024/Jun/29/jeremy-howard/#atom-everything](https://simonwillison.net/2024/Jun/29/jeremy-howard/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-29T22:52:41+00:00

<blockquote cite="https://twitter.com/jeremyphoward/status/1807162709664047144"><p>Absolutely any time I try to explore something even slightly against commonly accepted beliefs, LLMs <em>always</em> just rehash the commonly accepted beliefs.</p>
<p>As a researcher, I find this behaviour worse than unhelpful. It gives the mistaken impression that there's nothing to explore.</p></blockquote><p class="cite">&mdash; <a href="https://twitter.com/jeremyphoward/status/1807162709664047144">Jeremy Howard</a>

## Quoting ChatGPT is bullshit
 - [https://simonwillison.net/2024/Jun/29/chatgpt-is-bullshit/#atom-everything](https://simonwillison.net/2024/Jun/29/chatgpt-is-bullshit/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-29T13:50:56+00:00

<blockquote cite="https://link.springer.com/article/10.1007/s10676-024-09775-5"><p>We argued that ChatGPT is not designed to produce true utterances; rather, it is designed to produce text which is indistinguishable from the text produced by humans. It is aimed at being convincing rather than accurate. The basic architecture of these models reveals this: they are designed to come up with a <em>likely continuation of a string of text</em>. It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths. This is similar to standard cases of human bullshitters, who don’t care whether their utterances are true; good bullshit often contains some degree of truth, that’s part of what makes it convincing.</p></blockquote><p class="cite">&mdash

