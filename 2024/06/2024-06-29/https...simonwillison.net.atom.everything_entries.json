[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-06-29T23:07:42+00:00", "description": "<p><a href=\"https://marimo.app/\">marimo.app</a></p>\nThe Marimo reactive notebook (<a href=\"https://simonwillison.net/2024/Jan/12/marimo/\">previously</a>) - a Python notebook that's effectively a cross between Jupyter and Observable - now also has a version that runs entirely in your browser using WebAssembly and Pyodide. Here's <a href=\"https://docs.marimo.io/guides/wasm.html\">the documentation</a>.", "id": 791521, "language": "en-us", "link": "https://simonwillison.net/2024/Jun/29/marimo-app/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "marimo.app", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-06-29T22:52:41+00:00", "description": "<blockquote cite=\"https://twitter.com/jeremyphoward/status/1807162709664047144\"><p>Absolutely any time I try to explore something even slightly against commonly accepted beliefs, LLMs <em>always</em> just rehash the commonly accepted beliefs.</p>\n<p>As a researcher, I find this behaviour worse than unhelpful. It gives the mistaken impression that there's nothing to explore.</p></blockquote><p class=\"cite\">&mdash; <a href=\"https://twitter.com/jeremyphoward/status/1807162709664047144\">Jeremy Howard</a>", "id": 791522, "language": "en-us", "link": "https://simonwillison.net/2024/Jun/29/jeremy-howard/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Jeremy Howard", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-06-29T13:50:56+00:00", "description": "<blockquote cite=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"><p>We argued that ChatGPT is not designed to produce true utterances; rather, it is designed to produce text which is indistinguishable from the text produced by humans. It is aimed at being convincing rather than accurate. The basic architecture of these models reveals this: they are designed to come up with a <em>likely continuation of a string of text</em>. It\u2019s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths. This is similar to standard cases of human bullshitters, who don\u2019t care whether their utterances are true; good bullshit often contains some degree of truth, that\u2019s part of what makes it convincing.</p></blockquote><p class=\"cite\">&mdash", "id": 790155, "language": "en-us", "link": "https://simonwillison.net/2024/Jun/29/chatgpt-is-bullshit/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting ChatGPT is bullshit", "user": null, "vote": 0}]