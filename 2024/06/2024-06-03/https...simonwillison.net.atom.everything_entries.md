# Source:Simon Willison's Weblog, URL:https://simonwillison.net/atom/everything, language:en-us

## GPT-2 five years later
 - [https://simonwillison.net/2024/Jun/3/gpt-2-five-years-later/#atom-everything](https://simonwillison.net/2024/Jun/3/gpt-2-five-years-later/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-03T16:22:07+00:00

<p><a href="https://importai.substack.com/p/import-ai-375-gpt-2-five-years-later">GPT-2 five years later</a></p>
Jack Clark, now at Anthropic, was a researcher at OpenAI five years ago when they first trained GPT-2.</p>
<p>In this fascinating essay Jack revisits their decision not to release the full model, based on their concerns around potentially harmful ways that technology could be used.</p>
<p>(Today a GPT-2 class LLM can be trained from scratch <a href="https://simonwillison.net/2024/May/28/reproducing-gpt-2/">for around $20</a>, and much larger models are openly available.)</p>
<blockquote>
<p>There's a saying in the financial trading business which is 'the market can stay irrational longer than you can stay solvent' - though you might have the right idea about something that will happen in the future, your likelihood of correctly timing the market is pretty low. There's a truth to this for thinking about AI risks - yes, the things we forecast (as long as they're based on a go

## DuckDB 1.0
 - [https://simonwillison.net/2024/Jun/3/duckdb-10/#atom-everything](https://simonwillison.net/2024/Jun/3/duckdb-10/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-03T13:23:50+00:00

<p><a href="https://duckdb.org/2024/06/03/announcing-duckdb-100">DuckDB 1.0</a></p>
Six years in the making. The most significant feature in this milestone is stability of the file format: previous releases often required files to be upgraded to work with the new version.</p>

<p>This release also aspires to provide stability for both the SQL dialect and the C API, though these may still change with sufficient warning in the future.

    <p>Via <a href="https://twitter.com/duckdb/status/1797619191341551969">@duckdb</a></p>

## A look at Apple’s new Transformer-powered predictive text model
 - [https://simonwillison.net/2024/Jun/3/transformer-powered-predictive-text/#atom-everything](https://simonwillison.net/2024/Jun/3/transformer-powered-predictive-text/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-03T12:23:52+00:00

<p><a href="https://jackcook.com/2023/09/08/predictive-text.html">A look at Apple’s new Transformer-powered predictive text model</a></p>
Jack Cook reverse engineered the tiny LLM used for the predictive text keyboard in the latest iOS. It appears to be a GPT-2 style custom model with 34M parameters and a 15,000 token vocabulary.

## Katherine Michel's PyCon US 2024 Recap
 - [https://simonwillison.net/2024/Jun/3/katherine-michels-pycon-us-2024-recap/#atom-everything](https://simonwillison.net/2024/Jun/3/katherine-michels-pycon-us-2024-recap/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-03T09:31:15+00:00

<p><a href="https://katherinemichel.github.io/portfolio/pycon-us-2024-recap.html">Katherine Michel&#x27;s PyCon US 2024 Recap</a></p>
An informative write-up of this year’s PyCon US conference. It’s rare to see conference retrospectives with this much detail, this one is great!

    <p>Via <a href="https://twitter.com/katimichel/status/1796931565227778378">@katimichel</a></p>

