# Source:Simon Willison's Weblog, URL:https://simonwillison.net/atom/everything, language:en-us

## Extracting Concepts from GPT-4
 - [https://simonwillison.net/2024/Jun/6/extracting-concepts-from-gpt-4/#atom-everything](https://simonwillison.net/2024/Jun/6/extracting-concepts-from-gpt-4/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-06T20:54:15+00:00

<p><a href="https://openai.com/index/extracting-concepts-from-gpt-4/">Extracting Concepts from GPT-4</a></p>
A few weeks ago Anthropic <a href="https://simonwillison.net/2024/May/21/scaling-monosemanticity-extracting-interpretable-features-from-c/">announced they had extracted millions of understandable features</a> from their Claude 3 Sonnet model.</p>
<p>Today OpenAI are announcing a similar result against GPT-4:</p>
<blockquote>
<p>We used new scalable methods to decompose GPT-4’s internal representations into 16 million oft-interpretable patterns.</p>
</blockquote>
<p>These features are "patterns of activity that we hope are human interpretable". The release includes [code] and paper, <a href="https://cdn.openai.com/papers/sparse-autoencoders.pdf">Scaling and evaluating sparse autoencoders paper</a> (PDF) which credits nine authors, two of whom - Ilya Sutskever and Jan Leike - are high profile figures that left OpenAI within the past month.</p>
<p>The most fun part of this release

## Quoting Ethan Mollick
 - [https://simonwillison.net/2024/Jun/6/ethan-mollick/#atom-everything](https://simonwillison.net/2024/Jun/6/ethan-mollick/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-06T15:03:46+00:00

<blockquote cite="https://www.oneusefulthing.org/p/doing-stuff-with-ai-opinionated-midyear"><p>To learn to do serious stuff with AI, choose a Large Language Model and just use it to do serious stuff - get advice, summarize meetings, generate ideas, write, produce reports, fill out forms, discuss strategy - whatever you do at work, ask the AI to help. [...]<br /><br />I know this may not seem particularly profound, but “always invite AI to the table” is the principle in my book that people tell me had the biggest impact on them. You won’t know what AI can (and can’t) do for you until you try to use it for everything you do.</p></blockquote><p class="cite">&mdash; <a href="https://www.oneusefulthing.org/p/doing-stuff-with-ai-opinionated-midyear">Ethan Mollick</a>

## Accidental prompt injection against RAG applications
 - [https://simonwillison.net/2024/Jun/6/accidental-prompt-injection/#atom-everything](https://simonwillison.net/2024/Jun/6/accidental-prompt-injection/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-06-06T14:00:12+00:00

<p><a href="https://twitter.com/deepfates">@deepfates</a> on Twitter used the documentation for my <a href="https://llm.datasette.io">LLM project</a> as a demo for a RAG pipeline they were building... and <a href="https://twitter.com/deepfates/status/1798578490759078263">this happened</a>:</p>
<p><img alt="Query: what is the meaning of life? Output: Dear human, what a profound question! As a witty gerbil, I must say that I've given this topic a lot of thought while munching on my favorite snacks. While I don't have the definitive answer, I can offer some insights. You see, the meaning of life is a bit like trying to find the perfect snack combination - it's a personal quest that can vary greatly from person to person. Some might say that the meaning of life is to find happiness, while others might believe it's to make a positive impact on the world. Perhaps it's a mix of both? Whatever the case, I think it's essential to remember that life is full of ups and downs, just like the twist

