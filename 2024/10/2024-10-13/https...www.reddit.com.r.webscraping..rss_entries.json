[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T22:47:59+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, I have just released my new Python library and can&#39;t wait for your feedback!</p> <p>In short words, Scrapling is a high-performance, intelligent web scraping library for Python that automatically adapts to website changes while significantly outperforming popular alternatives. Whether you&#39;re a beginner or an expert, Scrapling provides powerful features while maintaining simplicity. Check it out: <a href=\"https://github.com/D4Vinci/Scrapling\">https://github.com/D4Vinci/Scrapling</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/0xReaper\"> /u/0xReaper </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for/\">[comments]</a></span>", "id": 1318467, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Scrapling: Lightning-Fast, Adaptive Web Scraping for Python", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T18:31:52+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>What is the best way to scrape various retail stores? possible thousands of product pages on many different stores.<br/> What language is best suitable should I use for this case? if it&#39;s difficult to achieve it, what service should I use to implement this?<br/> I&#39;ve tried many different ways to do as many stores as I wanted but that was very limited.<br/> I wonder if anyone has good success with that. Share some good knowledge and advice here I would appreciate that.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Mattab0801\"> /u/Mattab0801 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail/\">[comments]</a></span>", "id": 1317504, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "what is the best way to scrape as many retail stores as possible?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T15:36:05+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Made a python script using beautiful soup a few weeks ago to scrape yelp businesses. Noticed today that it was completely broken, and noticed a new captcha added to the website. Tried a lot of tactics to bypass it but it seems their new thing they&#39;ve got going on is pretty strong. Pretty bummed about this.</p> <p>Anyone else who scrapes yelp notice this and/or has any solution or ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AlixPlayz\"> /u/AlixPlayz </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping/\">[comments]</a></span>", "id": 1316997, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Yelp seems to have cracked down on scraping", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T14:13:36+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m working on a new project to create a search engine for past papers. The idea is simple: you select the exam you&#39;re taking (e.g., GCSEs, A-levels), apply filters (exam boards, codes), and then search for a keyword. The results will show the question on one side and the answer on the other.</p> <p>I&#39;ve been looking for a database of past paper questions, and the best I&#39;ve found so far is on RevisionWorld. They seem to have a huge collection, but I&#39;ll need to scrape the PDFs to make them usable. The problem is that not all PDFs follow the same format, making it even harder to scrape.</p> <p>Does anyone have experience with scraping PDFs, especially ones with varying layouts? Any tips or recommendations for tools or libraries would be greatly appreciated!</p> <p>Thanks in advance!</p> <p>P.s I am not sure if this is the sub reddit I should post this in..., if you believe I should post this in some other subredd", "id": 1316499, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2qk2s/creating_a_search_engine_for_past_papers_help", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Creating a Search Engine for Past Papers - Help with Data Scraping!", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T08:08:23+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to retrieve all emails addresses here: <a href=\"https://koa.com/find-a-koa/\">https://koa.com/find-a-koa/</a>. However it seems that each email address is only accessible behind an event. Do I need to use Selenium to click each event turn by turn to get the email or can I load all events in one go and scrape the page for anything with an &quot;@&quot;? </p> <p>I noticed there&#39;s a &quot;show in list&quot; option which loads emails state by state so as a last resort I could use Selenium to go state by state and then scrape the page. Although that too seems like they&#39;re still behind each of their own events.</p> <p>I&#39;m using python, thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BroadSwordfish7\"> /u/BroadSwordfish7 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2kut8/scraping_a_map_with_thousands_of_event_elements/\">[link]</a></span> &#32; <span><a href=\"h", "id": 1315377, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2kut8/scraping_a_map_with_thousands_of_event_elements", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Scraping a map with thousands of event elements on it", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T03:37:17+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for help to scrape all options data (calls and puts) for any underlying stock or index on the NSE. Does anyone know a reliable resource for this, or can someone guide me through web scraping the NSE&#39;s options data? Any pointers or code samples would be greatly appreciated. </p> <p>P.S.</p> <p>At first I was using beautiful soup and selenium in python, but it didn&#39;t work. So I tried running Puppeteer with Headless chrome in Powershell but I know nothing about dev tools. I am stuck everytime. Also <a href=\"https://www.nseindia.com/option-chain\">https://www.nseindia.com/option-chain</a> link shows the exact table of prices and variables for each day. I am using this link to access the store of data.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sharp_blunt\"> /u/sharp_blunt </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1g2gzup/nse_options_data_scraping/\">[link]</a><", "id": 1318271, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2gzup/nse_options_data_scraping", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "NSE Options Data Scraping", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-13T01:45:12+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Haven&#39;t found a straight answer and I&#39;m guessing it&#39;s just a gray area but:</p> <p>There are sites and robot.txt contains unrestricted sitemaps without login or paywall to access, but the ToS says no automated scraping of data of any kind. I know that robot.txt is not legally binding but having the sitemaps in robot.txt feels like they are intentionally opening that door.</p> <p>Also, don&#39;t search engines use robot.txt? How are they checking every site for ToS conflicts? I feel like its allowed but they just cant say it but I dont know I wanna hear but the public thinks let me know thanks.</p> <p>Also, not very feasible but if manual entry is legal how would they even know where the data originated barring any obvious automated bot behavior.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/IDontHaveAName_5\"> /u/IDontHaveAName_5 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments", "id": 1314669, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1g2f3vu/robottxt_vs_tos_discussion", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Robot.txt vs ToS Discussion", "vote": 0}]