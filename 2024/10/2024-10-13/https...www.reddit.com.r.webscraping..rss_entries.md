# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Scrapling: Lightning-Fast, Adaptive Web Scraping for Python
 - [https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for](https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for)
 - RSS feed: $source
 - date published: 2024-10-13T22:47:59+00:00

<!-- SC_OFF --><div class="md"><p>Hello everyone, I have just released my new Python library and can&#39;t wait for your feedback!</p> <p>In short words, Scrapling is a high-performance, intelligent web scraping library for Python that automatically adapts to website changes while significantly outperforming popular alternatives. Whether you&#39;re a beginner or an expert, Scrapling provides powerful features while maintaining simplicity. Check it out: <a href="https://github.com/D4Vinci/Scrapling">https://github.com/D4Vinci/Scrapling</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/0xReaper"> /u/0xReaper </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1g31y85/scrapling_lightningfast_adaptive_web_scraping_for/">[comments]</a></span>

## what is the best way to scrape as many retail stores as possible?
 - [https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail](https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail)
 - RSS feed: $source
 - date published: 2024-10-13T18:31:52+00:00

<!-- SC_OFF --><div class="md"><p>What is the best way to scrape various retail stores? possible thousands of product pages on many different stores.<br/> What language is best suitable should I use for this case? if it&#39;s difficult to achieve it, what service should I use to implement this?<br/> I&#39;ve tried many different ways to do as many stores as I wanted but that was very limited.<br/> I wonder if anyone has good success with that. Share some good knowledge and advice here I would appreciate that.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Mattab0801"> /u/Mattab0801 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1g2wcsy/what_is_the_best_way_to_scrape_as_many_retail/">[comments]</a></span>

## Yelp seems to have cracked down on scraping
 - [https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping](https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping)
 - RSS feed: $source
 - date published: 2024-10-13T15:36:05+00:00

<!-- SC_OFF --><div class="md"><p>Made a python script using beautiful soup a few weeks ago to scrape yelp businesses. Noticed today that it was completely broken, and noticed a new captcha added to the website. Tried a lot of tactics to bypass it but it seems their new thing they&#39;ve got going on is pretty strong. Pretty bummed about this.</p> <p>Anyone else who scrapes yelp notice this and/or has any solution or ideas?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AlixPlayz"> /u/AlixPlayz </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1g2sd32/yelp_seems_to_have_cracked_down_on_scraping/">[comments]</a></span>

## Creating a Search Engine for Past Papers - Help with Data Scraping!
 - [https://www.reddit.com/r/webscraping/comments/1g2qk2s/creating_a_search_engine_for_past_papers_help](https://www.reddit.com/r/webscraping/comments/1g2qk2s/creating_a_search_engine_for_past_papers_help)
 - RSS feed: $source
 - date published: 2024-10-13T14:13:36+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone,</p> <p>I&#39;m working on a new project to create a search engine for past papers. The idea is simple: you select the exam you&#39;re taking (e.g., GCSEs, A-levels), apply filters (exam boards, codes), and then search for a keyword. The results will show the question on one side and the answer on the other.</p> <p>I&#39;ve been looking for a database of past paper questions, and the best I&#39;ve found so far is on RevisionWorld. They seem to have a huge collection, but I&#39;ll need to scrape the PDFs to make them usable. The problem is that not all PDFs follow the same format, making it even harder to scrape.</p> <p>Does anyone have experience with scraping PDFs, especially ones with varying layouts? Any tips or recommendations for tools or libraries would be greatly appreciated!</p> <p>Thanks in advance!</p> <p>P.s I am not sure if this is the sub reddit I should post this in..., if you believe I should post this in some other subredd

## Scraping a map with thousands of event elements on it
 - [https://www.reddit.com/r/webscraping/comments/1g2kut8/scraping_a_map_with_thousands_of_event_elements](https://www.reddit.com/r/webscraping/comments/1g2kut8/scraping_a_map_with_thousands_of_event_elements)
 - RSS feed: $source
 - date published: 2024-10-13T08:08:23+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m trying to retrieve all emails addresses here: <a href="https://koa.com/find-a-koa/">https://koa.com/find-a-koa/</a>. However it seems that each email address is only accessible behind an event. Do I need to use Selenium to click each event turn by turn to get the email or can I load all events in one go and scrape the page for anything with an &quot;@&quot;? </p> <p>I noticed there&#39;s a &quot;show in list&quot; option which loads emails state by state so as a last resort I could use Selenium to go state by state and then scrape the page. Although that too seems like they&#39;re still behind each of their own events.</p> <p>I&#39;m using python, thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/BroadSwordfish7"> /u/BroadSwordfish7 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1g2kut8/scraping_a_map_with_thousands_of_event_elements/">[link]</a></span> &#32; <span><a href="h

## NSE Options Data Scraping
 - [https://www.reddit.com/r/webscraping/comments/1g2gzup/nse_options_data_scraping](https://www.reddit.com/r/webscraping/comments/1g2gzup/nse_options_data_scraping)
 - RSS feed: $source
 - date published: 2024-10-13T03:37:17+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m looking for help to scrape all options data (calls and puts) for any underlying stock or index on the NSE. Does anyone know a reliable resource for this, or can someone guide me through web scraping the NSE&#39;s options data? Any pointers or code samples would be greatly appreciated. </p> <p>P.S.</p> <p>At first I was using beautiful soup and selenium in python, but it didn&#39;t work. So I tried running Puppeteer with Headless chrome in Powershell but I know nothing about dev tools. I am stuck everytime. Also <a href="https://www.nseindia.com/option-chain">https://www.nseindia.com/option-chain</a> link shows the exact table of prices and variables for each day. I am using this link to access the store of data.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/sharp_blunt"> /u/sharp_blunt </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1g2gzup/nse_options_data_scraping/">[link]</a><

## Robot.txt vs ToS Discussion
 - [https://www.reddit.com/r/webscraping/comments/1g2f3vu/robottxt_vs_tos_discussion](https://www.reddit.com/r/webscraping/comments/1g2f3vu/robottxt_vs_tos_discussion)
 - RSS feed: $source
 - date published: 2024-10-13T01:45:12+00:00

<!-- SC_OFF --><div class="md"><p>Haven&#39;t found a straight answer and I&#39;m guessing it&#39;s just a gray area but:</p> <p>There are sites and robot.txt contains unrestricted sitemaps without login or paywall to access, but the ToS says no automated scraping of data of any kind. I know that robot.txt is not legally binding but having the sitemaps in robot.txt feels like they are intentionally opening that door.</p> <p>Also, don&#39;t search engines use robot.txt? How are they checking every site for ToS conflicts? I feel like its allowed but they just cant say it but I dont know I wanna hear but the public thinks let me know thanks.</p> <p>Also, not very feasible but if manual entry is legal how would they even know where the data originated barring any obvious automated bot behavior.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/IDontHaveAName_5"> /u/IDontHaveAName_5 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments

