# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Scrape mobile.de
 - [https://www.reddit.com/r/webscraping/comments/1fxqned/scrape_mobilede](https://www.reddit.com/r/webscraping/comments/1fxqned/scrape_mobilede)
 - RSS feed: $source
 - date published: 2024-10-06T20:55:01+00:00

<!-- SC_OFF --><div class="md"><p>Hello</p> <p>is it possible to scrape mobile.de? If yes, how? ðŸ˜†</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Potential_You42"> /u/Potential_You42 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fxqned/scrape_mobilede/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fxqned/scrape_mobilede/">[comments]</a></span>

## Does anyone here do large scale web scraping?
 - [https://www.reddit.com/r/webscraping/comments/1fxpi3w/does_anyone_here_do_large_scale_web_scraping](https://www.reddit.com/r/webscraping/comments/1fxpi3w/does_anyone_here_do_large_scale_web_scraping)
 - RSS feed: $source
 - date published: 2024-10-06T20:05:30+00:00

<!-- SC_OFF --><div class="md"><p>Hey guys, </p> <p>We&#39;re currently ramping up and doing a lot more web scraping, so I was wondering if there were any people that do web scraping on a regular basis that I can chat with to learn more about how you guys complete these tasks? </p> <p>Looking to learn more specifically around infrastructure of how you guys are hosting these web scrapers and best practices!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/youngkilog"> /u/youngkilog </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fxpi3w/does_anyone_here_do_large_scale_web_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fxpi3w/does_anyone_here_do_large_scale_web_scraping/">[comments]</a></span>

## Forward from BlogaBet to Telegram
 - [https://www.reddit.com/r/webscraping/comments/1fxj4qr/forward_from_blogabet_to_telegram](https://www.reddit.com/r/webscraping/comments/1fxj4qr/forward_from_blogabet_to_telegram)
 - RSS feed: $source
 - date published: 2024-10-06T15:31:08+00:00

<!-- SC_OFF --><div class="md"><p>Is there a way to forward private picks from blogabet to telegram? </p> <p>It needs to solve the captcha, etc etc, if someone sells this service or knows one, kindly tell me.</p> <p>Thanks a lot :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Chuti0800"> /u/Chuti0800 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fxj4qr/forward_from_blogabet_to_telegram/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fxj4qr/forward_from_blogabet_to_telegram/">[comments]</a></span>

## Product matching from different stores
 - [https://www.reddit.com/r/webscraping/comments/1fxg54g/product_matching_from_different_stores](https://www.reddit.com/r/webscraping/comments/1fxg54g/product_matching_from_different_stores)
 - RSS feed: $source
 - date published: 2024-10-06T13:10:25+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1fxg54g/product_matching_from_different_stores/"> <img src="https://b.thumbs.redditmedia.com/5T6uX1I2plEvsnl5TBaoSWytXyl2rE0zQfTKffeNheo.jpg" alt="Product matching from different stores" title="Product matching from different stores" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hey, I have been struggling to find a solution to this problem:</p> <p>Iâ€™m scraping 2 grocery stores - Store A and Store B - (maybe more in the future) that can sell the same products.</p> <p>On neither store I have a common ID that I can match from to say if a product on Store A is the same on Store B.</p> <p>I have the productâ€™s : Title, Picture, Net Volume (ex : 400g)</p> <p>My initial solution (which is working up to an extent) was : index all my products from Store A onto ElasticSearch and then, when I scrape Store B, I do some fuzzy matching so that I can match its products with Store Aâ€™s products. If no product is found, then 

