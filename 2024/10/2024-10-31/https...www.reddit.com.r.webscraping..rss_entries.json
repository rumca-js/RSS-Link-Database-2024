[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T22:33:23+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>AI scrapers just convert the webpage to text and search with an LLM to extract the information. Less reliable, costs more. But easier or quicker for beginners to use and less susceptible perhaps to changes in html code. </p> <p>Even if you don&#39;t think it is a good idea, what are the best Python libs in this class?</p> <ol> <li><a href=\"https://github.com/apify/crawlee-python\">https://github.com/apify/crawlee-python</a></li> <li><a href=\"https://github.com/ScrapeGraphAI/Scrapegraph-ai\">https://github.com/ScrapeGraphAI/Scrapegraph-ai</a></li> <li><a href=\"https://github.com/raznem/parsera\">https://github.com/raznem/parsera</a></li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/stvaccount\"> /u/stvaccount </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggqylr/best_ai_scraping_libs_for_python/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggqyl", "id": 1425690, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggqylr/best_ai_scraping_libs_for_python", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Best AI scraping libs for Python", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T20:36:16+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey all,</p> <p>noob question, but I&#39;m trying to create a program which will scrape marketplaces (ebay, amazon, etsy, etc) once a day to gather product data for specific searches. I kept getting flagged as a bot but finally have a working model thanks to a proxy service.</p> <p>My question is: if i were to run this bot for long enough and at a large enough scale, wouldn&#39;t the rotating IPs used by this service be flagged one-by-one and subsequently blocked? How do they avoid this? Should I worry that eventually this proxy service will be rendered obsolete by the website(s) i&#39;m trying to scrape?</p> <p>Sorry if it&#39;s a silly question. Thanks in advance</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SpecialSecret1248\"> /u/SpecialSecret1248 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggoetj/how_do_proxies_avoid_getting_blocked/\">[link]</a></span> &#32; <span><a href=\"http", "id": 1425132, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggoetj/how_do_proxies_avoid_getting_blocked", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "How do proxies avoid getting blocked?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T18:09:53+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been trying to implement a very simple telegram bot with python to track the prices of only a few products I&#39;m interested in buying. To start out, my code was as simple as this:</p> <pre><code>from bs4 import BeautifulSoup import requests import yaml # Get products URLs (currently only one) with open(&#39;./config/config.yaml&#39;, &#39;r&#39;) as file: config = yaml.safe_load(file) url = config[&#39;products&#39;][0][&#39;url&#39;] # Been trying to comment and uncomment these to see what works headers = { # &#39;accept&#39;: &#39;*/*&#39;, &#39;user-agent&#39;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0&quot;, # &quot;accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;, &quot;accept-language&quot;: &quot;pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3&quot;, # &quot;accept-encoding&quot;: &quot;gzip, deflate, br, zstd&quot;, # &quot;connection&quot;: &quot;keep-alive&", "id": 1423903, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggl05r/alternatives_to_scraping_amazon", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Alternatives to scraping Amazon?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T17:54:51+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I&#39;ve been working with web scraping for quite a while and consider myself pretty skilled in it. I&#39;ve automated a lot of tasks, gathered data from various sites, and am comfortable with handling obstacles like CAPTCHAs, proxies, and dynamic content.</p> <p>Now I&#39;m looking to take things a step further and monetize this skill. I&#39;m curious about the ways some of you have turned web scraping into a profitable venture. Whether it&#39;s freelance projects, creating products, or anything else, I&#39;d love to hear what has worked for you.</p> <p>Here are some of my ideas so far, but I&#39;d appreciate feedback and any new suggestions:</p> <ol> <li><p>Freelance Projects \u2013 Working on specific data extraction tasks for clients. How do you find high-paying clients?</p></li> <li><p>Building a SaaS Tool \u2013 Offering scraped data or insights on a subscription basis. What kind of data do you find the highest demand for?</p></li> <li><p>S", "id": 1423904, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggkn9w/how_to_monetize_web_scraping_skills_looking_for", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "How to Monetize Web Scraping Skills? Looking for Ideas...", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T15:03:25+00:00", "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1gggo0w/interactive_map_id_scraping/\"> <img src=\"https://b.thumbs.redditmedia.com/Q4mtKComzGajclaK_w2WbxlF8xVuarkegi0XrTGD7bY.jpg\" alt=\"Interactive Map ID Scraping\" title=\"Interactive Map ID Scraping\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/366ljlzcv3yd1.png?width=1603&amp;format=png&amp;auto=webp&amp;s=efe19c655312ab352930f5ce74e9951f4d04c95e\">https://preview.redd.it/366ljlzcv3yd1.png?width=1603&amp;format=png&amp;auto=webp&amp;s=efe19c655312ab352930f5ce74e9951f4d04c95e</a></p> <p>Can anyone tell me if it is possible to scrape every ID Number all at once in this interactive map? I already succeded in scraping one ID but I cant figure \u00f3ut how I can scrape all IDs at once</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CurlyFreeze17\"> /u/CurlyFreeze17 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1gggo0w/i", "id": 1422843, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1gggo0w/interactive_map_id_scraping", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 86, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": "https://b.thumbs.redditmedia.com/Q4mtKComzGajclaK_w2WbxlF8xVuarkegi0XrTGD7bY.jpg", "title": "Interactive Map ID Scraping", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T14:56:52+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi. I am trying to scrape date from this site: <a href=\"https://app.dcbyte.com/search?searchType=Colocation,Self-build&amp;earlyStageSchemes=true&amp;view=map\">https://app.dcbyte.com/search?searchType=Colocation,Self-build&amp;earlyStageSchemes=true&amp;view=map</a> It is a mapbox interactive map. Each point is a data center with some details (name, location, description). What would be the best way to scrape this? I want to scrape individual countries, or at least tag the countries. I saw that there is a tag &lt;h4 that gives the country name. Then there are &lt;p tags for data centre names. Anyway, if someone can help, I am very grateful</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/puglet1964\"> /u/puglet1964 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1gggigi/mapbox_data_extraction/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1gggigi/mapb", "id": 1422448, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1gggigi/mapbox_data_extraction", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Mapbox data extraction", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T12:28:50+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi there! Does anyone have a URL or documentation on how to use proxies with Scrapy Playwright? I\u2019ve tried everything I found\u2014from creating a middleware to adding the proxy directly in the spider when making requests. I even tried solutions from ChatGPT, but those didn\u2019t work either, lol. The proxy format I&#39;m using is IP:PORT:USER:PASS</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Complex-Branch-3003\"> /u/Complex-Branch-3003 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggdaxy/proxies_using_scrapy_playwright/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggdaxy/proxies_using_scrapy_playwright/\">[comments]</a></span>", "id": 1421922, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggdaxy/proxies_using_scrapy_playwright", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Proxies using Scrapy Playwright", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T11:36:55+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>What&#39;s the best free chrome extension that can scrape a list of URLs? Just need one element, not the full page. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ProductOfHostility\"> /u/ProductOfHostility </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggccvm/chrome_extension_that_can_scrape_a_list_of_urls/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ggccvm/chrome_extension_that_can_scrape_a_list_of_urls/\">[comments]</a></span>", "id": 1421066, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ggccvm/chrome_extension_that_can_scrape_a_list_of_urls", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "Chrome Extension That Can Scrape A List Of URLs?", "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-10-31T04:02:24+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve tried a plethora of solutions from Stack overflow, YouTube tutorials, and ChatGPT debugging. However, none of these solutions have worked. When I deploy my script with this dockerfile, this is the error I get in return. Any help would be appreciated!</p> <p>Dockerfile: FROM python:3.10-slim</p> <p>ENV DEBIAN_FRONTEND=noninteractive</p> <p>RUN apt-get update &amp;&amp; apt-get install -y \\ chromium-driver \\ chromium \\ fonts-liberation \\ libnss3 \\ libx11-6 \\ libatk-bridge2.0-0 \\ libatspi2.0-0 \\ libgtk-3-0 \\ libxcomposite1 \\ libxcursor1 \\ libxdamage1 \\ libxrandr2 \\ libgbm-dev \\ &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*</p> <p>ENV CHROME_BIN=/usr/bin/chromium \\ CHROME_DRIVER_BIN=/usr/bin/chromedriver</p> <p>WORKDIR /app</p> <p>COPY requirements.txt .</p> <p>RUN pip install --no-cache-dir -r requirements.txt</p> <p>COPY . .</p> <p>EXPOSE 8080</p> <p>Error: File &quot;/workspace/app/scraper.py&quot;, line 21, in driver = webdrive", "id": 1419675, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1gg5zzo/how_do_i_configure_dockerfile_to_run_chrome_with", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source__id": 467, "source_url": "https://www.reddit.com/r/webscraping/.rss", "status_code": 0, "tags": [], "thumbnail": null, "title": "How do I configure Dockerfile to Run Chrome with Selenium?", "vote": 0}]