# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## How is wayback able to webscrape/webcrawl without getting detected?
 - [https://www.reddit.com/r/webscraping/comments/1fut90l/how_is_wayback_able_to_webscrapewebcrawl_without](https://www.reddit.com/r/webscraping/comments/1fut90l/how_is_wayback_able_to_webscrapewebcrawl_without)
 - RSS feed: $source
 - date published: 2024-10-02T22:47:27+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m pretty new to this so apologies if my question is very newbish/ignorant</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Geyball"> /u/Geyball </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fut90l/how_is_wayback_able_to_webscrapewebcrawl_without/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fut90l/how_is_wayback_able_to_webscrapewebcrawl_without/">[comments]</a></span>

## LLM based web scrapping
 - [https://www.reddit.com/r/webscraping/comments/1fusczo/llm_based_web_scrapping](https://www.reddit.com/r/webscraping/comments/1fusczo/llm_based_web_scrapping)
 - RSS feed: $source
 - date published: 2024-10-02T22:01:37+00:00

<!-- SC_OFF --><div class="md"><p>I am wondering if there is any LLM based web scrapper that can remember multiple pages and gather data based on prompt?</p> <p>I believe this should be available!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Accomplished_Ad_655"> /u/Accomplished_Ad_655 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fusczo/llm_based_web_scrapping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fusczo/llm_based_web_scrapping/">[comments]</a></span>

## Crawl/Scrape Chewy.com
 - [https://www.reddit.com/r/webscraping/comments/1fuq8cc/crawlscrape_chewycom](https://www.reddit.com/r/webscraping/comments/1fuq8cc/crawlscrape_chewycom)
 - RSS feed: $source
 - date published: 2024-10-02T20:30:29+00:00

<!-- SC_OFF --><div class="md"><p>I have tried my heart out with requests, selenuim, different user agents, slowing down the crawl, etc., etc. </p> <p>I cannot figure out how to crawl/scrape chewy.com. Can anyone figure it out? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/opchopper10"> /u/opchopper10 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fuq8cc/crawlscrape_chewycom/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fuq8cc/crawlscrape_chewycom/">[comments]</a></span>

## Scraping the full list of subreddits with over 10k members
 - [https://www.reddit.com/r/webscraping/comments/1fuq2e3/scraping_the_full_list_of_subreddits_with_over](https://www.reddit.com/r/webscraping/comments/1fuq2e3/scraping_the_full_list_of_subreddits_with_over)
 - RSS feed: $source
 - date published: 2024-10-02T20:23:23+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve tried using Claude/Chatgpt to build a python bot to do it, but couldn&#39;t make it work. </p> <p>IDK if there is an API endpoint for this.</p> <p>Here&#39;s the link</p> <p><a href="https://www.reddit.com/best/communities/1/">https://www.reddit.com/best/communities/1/</a></p> <p>to </p> <p><a href="https://www.reddit.com/best/communities/85/">https://www.reddit.com/best/communities/85/</a></p> <p>So is just 85 pages.</p> <p>The output is to be saved on topsubreddits.csv in the same dir. </p> <p>Looking simple like this:</p> <p>Rank,subreddit,category,members</p> <p>1,<a href="/r/funny">r/funny</a>,funny/humor,64M</p> <p>Anyone?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/keyehi"> /u/keyehi </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fuq2e3/scraping_the_full_list_of_subreddits_with_over/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments

## How to use socks5 proxy with authentication in puppeteer
 - [https://www.reddit.com/r/webscraping/comments/1fupfa2/how_to_use_socks5_proxy_with_authentication_in](https://www.reddit.com/r/webscraping/comments/1fupfa2/how_to_use_socks5_proxy_with_authentication_in)
 - RSS feed: $source
 - date published: 2024-10-02T19:56:14+00:00

<!-- SC_OFF --><div class="md"><p>How to use authentication socks5 proxy on separate file like I have proxy. json with my proxy and username and password then I have another code where I use to excute tasks and I want to use socks5 proxy.. So that it fetchs the proxy from proxy. json </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/uncletee96"> /u/uncletee96 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fupfa2/how_to_use_socks5_proxy_with_authentication_in/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fupfa2/how_to_use_socks5_proxy_with_authentication_in/">[comments]</a></span>

## When You’ve Spent More Time Finding Docs Than Writing Code
 - [https://www.reddit.com/r/webscraping/comments/1fukatj/when_youve_spent_more_time_finding_docs_than](https://www.reddit.com/r/webscraping/comments/1fukatj/when_youve_spent_more_time_finding_docs_than)
 - RSS feed: $source
 - date published: 2024-10-02T16:24:03+00:00

<!-- SC_OFF --><div class="md"><p>Picture this: you’re halfway through coding a feature when you hit a wall. Naturally, you turn to the documentation for help. But instead of a quick solution, you’re met with a doc site that feels like it hasn&#39;t been updated since the age of dial-up. There’s no search bar and what should’ve taken five minutes ends up burning half your day (or a good hour of going back and forth).</p> <p>Meanwhile, I’ve tried using LLMs to speed up the process, but even they don’t always have the latest updates. So there I am, shuffling through doc pages like a madman trying to piece together a solution.</p> <p>After dealing with this mess for way too long, I did what any of us would do—complained about it first, then built something to fix it. That’s how DocTao was born. It scrapes the most up-to-date docs from the source, keeps them all in one place, and has an AI chat feature that helps you interact with the docs more efficiently and integrate what you&#39;ve fo

## Saving Scrapy Crawl Stats to PostgreSQL with a Custom Extension and SQLAlchemy
 - [https://www.reddit.com/r/webscraping/comments/1fuj9rk/saving_scrapy_crawl_stats_to_postgresql_with_a](https://www.reddit.com/r/webscraping/comments/1fuj9rk/saving_scrapy_crawl_stats_to_postgresql_with_a)
 - RSS feed: $source
 - date published: 2024-10-02T15:42:11+00:00

&#32; submitted by &#32; <a href="https://www.reddit.com/user/siegerts"> /u/siegerts </a> <br/> <span><a href="https://www.xiegerts.com/post/scrapy-extension-save-crawlstats-postgres/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fuj9rk/saving_scrapy_crawl_stats_to_postgresql_with_a/">[comments]</a></span>

## Google Map Scraping over limits
 - [https://www.reddit.com/r/webscraping/comments/1fuaxhz/google_map_scraping_over_limits](https://www.reddit.com/r/webscraping/comments/1fuaxhz/google_map_scraping_over_limits)
 - RSS feed: $source
 - date published: 2024-10-02T07:56:49+00:00

<!-- SC_OFF --><div class="md"><p>Hello,<br/> Are there people here who manage to fully scrape a city or country ? I’ve tried with several GitHub repo, but even on those where I can get past 120 results, I’m still limited. Do any of you have good resources for this please ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Sad_Depth_4846"> /u/Sad_Depth_4846 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fuaxhz/google_map_scraping_over_limits/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fuaxhz/google_map_scraping_over_limits/">[comments]</a></span>

## Need Advice - Web Scraping Technique
 - [https://www.reddit.com/r/webscraping/comments/1fu4xu0/need_advice_web_scraping_technique](https://www.reddit.com/r/webscraping/comments/1fu4xu0/need_advice_web_scraping_technique)
 - RSS feed: $source
 - date published: 2024-10-02T01:39:48+00:00

<!-- SC_OFF --><div class="md"><p>Hi there, </p> <p>Not sure if this is the right platform but here goes.</p> <p>I am pretty novice when it comes to web scraping and am looking to see if anyone has created a chrome extension that does web scraping. If so, would love to connect for some feedback.</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/bec777"> /u/bec777 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fu4xu0/need_advice_web_scraping_technique/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fu4xu0/need_advice_web_scraping_technique/">[comments]</a></span>

