[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-09T22:30:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>In a bid to \u201cdeepen the public conversation about how AI models should behave,\u201d AI company OpenAI has introduced Model Spec, a document that shares the company\u2019s approach to shaping desired model behavior.</p><p><a href=\"https://cdn.openai.com/spec/model-spec-2024-05-08.html\" rel=\"nofollow\">Model Spec</a>, now in a first draft, was <a href=\"https://openai.com/index/introducing-the-model-spec/\" rel=\"nofollow\">introduced May 8</a>. The document specifies OpenAI\u2019s approach to shaping desired model behavior and how the company evaluates trade-offs when conflicts arise. The approach includes objectives, rules, and default behaviors that will guide OpenAI\u2019s researchers and AI trainers who work on reinforcement learning from human feedback (RLHF). The company will also explore how much its models can learn directly from the Model Spec.</p><p class=\"jumpTag\"><a href=\"/article/3715399/openai-unveils-specs-for-desired-ai-model-behavior.html#jump\">To read thi", "language": "en-us", "link": "https://www.infoworld.com/article/3715399/openai-unveils-specs-for-desired-ai-model-behavior.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2023/07/openai-100943386-small.jpg", "title": "OpenAI unveils specs for desired AI model behavior", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-09T17:30:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>GitHub has introduced Artifact Attestations, a software signing and verification feature based on <a href=\"https://www.infoworld.com/article/3696209/sigstore-roots-of-trust-for-software-artifacts.html\">Sigstore</a> that protects the integrity of software builds in <a href=\"https://www.infoworld.com/article/3698188/what-is-github-actions-automated-cicd-for-github.html\">GitHub Actions</a> workflows. Artifiact Attestations is now available in a public beta.</p><p><a href=\"https://github.blog/2024-05-02-introducing-artifact-attestations-now-in-public-beta/\" rel=\"nofollow\">Announced May 2</a>, Artifact Attestations allows project maintainers to create a \u201ctamper-proof, unforgeable paper trail\u201d that links software artifacts to the process that created them. \u201cDownstream consumers of this metadata can use it as a foundation for new security and validity checks through policy evaluations via tools like <a href=\"https://www.openpolicyagent.org/docs/latest/pol", "language": "en-us", "link": "https://www.infoworld.com/article/3715460/github-takes-aim-at-software-supply-chain-security.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2024/05/shutterstock_712558591-100963102-small.jpg", "title": "GitHub takes aim at software supply chain security", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-09T09:00:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>Django has been a <a href=\"https://www.infoworld.com/article/3600191/5-big-and-powerful-python-web-frameworks.html\">leading \u201cbatteries included\u201d Python web framework</a> for more than a decade. The fifth major release, which arrived in December, brought even <a href=\"https://www.infoworld.com/article/3711367/5-great-new-features-in-django-5.html\">more power and ease to Django</a>.</p><p>Curious about the latest Django development trends? JetBrains\u2019 PyCharm team, in collaboration with the Django Foundation, surveyed over 4,000 developers worldwide to analyze framework usage. Here\u2019s what we found:</p><ul>\n<li>Django remains the top pick for 74% of developers.</li>\n<li>One in three Django developers also uses <a href=\"https://www.infoworld.com/article/3619522/get-started-with-flask-30.html\">Flask</a> or <a href=\"https://www.infoworld.com/article/3629409/get-started-with-fastapi.html\">FastAPI</a>.</li>\n<li>Django is commonly used for full-stack and API", "language": "en-us", "link": "https://www.infoworld.com/article/3715395/highlights-from-the-django-developer-survey-2024.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2023/04/shutterstock_1445051480-100940464-small.jpg", "title": "Highlights from the Django Developer Survey 2024", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-09T09:00:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>Both extremely promising and extremely risky, <a href=\"https://www.infoworld.com/article/3689973/what-is-generative-ai-artificial-intelligence-that-creates.html\">generative AI</a> has distinct failure modes that we need to defend against to protect our users and our code. We\u2019ve all seen the news, where chatbots are encouraged to be insulting or racist, or <a href=\"https://www.infoworld.com/article/3709489/large-language-models-the-foundations-of-generative-ai.html\">large language models</a> (LLMs) are exploited for malicious purposes, and where outputs are at best fanciful and at worst dangerous.</p><p>None of this is particularly surprising. It\u2019s possible to craft complex prompts that force undesired outputs, pushing the input window past the guidelines and guardrails we\u2019re using. At the same time, we can see outputs that go beyond the data in the foundation model, generating text that\u2019s no longer grounded in reality, producing plausible, semantic", "language": "en-us", "link": "https://www.infoworld.com/article/3715305/protecting-llm-applications-with-azure-ai-content-safety.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2024/05/shutterstock_77002051-100963133-small.jpg", "title": "Protecting LLM applications with Azure AI Content Safety", "user": null, "vote": 0}]