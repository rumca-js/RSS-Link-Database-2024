[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-21T20:04:30+00:00", "description": "<p><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1cxa6w5/phi3_small_medium_are_now_available_under_the_mit/\">New Phi-3 models: small, medium and vision</a></p>\nI couldn't find a good official announcement post to link to about these three newly released models, but this post on LocalLLaMA on Reddit has them in one place: Phi-3 small (7B), Phi-3 medium (14B) and Phi-3 vision (4.2B) (the previously released model was Phi-3 mini - 3.8B).</p>\n<p>You can try out the <a href=\"https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/1/registry/azureml\">vision model directly here</a>, no login required. It didn't do <a href=\"https://twitter.com/simonw/status/1793009034863260035\">a great job</a> with my first test image though, hallucinating the text.</p>\n<p>As with Mini these are all released under an MIT license.</p>\n<p>UPDATE: Here's <a href=\"https://github.com/microsoft/Phi-3CookBook/blob/main/md/01.Introduce/Phi3Family.md\">a page from the newly published Phi-3 Cookbo", "language": "en-us", "link": "https://simonwillison.net/2024/May/21/phi-3-models-small-medium-and-vision/#atom-everything", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "New Phi-3 models: small, medium and vision", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-21T18:25:40+00:00", "description": "<p><a href=\"https://transformer-circuits.pub/2024/scaling-monosemanticity/#safety-relevant-sycophancy\">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></p>\nBig advances in the field of LLM interpretability from Anthropic, who managed to extract millions of understandable features from their production Claude 3 Sonnet model (the mid-point between the inexpensive Haiku and the GPT-4-class Opus).</p>\n<p>Some delightful snippets in here such as this one:</p>\n<blockquote>\n<p>We also find a variety of features related to sycophancy, such as an empathy / \u201cyeah, me too\u201d feature 34M/19922975, a sycophantic praise feature 1M/847723, and a sarcastic praise feature 34M/19415708.</p>\n</blockquote>\n\n    <p>Via <a href=\"https://news.ycombinator.com/item?id=40429540\">Hacker News</a></p>", "language": "en-us", "link": "https://simonwillison.net/2024/May/21/scaling-monosemanticity-extracting-interpretable-features-from-c/#atom-everything", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet", "user": null, "vote": 0}]