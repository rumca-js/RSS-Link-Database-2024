[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-28T09:00:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>Historically, <a href=\"https://www.infoworld.com/article/3709489/large-language-models-the-foundations-of-generative-ai.html\">large language models</a> (LLMs) have required substantial computational resources. This means development and deployment are confined mainly to powerful centralized systems, such as public cloud providers. However, although many people believe that we need massive amounts of GPUs bound to vast amounts of storage to run generative AI, in truth, there are methods to use a tier or partitioned architecture to drive value for specific business use cases.</p><p>Somehow, it\u2019s in the generative AI zeitgeist that<a href=\"https://www.networkworld.com/article/964305/what-is-edge-computing-and-how-it-s-changing-the-network.html\"> edge computing</a> won\u2019t work. This is given the processing requirements of generative AI models and the need to drive high-performing inferences. I\u2019m often challenged when I suggest \u201cknowledge at the edge\u201d ar", "language": "en-us", "link": "https://www.infoworld.com/article/3715488/partitioning-an-llm-between-cloud-and-edge.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2020/04/digital_tunnel_streaming_binary_code_by_nadla_gettyimages-170947277_2400x1600-100839612-small.jpg", "title": "Partitioning an LLM between cloud and edge", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-05-28T09:00:00+00:00", "description": "<article>\n\t<section class=\"page\">\n<p>When we first got the personal computer, we didn\u2019t worry much about how things worked. We were, frankly, stunned that we even had such a thing. You had to learn some arcane lingo to type into a DOS prompt. That the computer might be difficult or awkward to use didn\u2019t occur to us. But things gradually got more sophisticated, and when the original Macintosh came out with its powerful graphical user interface, we started realizing that the process of interacting with a computer could matter to us.</p><p>Software developers started having to think not only about how the program was going to get the job done, but also about how the user was going to interact with the program to get the job done. It became clear that a good user interface was something that would sell more software. If it were easy and intuitive to interact with an application, then users could get more done and would love the application.</p><p class=\"jumpTag\"><a href=\"/article/3715333/", "language": "en-us", "link": "https://www.infoworld.com/article/3715333/the-decline-of-the-user-interface.html#tk.rss_all", "manual_status_code": 0, "page_rating": 32, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "http://www.infoworld.com/index.rss", "source_obj__id": 151, "status_code": 0, "tags": [], "thumbnail": "https://images.idgesg.net/images/article/2024/05/shutterstock_1725199078-100963186-small.jpg", "title": "The decline of the user interface", "user": null, "vote": 0}]