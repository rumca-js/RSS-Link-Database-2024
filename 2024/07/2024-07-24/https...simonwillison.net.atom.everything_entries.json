[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-24T18:29:55+00:00", "description": "<p><strong><a href=\"https://www.404media.co/google-is-the-only-search-engine-that-works-on-reddit-now-thanks-to-ai-deal/\">Google is the only search engine that works on Reddit now thanks to AI deal</a></strong></p>\nThis is depressing. As of around June 25th <a href=\"https://www.reddit.com/robots.txt\">reddit.com/robots.txt</a> contains this:</p>\n<pre><code>User-agent: *\nDisallow: /\n</code></pre>\n<p>Along with a link to Reddit's <a href=\"https://support.reddithelp.com/hc/en-us/articles/26410290525844-Public-Content-Policy\">Public Content Policy</a>.</p>\n<p>Is this a direct result of Google's deal to license Reddit content for AI training, rumored <a href=\"https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/\">at $60 million</a>? That's not been confirmed but it looks likely, especially since accessing that <code>robots.txt</code> using the <a href=\"https://search.google.com/test/rich-results\">Google Rich Results testing tool</a> (hence p", "id": 923742, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/24/google-reddit/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Google is the only search engine that works on Reddit now thanks to AI deal", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-24T15:56:23+00:00", "description": "<p><strong><a href=\"https://mistral.ai/news/mistral-large-2407/\">Mistral Large 2</a></strong></p>\nThe second release of a GPT-4 class open weights model in two days, after yesterday's <a href=\"https://simonwillison.net/2024/Jul/23/introducing-llama-31/\">Llama 3.1 405B</a>.</p>\n<p>The weights for this one are under Mistral's <a href=\"https://mistral.ai/licenses/MRL-0.1.md\">Research License</a>, which \"allows usage and modification for research and non-commercial usages\" - so not as open as Llama 3.1. You can use it commercially via the Mistral paid API.</p>\n<p>Mistral Large 2 is 123 billion parameters, \"designed for single-node inference\" (on a very expensive single-node!) and has a 128,000 token context window, the same size as Llama 3.1.</p>\n<p>Notably, according to Mistral's own benchmarks it out-performs the much larger Llama 3.1 405B on their code and math benchmarks. They trained on a lot of code:</p>\n<blockquote>\n<p>Following our experience with <a href=\"https://mistral.ai/news/", "id": 922402, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/24/mistral-large-2/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Mistral Large 2", "user": null, "vote": 0}]