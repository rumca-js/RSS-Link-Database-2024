[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-16T20:12:19+00:00", "description": "<p><a href=\"https://dev.jimgrey.net/2024/07/03/lessons-learned-in-35-years-of-making-software/\">Lessons learned in 35 years of making software</a></p>\nLots of great stuff in here from Jim Grey, with a strong focus on \"soft skills\" (I prefer the term \"professional skills\") around building relationships and making sure your contributions are visible.</p>\n<p>This tip resonated with me in particular:</p>\n<blockquote>\n<p><strong>There is no substitute for working software in Production</strong>. I can\u2019t believe now that I have been part of <em>18-month</em> release projects. This was back in the bad old waterfall days, but even then it was possible to release a lot more frequently than that. The software we build is valuable. It builds the value of the company. When you hold it until it\u2019s perfect, or everything you think it needs to be, you are holding back on building the company\u2019s value. Find the fastest, shortest path to getting the smallest increment of the thing that will work into th", "id": 877044, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/16/lessons-learned/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Lessons learned in 35 years of making software", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-16T18:25:01+00:00", "description": "<p><a href=\"https://eurekalabs.ai/\">Introducing Eureka Labs</a></p>\nAndrej Karpathy's new AI education company, exploring an AI-assisted teaching model:</p>\n<blockquote>\n<p>The teacher still designs the course materials, but they are supported, leveraged and scaled with an AI Teaching Assistant who is optimized to help guide the students through them. This Teacher + AI symbiosis could run an entire curriculum of courses on a common platform.</p>\n</blockquote>\n<p>On Twitter <a href=\"https://twitter.com/karpathy/status/1813263734707790301\">Andrej says</a>:</p>\n<blockquote>\n<p><a href=\"https://twitter.com/EurekaLabsAI\">@EurekaLabsAI</a> is the culmination of my passion in both AI and education over ~2 decades. My interest in education took me from YouTube tutorials on Rubik's cubes to starting CS231n at Stanford, to my more recent Zero-to-Hero AI series. While my work in AI took me from academic research at Stanford to real-world products at Tesla and AGI research at OpenAI. All of my wo", "id": 876037, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/16/introducing-eureka-labs/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Introducing Eureka Labs", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-16T16:29:08+00:00", "description": "<p><a href=\"https://mistral.ai/news/codestral-mamba/\">Codestral Mamba</a></p>\nNew 7B parameter Apache 2.0 licensed model from Mistral, released today. Codestral Mamba is \"a Mamba2 language model specialised in code generation, available under an Apache 2.0 license\".</p>\n<p>This the first LLM from Mistral that uses the <a href=\"https://arxiv.org/abs/2312.00752\">Mamba architecture</a>, as opposed to the much more common Transformers architecture. Mistral say that Mamba can offer faster responses irrespective of input length which makes it ideal for code auto-completion, hence why they chose to specialise the model in code.</p>\n<p>It's available to run locally with the <a href=\"https://github.com/mistralai/mistral-inference\">mistral-inference</a> GPU library, and Mistral say \"For local inference, keep an eye out for support in llama.cpp\" (<a href=\"https://github.com/ggerganov/llama.cpp/issues/7727\">relevant issue</a>).</p>\n<p>It's also available through Mistral's La Plateforme API. I jus", "id": 876038, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/16/codestral-mamba/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Codestral Mamba", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-16T16:06:35+00:00", "description": "<blockquote cite=\"https://twitter.com/random_walker/status/1813231384032649573\"><p>OpenAI and Anthropic focused on building models and not worrying about products. For example, it took 6 months for OpenAI to bother to release a ChatGPT iOS app and 8 months for an Android app!</p>\n<p>Google and Microsoft shoved AI into everything in a panicked race, without thinking about which products would actually benefit from AI and how they should be integrated.</p>\n<p>Both groups of companies forgot the \u201cmake something people want\u201d mantra. The generality of LLMs allowed developers to fool themselves into thinking that they were exempt from the need to find a product-market fit, as if prompting is a replacement for carefully designed products or features. [...]</p>\n<p>But things are changing. OpenAI and Anthropic seem to be transitioning from research labs focused on a speculative future to something resembling regular product companies. If you take all the human-interest elements out of the Open", "id": 874723, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/16/arvind-narayanan/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Arvind Narayanan", "user": null, "vote": 0}]