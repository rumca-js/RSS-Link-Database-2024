# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Playwright with brave browser
 - [https://www.reddit.com/r/webscraping/comments/1e436pm/playwright_with_brave_browser](https://www.reddit.com/r/webscraping/comments/1e436pm/playwright_with_brave_browser)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T19:07:20+00:00

<!-- SC_OFF --><div class="md"><p>I am using playwright (in python) with brave browser by using the &quot;executable_path&quot; feature of Playwright. There are certain websites that throw a 403 with the exception &quot;enable JS and disable add blockers&quot;. Is there a way to overcome this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/happyotaku35"> /u/happyotaku35 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e436pm/playwright_with_brave_browser/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e436pm/playwright_with_brave_browser/">[comments]</a></span>

## Website limiting the max page I can pull from?
 - [https://www.reddit.com/r/webscraping/comments/1e41il6/website_limiting_the_max_page_i_can_pull_from](https://www.reddit.com/r/webscraping/comments/1e41il6/website_limiting_the_max_page_i_can_pull_from)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T18:01:15+00:00

<!-- SC_OFF --><div class="md"><p>Working on pulling all books as seen at Barnes and Noble: <a href="https://www.barnesandnoble.com/b/books/_/N-1z141wbZ29Z8q8?Nrpp=40&amp;page=1">https://www.barnesandnoble.com/b/books/_/N-1z141wbZ29Z8q8?Nrpp=40&amp;page=1</a> </p> <p>When editing the url, the number of results per page only accepts between 20-40 and the last page I can pull is 50 before it returns to the first page.</p> <p>I am using python requests with bs4 and can pull the page data just fine, but am stuck trying to get past this limitation as it somehow shows there are 4456467 results left after the first 2k I can actually pull.</p> <p>Looking at network I don't see any fetch requests with payloads (aside from reviews etc. that require the product ids already) it looks like the the query is the only part that pulls in products.</p> <p>Any experience or insight on how to get around this would be appreciated, thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https

## HELP
 - [https://www.reddit.com/r/webscraping/comments/1e3xk7q/help](https://www.reddit.com/r/webscraping/comments/1e3xk7q/help)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T15:25:46+00:00

<!-- SC_OFF --><div class="md"><p>Im trying to bypass captcha for a website with text captcha and nothing seems to be working. Everyone is using paid captcha bypassers or directly creating their own models. I just want a way to bypass the captcha on a webstite, any suggestions? (GitHub links or even tutorials would be really helpful thanks!)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/bi_simp29"> /u/bi_simp29 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3xk7q/help/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3xk7q/help/">[comments]</a></span>

## How do I scrape emails from contact pages of websites?
 - [https://www.reddit.com/r/webscraping/comments/1e3u5s9/how_do_i_scrape_emails_from_contact_pages_of](https://www.reddit.com/r/webscraping/comments/1e3u5s9/how_do_i_scrape_emails_from_contact_pages_of)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T12:58:33+00:00

<!-- SC_OFF --><div class="md"><p>I have a long list of websites. I'd like to get the contact emails off the pages. What's the cleanest way to do this? Online or locally. Best tool or website?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/BTtheVoice"> /u/BTtheVoice </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3u5s9/how_do_i_scrape_emails_from_contact_pages_of/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3u5s9/how_do_i_scrape_emails_from_contact_pages_of/">[comments]</a></span>

## do you know how to crawl wsj with scrapy
 - [https://www.reddit.com/r/webscraping/comments/1e3rbly/do_you_know_how_to_crawl_wsj_with_scrapy](https://www.reddit.com/r/webscraping/comments/1e3rbly/do_you_know_how_to_crawl_wsj_with_scrapy)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T10:20:30+00:00

<!-- SC_OFF --><div class="md"><p>I just copy the cookie ,but after some requests ,will response 401,and need to pass a slider</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AdPositive9068"> /u/AdPositive9068 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3rbly/do_you_know_how_to_crawl_wsj_with_scrapy/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3rbly/do_you_know_how_to_crawl_wsj_with_scrapy/">[comments]</a></span>

## Scraping several pages when there is no next button on website -> tool recommendation
 - [https://www.reddit.com/r/webscraping/comments/1e3ppxh/scraping_several_pages_when_there_is_no_next](https://www.reddit.com/r/webscraping/comments/1e3ppxh/scraping_several_pages_when_there_is_no_next)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T08:31:33+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone, </p> <p>I am looking for a tool recommendation as I want to scrape a page that cannot be paginated with my favourite scraping tool InstantDataScraper. I would like to scrape<br /> <a href="https://borntobeglobal.com/blog/?query-12-page=14&amp;cst">https://borntobeglobal.com/blog/?query-12-page=14&amp;cst</a> which does not have pagination (next page) but just navigation through the page numbers / url. Can you recommend a free-to-use and easy webscraping tool that can still automatically navigate to the page or where I can easily paste all URLs (&quot;page=2&quot;, &quot;....=3&quot; etc)?</p> <p>Thanks for any hints! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Equal_Highlight_9820"> /u/Equal_Highlight_9820 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3ppxh/scraping_several_pages_when_there_is_no_next/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/

## master data v3 portal scraping
 - [https://www.reddit.com/r/webscraping/comments/1e3n67n/master_data_v3_portal_scraping](https://www.reddit.com/r/webscraping/comments/1e3n67n/master_data_v3_portal_scraping)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T05:41:57+00:00

<!-- SC_OFF --><div class="md"><p>Ministry of corporate affairs, India have a service where we can get the data about the company . The website is mostly used by CA . and it have a captha . I have list of companies and I want to scrap their info which is in &quot; master data v3 portal &quot; .is there any free tool to do so . </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Babuji_EXP"> /u/Babuji_EXP </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3n67n/master_data_v3_portal_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3n67n/master_data_v3_portal_scraping/">[comments]</a></span>

## Good Scraping Libraries
 - [https://www.reddit.com/r/webscraping/comments/1e3ismt/good_scraping_libraries](https://www.reddit.com/r/webscraping/comments/1e3ismt/good_scraping_libraries)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T01:37:01+00:00

<!-- SC_OFF --><div class="md"><p>Hi all. Are the any solid Python scraping libraries that leverage Playwright or Puppeteer? I'm actually mostly interested in studying the code and trying to learn from it. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/qa_anaaq"> /u/qa_anaaq </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3ismt/good_scraping_libraries/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3ismt/good_scraping_libraries/">[comments]</a></span>

## Weekly LinkedIn, X/Twitter, Facebook - 15 Jul 2024
 - [https://www.reddit.com/r/webscraping/comments/1e3i022/weekly_linkedin_xtwitter_facebook_15_jul_2024](https://www.reddit.com/r/webscraping/comments/1e3i022/weekly_linkedin_xtwitter_facebook_15_jul_2024)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T00:57:08+00:00

<!-- SC_OFF --><div class="md"><p>Please direct all your questions about scraping from these sites to this thread</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AutoModerator"> /u/AutoModerator </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3i022/weekly_linkedin_xtwitter_facebook_15_jul_2024/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3i022/weekly_linkedin_xtwitter_facebook_15_jul_2024/">[comments]</a></span>

## I'm trying to scrap this site but i can't figure it out
 - [https://www.reddit.com/r/webscraping/comments/1e3h1bq/im_trying_to_scrap_this_site_but_i_cant_figure_it](https://www.reddit.com/r/webscraping/comments/1e3h1bq/im_trying_to_scrap_this_site_but_i_cant_figure_it)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-15T00:09:32+00:00

<!-- SC_OFF --><div class="md"><p><a href="https://www.uefa.com/euro2024/statistics/teams/">https://www.uefa.com/euro2024/statistics/teams/</a></p> <p>I'm trying to get the data from this site but it seems impossible for me, i dunno if it's too complicated or i just suck. I'm using beautifulsoup btw. Any idea how do i go about that ? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Outsoup2t"> /u/Outsoup2t </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e3h1bq/im_trying_to_scrap_this_site_but_i_cant_figure_it/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e3h1bq/im_trying_to_scrap_this_site_but_i_cant_figure_it/">[comments]</a></span>

