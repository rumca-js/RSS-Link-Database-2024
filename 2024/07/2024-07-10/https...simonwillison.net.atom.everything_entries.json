[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-10T18:56:40+00:00", "description": "<blockquote cite=\"https://twitter.com/alexalbert__/status/1811101055054402019\"><p>Yeah, unfortunately vision prompting has been a tough nut to crack. We've found it's very challenging to improve Claude's actual \"vision\" through just text prompts, but we can of course improve its reasoning and thought process once it extracts info from an image. </p>\n<p>In general, I think vision is still in its early days, although 3.5 Sonnet is noticeably better than older models.</p></blockquote><p class=\"cite\">&mdash; <a href=\"https://twitter.com/alexalbert__/status/1811101055054402019\">Alex Albert (Anthropic)</a>", "id": 843264, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/10/alex-albert/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Alex Albert (Anthropic)", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-10T18:38:10+00:00", "description": "<p><a href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal\">Anthropic cookbook: multimodal</a></p>\nI'm currently on the lookout for high quality sources of information about vision LLMs, including prompting tricks for getting the most out of them.</p>\n<p>This set of Jupyter notebooks from Anthropic (published four months ago to accompany the original Claude 3 models) is the best I've found so far. <a href=\"https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/best_practices_for_vision.ipynb\">Best practices for using vision with Claude</a> includes advice on multi-shot prompting with example, plus this interesting think step-by-step style prompt for improving Claude's ability to count the dogs in an image:</p>\n<blockquote>\n<p>You have perfect vision and pay great attention to detail which makes you an expert at counting objects in images. How many dogs are in this picture? Before providing the answer in <code>&lt;answer&gt;</code> tags, think step", "id": 843265, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/10/anthropic-cookbook-multimodal/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Anthropic cookbook: multimodal", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-10T18:17:14+00:00", "description": "<p><a href=\"https://vlmsareblind.github.io/\">Vision language models are blind</a></p>\nA new paper exploring vision LLMs, comparing GPT-4o, Gemini 1.5 Pro, Claude 3 Sonnet and Claude 3.5 Sonnet (I'm surprised they didn't include Claude 3 Opus and Haiku, which are more interesting than Claude 3 Sonnet in my opinion).</p>\n<p>I don't like the title and framing of this paper. They describe seven tasks that vision models have trouble with - mainly geometric analysis like identifying intersecting shapes or counting things - and use those to support the following statement:</p>\n<blockquote>\n<p>The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses.</p>\n</blockquote>\n<p>This is an interesting result. I've felt starved for information about the strengths and weaknesses of these vision LLMs since the good ones started", "id": 843266, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/10/vision-language-models-are-blind/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Vision language models are blind", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-07-10T17:43:57+00:00", "description": "<blockquote cite=\"https://www.garbageday.email/p/slop-void\"><p>Content slop has three important characteristics. The first being that, to the user, the viewer, the customer, it feels worthless. This might be because it was clearly generated in bulk by a machine or because of how much of that particular content is being created. The next important feature of slop is that feels forced upon us, whether by a corporation or an algorithm. It\u2019s in the name. We\u2019re the little piggies and it\u2019s the gruel in the trough. But the last feature is the most crucial. It not only feels worthless and ubiquitous, it also feels optimized to be so. The Charli XCX \u201cBrat summer\u201d meme does not feel like slop, nor does Kendrick Lamar\u2019s extremely long \u201cNot Like Us\u201d roll out. But Taylor Swift\u2019s cascade of alternate versions of her songs does. The jury\u2019s still out on Sabrina Carpenter. Similarly, last summer\u2019s Barbenheimer phenomenon did not, to me, feel like slop. <em>Dune: Part Two</em> didn\u2019t either. But <em>De", "id": 843267, "language": "en-us", "link": "https://simonwillison.net/2024/Jul/10/ryan-broderick/#atom-everything", "manual_status_code": 0, "page_rating": 29, "page_rating_contents": 90, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Ryan Broderick", "user": null, "vote": 0}]