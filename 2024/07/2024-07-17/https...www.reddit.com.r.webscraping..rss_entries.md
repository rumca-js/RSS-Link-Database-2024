# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## How hard would this be to do ?
 - [https://www.reddit.com/r/webscraping/comments/1e5thgl/how_hard_would_this_be_to_do](https://www.reddit.com/r/webscraping/comments/1e5thgl/how_hard_would_this_be_to_do)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T21:11:05+00:00

<!-- SC_OFF --><div class="md"><p>. i want to make a discord bot that basically scans my area( or other customers areas ) in facebook marketplace and scans almost every item or certain items i give it and basically it scans each item‚Äôs price on sold items section on ebay and if the price say is $100 on facebook and usually sells for $200 on ebay it would notify me/ customer . As i know facebook does not offer api so i‚Äôm pretty sure this is the right sub if not please lmk. If you think any implications would start then that would be much appreciated üëçüèª</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/BowlFew3641"> /u/BowlFew3641 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5thgl/how_hard_would_this_be_to_do/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e5thgl/how_hard_would_this_be_to_do/">[comments]</a></span>

## Speeding up to do a request
 - [https://www.reddit.com/r/webscraping/comments/1e5o5aa/speeding_up_to_do_a_request](https://www.reddit.com/r/webscraping/comments/1e5o5aa/speeding_up_to_do_a_request)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T17:33:17+00:00

<!-- SC_OFF --><div class="md"><p>Hello! I'm using Puppeteer to do web scraping and secure an appointment for my Italian citizenship. The appointment is in another country, and I need to make the request very quickly. The problem is that, due to the high traffic, the request takes about 1 minute to send (the appointment must be booked at a specific time). How can I make it faster? I'm thinking of using a proxy close to the server, but my request must come from my city; otherwise, it will be canceled.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Icy-Improvement-9422"> /u/Icy-Improvement-9422 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5o5aa/speeding_up_to_do_a_request/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e5o5aa/speeding_up_to_do_a_request/">[comments]</a></span>

## Need help with a quick hobby scrape -- will tip
 - [https://www.reddit.com/r/webscraping/comments/1e5o2ez/need_help_with_a_quick_hobby_scrape_will_tip](https://www.reddit.com/r/webscraping/comments/1e5o2ez/need_help_with_a_quick_hobby_scrape_will_tip)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T17:29:59+00:00

<!-- SC_OFF --><div class="md"><p>Goal of the project is to export game details/log to google sheets for record keeping.</p> <p>example url: <a href="https://rally-the-troops.com/pax-pamir/play.html?game=79299">https://rally-the-troops.com/pax-pamir/play.html?game=79299</a></p> <p>I tried apify and here are the results: <a href="https://api.apify.com/v2/datasets/2eaM9hdSoLLMxnqVb/items?clean=true&amp;format=html&amp;limit=1000">https://api.apify.com/v2/datasets/2eaM9hdSoLLMxnqVb/items?clean=true&amp;format=html&amp;limit=1000</a></p> <p>But unfortunately it doesn't capture the player names in the top right. I tried to find the xpath of these items for direct import into google sheets with IMPORTXML() but couldn't get that to work either.</p> <p>Any thoughts? Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/fillingupthecorners"> /u/fillingupthecorners </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5o2ez/need_he

## Linkedin Scraping
 - [https://www.reddit.com/r/webscraping/comments/1e5mzrf/linkedin_scraping](https://www.reddit.com/r/webscraping/comments/1e5mzrf/linkedin_scraping)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T16:46:55+00:00

<!-- SC_OFF --><div class="md"><p>How to scrape private Linkedin profiles. Any leads?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Starry_Nomad"> /u/Starry_Nomad </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5mzrf/linkedin_scraping/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e5mzrf/linkedin_scraping/">[comments]</a></span>

## 403 errors
 - [https://www.reddit.com/r/webscraping/comments/1e5huvr/403_errors](https://www.reddit.com/r/webscraping/comments/1e5huvr/403_errors)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T13:11:47+00:00

<!-- SC_OFF --><div class="md"><p>So I've been scraping a website for 6-7 months now using proxies (both ipv4 and recently ipv6). Had no issues whatsoever. I had 50 IPs which I rotated in the code and had about 40-45 queries so it was a request each second. I didn't do anything more than spoof my User agent.</p> <p>Today I'm getting hit by 403 errors. Bought new IPs and it's the same deal. I tested 1 proxy IP with 1 query and still was blocked. Interestingly if I access the website through the proxy on my browser I can access it fine with no blocks. Any suggestions on what they might've done or what to look for?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Fuarkistani"> /u/Fuarkistani </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5huvr/403_errors/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e5huvr/403_errors/">[comments]</a></span>

## Anyone solved this puppeteer detection method?
 - [https://www.reddit.com/r/webscraping/comments/1e5delp/anyone_solved_this_puppeteer_detection_method](https://www.reddit.com/r/webscraping/comments/1e5delp/anyone_solved_this_puppeteer_detection_method)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T08:55:01+00:00

<!-- SC_OFF --><div class="md"><p>Im wondering if anyone solved this: <a href="https://datadome.co/threat-research/how-new-headless-chrome-the-cdp-signal-are-impacting-bot-detection/">https://datadome.co/threat-research/how-new-headless-chrome-the-cdp-signal-are-impacting-bot-detection/</a></p> <p>With those simple checks they can detect if devtool is opened or if the browser is automated with puppeteer: </p> <p>var detected = false;<br /> var e = new Error();<br /> Object.defineProperty(e, 'stack', {<br /> get() {<br /> detected = true;<br /> }<br /> });<br /> console.log(e);</p> <p>// detected will be true if puppeteer or dev tools used</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Puzzleheaded_Chair16"> /u/Puzzleheaded_Chair16 </a> <br /> <span><a href="https://www.reddit.com/r/webscraping/comments/1e5delp/anyone_solved_this_puppeteer_detection_method/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1e5d

## HomeDepot Scraping-Cookie issues
 - [https://www.reddit.com/r/webscraping/comments/1e5axw1/homedepot_scrapingcookie_issues](https://www.reddit.com/r/webscraping/comments/1e5axw1/homedepot_scrapingcookie_issues)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-07-17T06:08:07+00:00

<!-- SC_OFF --><div class="md"><p>I am working on a project to scrape <a href="http://HomeDepot.com">HomeDepot.com</a> website. This website has graphql request. So I have to send multiple requests to the server to get construct my desired response. If I send the request without the cookies, my request gets rejected or there is some error in the response. But when I send request including cookies my response is correct. The issue is that how to scrape this website because constructing cookies json is pretty difficult, isn't it? I have attached the cookies key that are being used in the request(I have removed the values FYI)</p> <pre><code> cookies = { 'THD_PERSIST': '', 'THD_CACHE_NAV_PERSIST': '', 'DELIVERY_ZIP_TYPE': 'DEFAULT', 'thda.u': '68a', '_px_f394gi7Fvmc43dfg_user_id': '', 'QuantumMetricUserID': '', 'ajs_anonymous_id': '', 'trx': '', 'aam_uuid': '', '_gcl_au': '1.1..', '_ga': 'GA1.2..', '_ga_9H2R4ZXG4J': '', 'THD_NR': '1', 'THD_SESSION': '', 'THD_CACHE_NAV_SESSION': '', 'ak_b

