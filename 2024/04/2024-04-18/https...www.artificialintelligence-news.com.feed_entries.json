[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-18T14:39:18+00:00", "description": "<p>Mistral AI has released Mixtral 8x22B, which sets a new benchmark for open source models in performance and efficiency. The model boasts robust multilingual capabilities and superior mathematical and coding prowess. Mixtral 8x22B operates as a Sparse Mixture-of-Experts (SMoE) model, utilising just 39 billion of its 141 billion parameters when active. Beyond its efficiency, the<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models/\" title=\"ReadMixtral 8x22B sets new benchmark for open models\">... Read more &#187;</a></p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models/\">Mixtral 8x22B sets new benchmark for open models</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>", "language": "en-GB", "link": "https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.artificialintelligence-news.com/feed", "source_obj__id": 10, "status_code": 0, "tags": [], "thumbnail": null, "title": "Mixtral 8x22B sets new benchmark for open models", "user": null, "vote": 0}]