# Source:AI News, URL:https://www.artificialintelligence-news.com/feed, language:en-GB

## Mixtral 8x22B sets new benchmark for open models
 - [https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models](https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models)
 - RSS feed: https://www.artificialintelligence-news.com/feed
 - date published: 2024-04-18T14:39:18+00:00

<p>Mistral AI has released Mixtral 8x22B, which sets a new benchmark for open source models in performance and efficiency. The model boasts robust multilingual capabilities and superior mathematical and coding prowess. Mixtral 8x22B operates as a Sparse Mixture-of-Experts (SMoE) model, utilising just 39 billion of its 141 billion parameters when active. Beyond its efficiency, the<a class="excerpt-read-more" href="https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models/" title="ReadMixtral 8x22B sets new benchmark for open models">... Read more &#187;</a></p>
<p>The post <a href="https://www.artificialintelligence-news.com/2024/04/18/mixtral-8x22b-sets-new-benchmark-open-models/">Mixtral 8x22B sets new benchmark for open models</a> appeared first on <a href="https://www.artificialintelligence-news.com">AI News</a>.</p>

