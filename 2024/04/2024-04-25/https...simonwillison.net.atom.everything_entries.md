# Source:Simon Willison's Weblog, URL:https://simonwillison.net/atom/everything, language:en-us

## Quoting Alex Jason, via Adam Savage
 - [https://simonwillison.net/2024/Apr/25/alex-jason-via-adam-savage/#atom-everything](https://simonwillison.net/2024/Apr/25/alex-jason-via-adam-savage/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-25T14:17:57+00:00

<blockquote cite="https://web.archive.org/web/20170703154530/https://www.tested.com/art/makers/557288-origin-only-difference-between-screwing-around-and-science-writing-it-down/"><p>The only difference between screwing around and science is writing it down</p></blockquote><p class="cite">&mdash; <a href="https://web.archive.org/web/20170703154530/https://www.tested.com/art/makers/557288-origin-only-difference-between-screwing-around-and-science-writing-it-down/">Alex Jason, via Adam Savage</a>

## Quoting James Betker
 - [https://simonwillison.net/2024/Apr/25/james-betker/#atom-everything](https://simonwillison.net/2024/Apr/25/james-betker/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-25T05:13:04+00:00

<blockquote cite="https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/"><p>I’ve been at OpenAI for almost a year now. In that time, I’ve trained a lot of generative models. [...] It’s becoming awfully clear to me that these models are truly approximating their datasets to an incredible degree. [...] What this manifests as is – trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. [...] This is a surprising observation! It implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.</p></blockquote><p class="cite">&mdash; <a href="https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/">James Betker</a>

## Blogmarks that use markdown
 - [https://simonwillison.net/2024/Apr/25/blogmarks-that-use-markdown/#atom-everything](https://simonwillison.net/2024/Apr/25/blogmarks-that-use-markdown/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-25T04:34:18+00:00

<p><a href="https://simonwillison.net/dashboard/blogmarks-that-use-markdown/">Blogmarks that use markdown</a></p>
I needed to attach a correction to an older blogmark (my 20-year old name for short-form links with commentary on my blog) today - but the commentary field has always been text, not HTML, so I didn't have a way to add the necessary link.</p>
<p>This motivated me to finally add optional <strong>Markdown</strong> support for blogmarks to my blog's custom Django CMS. I then went through and added inline code markup to a bunch of different older posts, and built this Django SQL Dashboard to keep track of which posts I had updated.

## No, Most Books Don't Sell Only a Dozen Copies
 - [https://simonwillison.net/2024/Apr/25/no-most-books-dont-sell-only-a-dozen-copies/#atom-everything](https://simonwillison.net/2024/Apr/25/no-most-books-dont-sell-only-a-dozen-copies/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-25T03:41:12+00:00

<p><a href="https://countercraft.substack.com/p/no-most-books-dont-sell-only-a-dozen">No, Most Books Don&#x27;t Sell Only a Dozen Copies</a></p>
<p>I linked to a story the other day about book sales claiming &quot;90 percent of them sold fewer than 2,000 copies and 50 percent sold less than a dozen copies&quot;, based on numbers released in the Penguin antitrust lawsuit. It turns out those numbers were interpreted incorrectly.</p>

<p>In this piece from September 2022 Lincoln Michel addresses this and other common misconceptions about book statistics.</p>

<p>Understanding these numbers requires understanding a whole lot of intricacies about how publishing actually works. Here&#x27;s one illustrative snippet:</p>

<p>&quot;Take the statistic that most published books only sell 99 copies. This seems shocking on its face. But if you dig into it, you’ll notice it was counting one year’s sales of all books that were in BookScan’s system. That’s quite different statistic than saying most b

## Snowflake Arctic Cookbook
 - [https://simonwillison.net/2024/Apr/25/snowflake-arctic-cookbook/#atom-everything](https://simonwillison.net/2024/Apr/25/snowflake-arctic-cookbook/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-25T02:47:50+00:00

<p><a href="https://www.snowflake.com/en/data-cloud/arctic/cookbook/">Snowflake Arctic Cookbook</a></p>
<p>Today&#x27;s big model release was Snowflake Arctic, an enormous 480B model with a 128×3.66B MoE (Mixture of Experts) architecture. It&#x27;s Apache 2 licensed and Snowflake state that &quot;in addition, we are also open sourcing all of our data recipes and research insights.&quot;</p>

<p>The research insights will be shared on this Arctic Cookbook blog - which currently has two articles covering the MoE architecture and describing how they optimized their training run in great detail.</p>

<p>They also list dozens of &quot;coming soon&quot; posts, which should be pretty interesting given how much depth they&#x27;ve provided in their writing so far.</p>

