[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-23T19:00:54+00:00", "description": "<blockquote cite=\"https://newsletter.pragmaticengineer.com/p/bluesky\"><p>We [Bluesky] took a somewhat novel approach of giving every user their own SQLite database. By removing the Postgres dependency, we made it possible to run a \u2018PDS in a box\u2019 [Personal Data Server] without having to worry about managing a database. We didn\u2019t have to worry about things like replicas or failover. For those thinking this is irresponsible: don\u2019t worry, we are backing up all the data on our PDSs!<br /><br />SQLite worked really well because the PDS \u2013 in its ideal form \u2013 is a single-tenant system. We owned up to that by having these single tenant SQLite databases.</p></blockquote><p class=\"cite\">&mdash; <a href=\"https://newsletter.pragmaticengineer.com/p/bluesky\">Daniel Holmgren</a>", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/23/daniel-holmgren/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Daniel Holmgren", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-23T17:40:16+00:00", "description": "<p><a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf\">microsoft/Phi-3-mini-4k-instruct-gguf</a></p>\n<p>Microsoft&#x27;s Phi-3 LLM is out and it&#x27;s really impressive. This 4,000 token context GGUF model is just a 2.2GB (for the Q4 version) and ran on my Mac using the llamafile option described in the README. I could then run prompts through it using the llm-llamafile plugin.</p>\n\n<p>The vibes are good! Initial test prompts I&#x27;ve tried feel similar to much larger 7B models, despite using just a few GBs of RAM. Tokens are returned fast too - it feels like the fastest model I&#x27;ve tried yet.</p>\n\n    <p>Via <a href=\"https://twitter.com/simonw/status/1782813304685310275\">@simonw</a></p>", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/23/phi-3-mini-4k/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "microsoft/Phi-3-mini-4k-instruct-gguf", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-23T16:30:00+00:00", "description": "<p>Llama 3 landed on Thursday. I ended up updating a whole bunch of different plugins to work with it, described in <a href=\"https://simonwillison.net/2024/Apr/22/llama-3/\">Options for accessing Llama 3 from the terminal using LLM</a>.</p>\n<p>I also wrote up the talk I gave at Stanford a few weeks ago: <a href=\"https://simonwillison.net/2024/Apr/17/ai-for-data-journalism/\">AI for Data Journalism: demonstrating what we can do with this stuff right now</a>.</p>\n<p>That talk had 12 different live demos in it, and a bunch of those were software that I hadn't released yet when I gave the talk - so I spent quite a bit of time cleaning those up for release. The most notable of those is <a href=\"https://datasette.io/plugins/datasette-query-assistant\">datasette-query-assistant</a>, a plugin built on top of Claude 3 that takes a question in English and converts that into a SQL query. Here's the <a href=\"https://www.youtube.com/watch?v=BJxPKr6ixSM&amp;t=11m08s\">section of that video with the dem", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/23/weeknotes/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Weeknotes: Llama 3, AI for Data Journalism, llm-evals and datasette-secrets", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-23T03:36:32+00:00", "description": "<p><a href=\"https://arxiv.org/abs/2404.13208\">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</a></p>\n<p>By far the most detailed paper on prompt injection I&#x27;ve seen yet from OpenAI, published a few days ago and with six credited authors: Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke and Alex Beutel.</p>\n\n<p>The paper notes that prompt injection mitigations which completely refuse any form of instruction in an untrusted prompt may not actually be ideal: some forms of instruction are harmless, and refusing them may provide a worse experience.</p>\n\n<p>Instead, it proposes a hierarchy - where models are trained to consider if instructions from different levels conflict with or support the goals of the higher-level instructions - if they are aligned or misaligned with them.</p>\n\n<p>The authors tested this idea by fine-tuning a model on top of GPT 3.5, and claim that it shows greatly improved performance against numerous prompt inj", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/23/the-instruction-hierarchy/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-23T03:00:18+00:00", "description": "<blockquote cite=\"https://arxiv.org/html/2404.14219v1\"><p>We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone.</p></blockquote><p class=\"cite\">&mdash; <a href=\"https://arxiv.org/html/2404.14219v1\">Phi-3 Technical Report</a>", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/23/phi-3-technical-report/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Quoting Phi-3 Technical Report", "user": null, "vote": 0}]