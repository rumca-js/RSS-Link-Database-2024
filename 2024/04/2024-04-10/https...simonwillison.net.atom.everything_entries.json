[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-10T05:09:20+00:00", "description": "<p>I'm a bit behind on my <a href=\"https://simonwillison.net/tags/weeknotes/\">weeknotes</a>, so there's a lot to cover here. But first... a review of the last 24 hours of Large Language Model news. All times are in US Pacific.</p>\n<ul>\n<li>11:01am: Google Gemini Pro 1.5 hits general availability, here's <a href=\"https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html\">the blog post</a> - their 1 million token context GPT-4 class model now has no waitlist, is available to anyone in 180 countries (not including Europe or the UK as far as I can tell) and most impressively all the API has a <strong>free tier</strong> that allows up to 50 requests a day, though rate limited to 2 per minute. Beyond that you can pay $7/million input tokens and $21/million output tokens, which is slightly less than GPT-4 Turbo and a little more than Claude 3 Sonnet. Gemini Pro also now support audio inputs and system prompts.</li>\n<li>11:44am: OpenAI finally released t", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/10/weeknotes-llm-releases/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Three major LLM releases in 24 hours (plus weeknotes)", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-10T02:38:55+00:00", "description": "<p><a href=\"https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html\">Gemini 1.5 Pro public preview</a></p>\n<p>Huge release from Google: Gemini 1.5 Pro - the GPT-4 competitive model with the incredible 1 million token context length - is now available without a waitlist in 180+ countries (including the USA but not Europe or the UK as far as I can tell)... and the API is free for 50 requests/day (rate limited to 2/minute).</p>\n\n<p>Beyond that you&#x27;ll need to pay - $7/million input tokens and $21/million output tokens, which is slightly less than GPT-4 Turbo and a little more than Claude 3 Sonnet.</p>\n\n<p>They also announced audio input (up to 9.5 hours in a single prompt), system instruction support and a new JSON mod.</p>\n\n    <p>Via <a href=\"https://twitter.com/liambolling/status/1777758743637483562\">@liambolling</a></p>", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/10/gemini-15-pro-public-preview/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Gemini 1.5 Pro public preview", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-04-10T02:31:27+00:00", "description": "<p><a href=\"https://twitter.com/MistralAI/status/1777869263778291896\">Mistral tweet a magnet link for mixtral-8x22b</a></p>\n<p>Another open model release from Mistral using their now standard operating procedure of tweeting out a raw torrent link.</p>\n\n<p>This one is an 8x22B Mixture of Experts model. Their previous most powerful openly licensed release was Mixtral 8x7B, so this one is a whole lot bigger (a 281GB download) - and apparently has a 65,536 context length, at least according to initial rumors on Twitter.</p>", "language": "en-us", "link": "https://simonwillison.net/2024/Apr/10/mixtral-8x22b/#atom-everything", "manual_status_code": 0, "page_rating": 100, "page_rating_contents": 100, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://simonwillison.net/atom/everything", "source_obj__id": 423, "status_code": 0, "tags": [], "thumbnail": null, "title": "Mistral tweet a magnet link for mixtral-8x22b", "user": null, "vote": 0}]