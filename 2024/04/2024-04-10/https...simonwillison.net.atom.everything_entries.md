# Source:Simon Willison's Weblog, URL:https://simonwillison.net/atom/everything, language:en-us

## Three major LLM releases in 24 hours (plus weeknotes)
 - [https://simonwillison.net/2024/Apr/10/weeknotes-llm-releases/#atom-everything](https://simonwillison.net/2024/Apr/10/weeknotes-llm-releases/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-10T05:09:20+00:00

<p>I'm a bit behind on my <a href="https://simonwillison.net/tags/weeknotes/">weeknotes</a>, so there's a lot to cover here. But first... a review of the last 24 hours of Large Language Model news. All times are in US Pacific.</p>
<ul>
<li>11:01am: Google Gemini Pro 1.5 hits general availability, here's <a href="https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html">the blog post</a> - their 1 million token context GPT-4 class model now has no waitlist, is available to anyone in 180 countries (not including Europe or the UK as far as I can tell) and most impressively all the API has a <strong>free tier</strong> that allows up to 50 requests a day, though rate limited to 2 per minute. Beyond that you can pay $7/million input tokens and $21/million output tokens, which is slightly less than GPT-4 Turbo and a little more than Claude 3 Sonnet. Gemini Pro also now support audio inputs and system prompts.</li>
<li>11:44am: OpenAI finally released t

## Gemini 1.5 Pro public preview
 - [https://simonwillison.net/2024/Apr/10/gemini-15-pro-public-preview/#atom-everything](https://simonwillison.net/2024/Apr/10/gemini-15-pro-public-preview/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-10T02:38:55+00:00

<p><a href="https://developers.googleblog.com/2024/04/gemini-15-pro-in-public-preview-with-new-features.html">Gemini 1.5 Pro public preview</a></p>
<p>Huge release from Google: Gemini 1.5 Pro - the GPT-4 competitive model with the incredible 1 million token context length - is now available without a waitlist in 180+ countries (including the USA but not Europe or the UK as far as I can tell)... and the API is free for 50 requests/day (rate limited to 2/minute).</p>

<p>Beyond that you&#x27;ll need to pay - $7/million input tokens and $21/million output tokens, which is slightly less than GPT-4 Turbo and a little more than Claude 3 Sonnet.</p>

<p>They also announced audio input (up to 9.5 hours in a single prompt), system instruction support and a new JSON mod.</p>

    <p>Via <a href="https://twitter.com/liambolling/status/1777758743637483562">@liambolling</a></p>

## Mistral tweet a magnet link for mixtral-8x22b
 - [https://simonwillison.net/2024/Apr/10/mixtral-8x22b/#atom-everything](https://simonwillison.net/2024/Apr/10/mixtral-8x22b/#atom-everything)
 - RSS feed: https://simonwillison.net/atom/everything
 - date published: 2024-04-10T02:31:27+00:00

<p><a href="https://twitter.com/MistralAI/status/1777869263778291896">Mistral tweet a magnet link for mixtral-8x22b</a></p>
<p>Another open model release from Mistral using their now standard operating procedure of tweeting out a raw torrent link.</p>

<p>This one is an 8x22B Mixture of Experts model. Their previous most powerful openly licensed release was Mixtral 8x7B, so this one is a whole lot bigger (a 281GB download) - and apparently has a 65,536 context length, at least according to initial rumors on Twitter.</p>

