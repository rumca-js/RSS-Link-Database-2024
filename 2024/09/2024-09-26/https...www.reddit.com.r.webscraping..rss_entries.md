# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Saving store info for vector search or RAG in the future
 - [https://www.reddit.com/r/webscraping/comments/1fq9lpl/saving_store_info_for_vector_search_or_rag_in_the](https://www.reddit.com/r/webscraping/comments/1fq9lpl/saving_store_info_for_vector_search_or_rag_in_the)
 - RSS feed: $source
 - date published: 2024-09-26T23:04:05+00:00

<!-- SC_OFF --><div class="md"><p>Hey there</p> <ul> <li>scraping every car wash in a certain country</li> <li>putting into searchable database with simple front end </li> <li>what is the best way to grab all the text off their homepage so I can use some kind of AI/elastic search/vector db to find matching locations</li> </ul> <p>For example if I want to find all car washes that mention they are family owned</p> <p>Appreciate any help here </p> <p>Many thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/apple1064"> /u/apple1064 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fq9lpl/saving_store_info_for_vector_search_or_rag_in_the/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fq9lpl/saving_store_info_for_vector_search_or_rag_in_the/">[comments]</a></span>

## Getting the CMS used for over 1 Million Sites
 - [https://www.reddit.com/r/webscraping/comments/1fq92bk/getting_the_cms_used_for_over_1_million_sites](https://www.reddit.com/r/webscraping/comments/1fq92bk/getting_the_cms_used_for_over_1_million_sites)
 - RSS feed: $source
 - date published: 2024-09-26T22:38:54+00:00

<!-- SC_OFF --><div class="md"><p>Hi All,</p> <p>Hypothetically, if you had a week to find out as quickly as possible which site out of the 1 million unique site URLs you had ran on Wordpress, how would you go about it as quickly as possible?</p> <p>Using <a href="https://github.com/richardpenman/builtwith">https://github.com/richardpenman/builtwith</a> does the job but it&#39;s quite slow. </p> <p>Using scrapy and looking for anything wix related in response body would be quite fast but could potentially produce inaccuracies depending on what is searched. </p> <p>Interested to know the approaches from some of the wizards which reside here.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/theresumeartisan"> /u/theresumeartisan </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fq92bk/getting_the_cms_used_for_over_1_million_sites/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fq92bk/get

## Open-source tool to perform sentiment analysis on websites
 - [https://www.reddit.com/r/webscraping/comments/1fq8wek/opensource_tool_to_perform_sentiment_analysis_on](https://www.reddit.com/r/webscraping/comments/1fq8wek/opensource_tool_to_perform_sentiment_analysis_on)
 - RSS feed: $source
 - date published: 2024-09-26T22:31:22+00:00

<!-- SC_OFF --><div class="md"><p>Hey everyone!</p> <p>I&#39;m a software engineer who builds web automation tools, and wanted to share this python script I put together for sentiment analysis on YouTube comments. The tool uses OpenAI API and AgentQL (AI scraper built by my team) to analyze the emotional tone of comments on any YouTube video.</p> <p>The reason I’m sharing this is because the project is designed to be easily expandable to other types of forums and websites. So ideally, you can scrape the sentiment of ANY site. The AgentQL scraper I use handles all of the data extraction logic with AI calls rather than me having to write custom scraping code or deal with complex APIs.</p> <p>I’d like to make this a tool and open source it. How would you like it to work in your workflow? Check out the github <a href="https://github.com/tinyfish-io/fish-tank/tree/main/application_examples/perform_sentiment_analysis">here</a> and let me know what you guys think. It’s free to set up and use

## Webscraping script to book list budget
 - [https://www.reddit.com/r/webscraping/comments/1fq7yuz/webscraping_script_to_book_list_budget](https://www.reddit.com/r/webscraping/comments/1fq7yuz/webscraping_script_to_book_list_budget)
 - RSS feed: $source
 - date published: 2024-09-26T21:49:06+00:00

<!-- SC_OFF --><div class="md"><p>Hello there people,</p> <p>So, i&#39;m making a webscraping script in python to perform a webscraping function to get prices and book stores URLs. Since it&#39;s a big ass long list, webscraping was the way to go.</p> <p>To give proper context, the list is on a excel spreadsheet, on the column A, is the item number, on column Bm the book title, on the C, the authors name, on D, the ISBN number, and E, the publisher name.</p> <p><em>What the code should to is to read the titles, authors name&#39;s, and infos on columns B to E, search in, and get the URLs in google at online bookstores, and return the price and the URLs where this info was taken. It should return three different prices and URLs for the budget analysis.</em></p> <p>I&#39;ve done a code and it kinda worked, partially, it got me the URLs, but didn&#39;t returned me the prices. I&#39;m stuck on that and need some help to get this also working. Could anybody look at my could and give me some

## Having a hard time webscraping soccer data
 - [https://www.reddit.com/r/webscraping/comments/1fq1ec8/having_a_hard_time_webscraping_soccer_data](https://www.reddit.com/r/webscraping/comments/1fq1ec8/having_a_hard_time_webscraping_soccer_data)
 - RSS feed: $source
 - date published: 2024-09-26T17:11:08+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1fq1ec8/having_a_hard_time_webscraping_soccer_data/"> <img src="https://preview.redd.it/yxnmkdwur6rd1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bbea44671f6d08b13cc122bcd26661262e2d213" alt="Having a hard time webscraping soccer data " title="Having a hard time webscraping soccer data " /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hello everyone, </p> <p>I’m working on this little project with a friend where we need to scrape all games in the League Two, La Liga and La Segunda Division.</p> <p>He wants this data in each teams last 5 league games:</p> <p>O/U 0.5 total goals O/U 1.5 total goals O/U 2.5 total goals O/U 5.5 total goals</p> <p>O/U 0.5 team goals O/U 1.5 team goals</p> <p>O/U 0.5 1st/2nd half goals O/U 1.5 1st/2nd half goals O/U 2.5 1st/2nd half goals O/U 5.5 1st/2nd half goals</p> <p>Difference between score (for example: Team A 3 - 1 Team B = difference of 2 goals in favour of Team A)</

## Need Help Scraping Business Locations Across the U.S.
 - [https://www.reddit.com/r/webscraping/comments/1fpys12/need_help_scraping_business_locations_across_the](https://www.reddit.com/r/webscraping/comments/1fpys12/need_help_scraping_business_locations_across_the)
 - RSS feed: $source
 - date published: 2024-09-26T15:21:25+00:00

<!-- SC_OFF --><div class="md"><p>I stumbled on this subreddit and figured it was worth a shot asking for help.</p> <p>I&#39;ve been going to large company websites and using their &quot;find a location&quot; site map page and copy/pasting each branch address, state, zip code, and phone number into an excel sheet. Some of these businesses have over 500 locations and at this rate it&#39;s going to take 10 years to do them all.</p> <p>Then I learned a little bit about scraping. I asked chatGPT to help me with code and instructions, but it&#39;s going over my head at the moment.</p> <p>Anyone have any advice or can help with someone new just starting this sort of thing?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/averymessypoop"> /u/averymessypoop </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fpys12/need_help_scraping_business_locations_across_the/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscr

## How do I scrape a website with a login page
 - [https://www.reddit.com/r/webscraping/comments/1fpxrbn/how_do_i_scrape_a_website_with_a_login_page](https://www.reddit.com/r/webscraping/comments/1fpxrbn/how_do_i_scrape_a_website_with_a_login_page)
 - RSS feed: $source
 - date published: 2024-09-26T14:38:27+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1fpxrbn/how_do_i_scrape_a_website_with_a_login_page/"> <img src="https://external-preview.redd.it/YhVOVxJeJnORDtwl1gezMR4jQhuiRftduayJEB-wrzU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9b78c163af207f1b676d2957678d8f51a67f45e9" alt="How do I scrape a website with a login page" title="How do I scrape a website with a login page" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hi, I&#39;m trying to scrape this page to get the balance of my public transport card, the problem is that when I login with python requests the url redirects me back to the main page, for some reason it is not accessing.</p> <p>I must clarify that I am new with web scraping and surely my script is not the best. Basically what I tried was to send a POST request with the payload that I got from the Network section in the browser development tools.</p> <p>This is how the login page looks like where I have to enter my data.</p> <p><a hre

## EmailSpy - Find public emails across domains. Built with n8n, free, fully cloneable/ self-hostable
 - [https://www.reddit.com/r/webscraping/comments/1fptnet/emailspy_find_public_emails_across_domains_built](https://www.reddit.com/r/webscraping/comments/1fptnet/emailspy_find_public_emails_across_domains_built)
 - RSS feed: $source
 - date published: 2024-09-26T11:13:44+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1fptnet/emailspy_find_public_emails_across_domains_built/"> <img src="https://external-preview.redd.it/67dVbROIPXOypkCAN_s8iAcztC448SqfHtHRVj_-sKw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ee0011677d899b43a6561e3865ce68787ca2a4d" alt="EmailSpy - Find public emails across domains. Built with n8n, free, fully cloneable/ self-hostable" title="EmailSpy - Find public emails across domains. Built with n8n, free, fully cloneable/ self-hostable" /> </a> </td><td> &#32; submitted by &#32; <a href="https://www.reddit.com/user/EbbConstant4162"> /u/EbbConstant4162 </a> <br/> <span><a href="https://www.producthunt.com/posts/emailspy">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fptnet/emailspy_find_public_emails_across_domains_built/">[comments]</a></span> </td></tr></table>

## Same request works on developer console fetch but not on python
 - [https://www.reddit.com/r/webscraping/comments/1fpt8tl/same_request_works_on_developer_console_fetch_but](https://www.reddit.com/r/webscraping/comments/1fpt8tl/same_request_works_on_developer_console_fetch_but)
 - RSS feed: $source
 - date published: 2024-09-26T10:47:39+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1fpt8tl/same_request_works_on_developer_console_fetch_but/"> <img src="https://b.thumbs.redditmedia.com/arDWq4jKjhmPmJqSF_Rrc8PqDX9-FIsU0jVrKVw1uys.jpg" alt="Same request works on developer console fetch but not on python" title="Same request works on developer console fetch but not on python" /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>Hello friends, i am trying to scrape some Appointment page to see available times but i cant do this request with python. It works flawlessly on developer console but i am getting 403 on python requests.</p> <p>What am i missing? Do they have some kinda bot protection?</p> <p>fetch(&quot;<a href="https://api.url.com/api/Appointment/Date?fId=1">https://api.url.com/api/Appointment/Date?fId=1</a>&quot;, {<br/> &quot;headers&quot;: {<br/> &quot;accept&quot;: &quot;application/json&quot;,<br/> &quot;sec-ch-ua&quot;: &quot;\&quot;Not/A)Brand\&quot;;v=\&quot;8\&quot;, \&quot;Chrom

## Google Reviewer pages
 - [https://www.reddit.com/r/webscraping/comments/1fps6li/google_reviewer_pages](https://www.reddit.com/r/webscraping/comments/1fps6li/google_reviewer_pages)
 - RSS feed: $source
 - date published: 2024-09-26T09:32:41+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m looking to extract information from a reviewer page on Maps. Any tips?</p> <p>e.g.<br/> <a href="https://www.google.com/maps/contrib/100939884779737895108/reviews?hl=en-GB">https://www.google.com/maps/contrib/100939884779737895108/reviews?hl=en-GB</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/taariqismail"> /u/taariqismail </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fps6li/google_reviewer_pages/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fps6li/google_reviewer_pages/">[comments]</a></span>

## Any ideas to scrape any URLs on e-commerce webpage?
 - [https://www.reddit.com/r/webscraping/comments/1fpp642/any_ideas_to_scrape_any_urls_on_ecommerce_webpage](https://www.reddit.com/r/webscraping/comments/1fpp642/any_ideas_to_scrape_any_urls_on_ecommerce_webpage)
 - RSS feed: $source
 - date published: 2024-09-26T05:42:34+00:00

<!-- SC_OFF --><div class="md"><p>Since every web store has different structure, I find it very hard to implement scraping any URL’s product page info. Some sites work some don’t.</p> <p>Is there any ways to universally scrap various e-commerce product pages? Or you should work on individual site? If it’s hard, any recommendations on external services?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Mattab0801"> /u/Mattab0801 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fpp642/any_ideas_to_scrape_any_urls_on_ecommerce_webpage/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fpp642/any_ideas_to_scrape_any_urls_on_ecommerce_webpage/">[comments]</a></span>

## Sources of proxies
 - [https://www.reddit.com/r/webscraping/comments/1fpouku/sources_of_proxies](https://www.reddit.com/r/webscraping/comments/1fpouku/sources_of_proxies)
 - RSS feed: $source
 - date published: 2024-09-26T05:20:32+00:00

<!-- SC_OFF --><div class="md"><p>I am thinking of how to create a ton of proxies, with unique IPs, that are unlikely to be IPs that are already sending many requests (or any requests) to the sites I want to scrape.</p> <p>But at the same time be cheap and reliable.</p> <p>My initial thoughts were to go with something like lambda functions in AWS.</p> <p>Has anyone tried this? How well did it work?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/lionhydrathedeparted"> /u/lionhydrathedeparted </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fpouku/sources_of_proxies/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fpouku/sources_of_proxies/">[comments]</a></span>

