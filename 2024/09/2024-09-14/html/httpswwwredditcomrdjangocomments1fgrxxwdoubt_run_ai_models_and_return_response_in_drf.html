<!DOCTYPE html><html><head>   <meta>     <title>[ Doubt ] Run AI models and return response in DRF</title>   </meta>   <style>
.youtube_player_container {
    position: relative;
    width: 50%;
    padding-bottom: 26.25%;
    /* background-color: yellow; */
}
.youtube_player_frame {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border: 0;
}
       </style></head><body><a href="index.html"><h2>Index</h2></a><a href="https://www.reddit.com/r/django/comments/1fgrxxw/doubt_run_ai_models_and_return_response_in_drf"><h1>[1145750] [ Doubt ] Run AI models and return response in DRF</h1></a><div>https://www.reddit.com/r/django/.rss</div><div>2024-09-14 17:55:43+00:00</div><div><pre><!-- SC_OFF --><div><p>I want to use one of llama 27B simple model in my rest api backend for certain endpoints in an open source manner.<br/> On hugging face I found these large .gguf model files <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF">llama 27B .gguf </a> which people seem to use to load using llama cpp package/method in python and query the model.</p> <p>I am using DRF for implementing rest APIs and Postgres for database and trying to keep it simple and open source.</p> <p>My question is that the file size of these .gguf files is generally of the order of GB's so will it be a good practice to store these on my production server instances?<br/> and load them in memory to query against APIs</p> <p>Or should I take some other approach, I Don't want to deploy on cloud like AWS/GCP and pay for it every month..</p> <p>Sorry for posting an AI flavoured question here....</p> </div><!-- SC_ON -->   submitted by   &lt;a href="<a href="https://www.reddit.">https://www.reddit.</a></pre></div></body></html>