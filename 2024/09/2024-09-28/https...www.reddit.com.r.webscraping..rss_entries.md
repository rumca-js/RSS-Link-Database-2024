# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Spiderfoot And TruePeopleSearch Integration!
 - [https://www.reddit.com/r/webscraping/comments/1frillh/spiderfoot_and_truepeoplesearch_integration](https://www.reddit.com/r/webscraping/comments/1frillh/spiderfoot_and_truepeoplesearch_integration)
 - RSS feed: $source
 - date published: 2024-09-28T16:50:26+00:00

<table> <tr><td> <a href="https://www.reddit.com/r/webscraping/comments/1frillh/spiderfoot_and_truepeoplesearch_integration/"> <img src="https://b.thumbs.redditmedia.com/ZIjOXccYgSFIaIyXaIeXd7oqmSLc_libmR2Gqo4vm8c.jpg" alt="Spiderfoot And TruePeopleSearch Integration! " title="Spiderfoot And TruePeopleSearch Integration! " /> </a> </td><td> <!-- SC_OFF --><div class="md"><p>I was interested in using Endato&#39;s API (API Maker Behind TPS) to be an active module in Spiderfoot. My coding knowledge is not too advanced but I am proficient in the use of LLM&#39;s. I was able to write my own module with the help of Claude and GPT by just converting both Spiderfoot&#39;s and Endato&#39;s API documentation into PDFS and then giving it to them so they could understand how it could work together. It works but I would like to be able to format the response that the API sends back to Spiderfoot&#39;s end, a little better. Anyone with knowledge or ideas, please share! I&#39;ve attached what the cu

## Web scraping automation
 - [https://www.reddit.com/r/webscraping/comments/1frh0g8/web_scraping_automation](https://www.reddit.com/r/webscraping/comments/1frh0g8/web_scraping_automation)
 - RSS feed: $source
 - date published: 2024-09-28T15:38:47+00:00

<!-- SC_OFF --><div class="md"><p>So, I’ve recently been diving into web scraping, and the results have been pretty eye-opening. I’ve managed to automate scraping for emails, job titles, and other data directly from social media profiles, which has significantly boosted my outreach campaigns. Instead of manually hunting for details, the scraper handles it all and even validates emails before they land in my CRM.</p> <p>I know scraping can have its challenges with rate limits and terms of service, so I’m curious—what’s been your experience with scraping on social media platforms? Have you found any tricks to increase efficiency without getting blocked? Would love to hear what’s been working for you all!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Timofeir"> /u/Timofeir </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1frh0g8/web_scraping_automation/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscra

## How to automate accepting cookies content from different websites
 - [https://www.reddit.com/r/webscraping/comments/1frg3nz/how_to_automate_accepting_cookies_content_from](https://www.reddit.com/r/webscraping/comments/1frg3nz/how_to_automate_accepting_cookies_content_from)
 - RSS feed: $source
 - date published: 2024-09-28T14:57:13+00:00

<!-- SC_OFF --><div class="md"><p>Im trying to warm up my own browser core with randomly accessed websites, what i want to know is what’s the best way to accept cookies consent, since am accessing around 50 websites one by one i cant be rely on an specific selector or a popup to click the ‘Accept’ button.</p> <p>What would be the best way?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1frg3nz/how_to_automate_accepting_cookies_content_from/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1frg3nz/how_to_automate_accepting_cookies_content_from/">[comments]</a></span>

## Web scraping automation
 - [https://www.reddit.com/r/webscraping/comments/1frep8k/web_scraping_automation](https://www.reddit.com/r/webscraping/comments/1frep8k/web_scraping_automation)
 - RSS feed: $source
 - date published: 2024-09-28T13:48:41+00:00

<!-- SC_OFF --><div class="md"><p>So, I’ve recently been diving into web scraping, and the results have been pretty eye-opening. I’ve managed to automate scraping for emails, job titles, and other data directly from social media profiles, which has significantly boosted my outreach campaigns. Instead of manually hunting for details, the scraper handles it all and even validates emails before they land in my CRM.</p> <p>I know scraping can have its challenges with rate limits and terms of service, so I’m curious—what’s been your experience with scraping on social media platforms? Have you found any tricks to increase efficiency without getting blocked? Would love to hear what’s been working for you all!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Timofeir"> /u/Timofeir </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1frep8k/web_scraping_automation/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscra

## How to get started in bot detection and bot development?
 - [https://www.reddit.com/r/webscraping/comments/1fre62u/how_to_get_started_in_bot_detection_and_bot](https://www.reddit.com/r/webscraping/comments/1fre62u/how_to_get_started_in_bot_detection_and_bot)
 - RSS feed: $source
 - date published: 2024-09-28T13:21:14+00:00

&#32; submitted by &#32; <a href="https://www.reddit.com/user/antvas"> /u/antvas </a> <br/> <span><a href="https://deviceandbrowserinfo.com/learning_zone/articles/getting-started-bot-detection">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fre62u/how_to_get_started_in_bot_detection_and_bot/">[comments]</a></span>

