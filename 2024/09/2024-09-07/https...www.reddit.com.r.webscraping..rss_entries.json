[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T20:45:13+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I wanted to extract the ride passes data from an ebike app and got the api and all other request parameters by interception. As i&#39;m trying to mock the request via requests library python i was getting detected by cloudfare and error 403 so then i searched a lot and got to know about hrequests library , now i&#39;m using it and getting status code as 200 and some response too but the cloudfare is changing my accept-encoding headers midway so that i am not able to get the final data.</p> <p>In the response it is saying this :</p> <p><strong>// CF overwrites accept-encoding and infra can&#39;t fix.</strong></p> <p>This is what i&#39;m requesting</p> <pre><code>import hrequests import time import uuid session = str(int(time.time()*1000)) url = f&quot;https://web-production.lime.bike/lime_pass/subscriptions/new?_amplitudeSessionId={session}&quot; id = &lt;my_id&gt; token = &lt;my_token&gt; headers = { &#39;accept&#39;: &#39;text/html,application/xhtml+", "id": 1102954, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fbgr95/scraping_data_from_an_ebike_app", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Scraping data from an ebike app", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T20:23:31+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey, I\u2019m pretty new to web scraping, but has anyone had any success with scraping any of the player props from sports books recently? I\u2019ve tried FanDuel and DraftKings but haven\u2019t had much success so far. Thanks for any info given!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/kylej37\"> /u/kylej37 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fbga78/web_scraping_sportsbook/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fbga78/web_scraping_sportsbook/\">[comments]</a></span>", "id": 1102767, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fbga78/web_scraping_sportsbook", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Web scraping Sportsbook", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T16:11:07+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been looking into the different ways to scrape LinkedIn and what can be achieved. Pretty well-documented, except in one specific area: documents in posts.</p> <p>For context, LinkedIn removed its carousel feature but retained the ability to upload PDF documents in your posts. Colloquially, this is still referred to as carousels.</p> <p>I want to be able to have a list of LinkedIn post URLs in a spreadsheet, feed it in, and populate an Airtable base with all the relevant info.</p> <p>So, the columns would be:</p> <ul> <li>Full name</li> <li>Date/time posted</li> <li>Content (whatever text is in the post, retaining hyperlinks for tagged accounts if possible)&#39;</li> <li># of engagements (including likes, hearts, etc. and preferably would be able to grab the granular number of each type)</li> <li>List of accounts that engaged</li> <li># of comments</li> <li>List of accounts that commented</li> </ul> <p>And then these four columns, a given post", "id": 1101768, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fbaj1w/scraping_linkedin_posts_with_carouselsdocuments", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Scraping LinkedIn posts with carousels/documents", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T11:46:43+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m trying to scrape the popcorn meter (audience scores) for a list of 700 movies. My current script searches for the movie and tries to find the score from the search results but this isn\u2019t working. Is there a better way to go around this?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Tubby_6\"> /u/Tubby_6 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fb51t7/webscraping_rotten_tomato_popcorn_meter_scores/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fb51t7/webscraping_rotten_tomato_popcorn_meter_scores/\">[comments]</a></span>", "id": 1100752, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fb51t7/webscraping_rotten_tomato_popcorn_meter_scores", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Webscraping Rotten Tomato Popcorn Meter Scores?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T10:37:20+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>So i cant quite understand how does that work.Lets suppose i need 20 proxies for 20 different accounts for some kind of social network.If i buy lets say 20gb of residential proxies does that mean i only get 1 proxy with 20 gb of traffic limit?Or I get unlimited number of proxies with shared traffic limit of 20GB?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ivanci55\"> /u/ivanci55 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fb3z5y/how_does_gb_proxies_work/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fb3z5y/how_does_gb_proxies_work/\">[comments]</a></span>", "id": 1100753, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fb3z5y/how_does_gb_proxies_work", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "How does $/GB proxies work?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T09:18:59+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi, I made a scraper to practice my concepts and keep improving with &quot;real-scenarios&quot;. I pick random websites and make a scraper for it. </p> <p>URL=<a href=\"https://www.willamettewines.com/members-only/membership/directories-contacts/member-directory/?view=list&amp;sort=qualityScore\">https://www.willamettewines.com/members-only/membership/directories-contacts/member-directory/?view=list&amp;sort=qualityScore</a></p> <p>Is this scraper considered up to current standards ? </p> <p>Is there a way to better optimize it ? </p> <p>Any tip, comment would be appreciated. Thank you </p> <pre><code>import sys import requests from bs4 import BeautifulSoup import json sys.stdout.reconfigure(encoding=&#39;utf-8&#39;) current_skip=0 for items in range (1,11): url = f&quot;https://www.willamettewines.com/includes/rest_v2/plugins_listings_listings/find/?json=%7B%22filter%22%3A%7B%22%24and%22%3A%5B%7B%22filter_tags%22%3A%7B%22%24in%22%3A%5B%22site_primary_s", "id": 1100561, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fb2xdl/noob_question_asking_for_guidance", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Noob question, asking for guidance.", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T03:54:35+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, I&#39;m interested to learn how OpenAI, Perplexity, Bing, etc., when generating GPT answers, scrape the data from websites without getting blocked? How do they prevent being identified as bots since a lot of websites do not allow bot scraping.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Responsible-Prize848\"> /u/Responsible-Prize848 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fay7ru/openai_perplexity_bing_scraping_not_getting/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fay7ru/openai_perplexity_bing_scraping_not_getting/\">[comments]</a></span>", "id": 1099512, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fay7ru/openai_perplexity_bing_scraping_not_getting", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "OpenAI, Perplexity, Bing scraping not getting blocked while generating answer", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T02:09:40+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m using Python Selenium to collect usernames from accounts related to my instagram niche, then I eventually engage with those usernames (like, comment, and follow). I&#39;m still in the process of testing, so I&#39;m sending too much request and got flagged for scraping. I have randomized sleep times for every action, and I&#39;m making it gradual and as slow as possible.</p> <p>What are other best practices to avoid getting flagged? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jgsd_\"> /u/jgsd_ </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fawbzx/instagram_webscraperbot/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fawbzx/instagram_webscraperbot/\">[comments]</a></span>", "id": 1099279, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fawbzx/instagram_webscraperbot", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Instagram Webscraper/Bot", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T01:45:44+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I was scraping google for the last month (just the first page) but recently was rate limited. </p> <p>Are there any other search engines that are scrape-able and will not rate limit?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cupojoe4me\"> /u/cupojoe4me </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1favvtq/most_scrapable_search_engine/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1favvtq/most_scrapable_search_engine/\">[comments]</a></span>", "id": 1099280, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1favvtq/most_scrapable_search_engine", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Most scrap-able search engine?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-07T00:25:42+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m a newbie, I&#39;m trying to scrape a sportsbook page for props and lines, i started my research in internet with bet365 just to find out that people don&#39;t usually get it, idk, read that bet365 blocks your Ip or something like that, i know that there are some services that help with this like scraping bee, but as i said at the beginning, I&#39;m just a newbie, so I&#39;d like some advice on how to handle this</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jamesposting\"> /u/jamesposting </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1faucvn/some_advice_scraping_draftkings/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1faucvn/some_advice_scraping_draftkings/\">[comments]</a></span>", "id": 1098940, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1faucvn/some_advice_scraping_draftkings", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Some advice scraping draftkings?", "user": null, "vote": 0}]