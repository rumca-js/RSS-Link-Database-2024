<!DOCTYPE html><html><head>   <meta>     <title>Show HN: Wordllama – Things you can do with the token embeddings of an LLM</title>   </meta>   <style>
.youtube_player_container {
    position: relative;
    width: 50%;
    padding-bottom: 26.25%;
    /* background-color: yellow; */
}
.youtube_player_frame {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border: 0;
}
       </style></head><body><a href="index.html"><h2>Index</h2></a><a href="https://github.com/dleemiller/WordLlama"><h1>[1147208] Show HN: Wordllama – Things you can do with the token embeddings of an LLM</h1></a><div>https://hnrss.org/frontpage</div><div>2024-09-15 03:25:14+00:00</div><div><pre>
<p>After working with LLMs for long enough, I found myself wanting a lightweight utility for doing various small tasks to prepare inputs, locate information and create evaluators. This library is two things: a very simple model and utilities that inference it (eg. fuzzy deduplication). The target platform is CPU, and it’s intended to be light, fast and pip installable — a library that lowers the barrier to working with strings <i>semantically</i>. You don’t need to install pytorch to use it, or any deep learning runtimes.<p>How can this be accomplished? The model is simply token embeddings that are average pooled. To create this model, I extracted token embedding (nn.Embedding) vectors from LLMs, concatenated them along the embedding dimension, added a learnable weight parameter, and projected them to a smaller dimension. Using the sentence transformers framework and datasets, I trained the pooled embedding with multiple negatives ranking loss and matryoshka representation learning s</p></p></pre></div></body></html>