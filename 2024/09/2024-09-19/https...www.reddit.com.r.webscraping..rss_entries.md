# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Step-by-Step Guide: Building Your Own Web Scraping Bot Without Coding
 - [https://www.reddit.com/r/webscraping/comments/1fkta38/stepbystep_guide_building_your_own_web_scraping](https://www.reddit.com/r/webscraping/comments/1fkta38/stepbystep_guide_building_your_own_web_scraping)
 - RSS feed: $source
 - date published: 2024-09-19T19:43:47+00:00

<!-- SC_OFF --><div class="md"><p>Hi everyone!</p> <p>I wanted to share a detailed guide on how you can build your own web scraping bot without needing to code. This can be super useful for anyone looking to automate data collection from websites, whether for personal use or for business purposes.</p> <p>In the guide, I go over:</p> <ul> <li>Selecting the right no-code tool for your project.</li> <li>Setting up the scraper step-by-step.</li> <li>Practical uses like price tracking, gathering SEO data, and more.</li> </ul> <p>If you&#39;re interested in learning how you can automate tasks without coding, feel free to check out the guide. It’s meant to be beginner-friendly, so anyone can follow along!</p> <p>read full article here: <a href="https://all-tools.github.io/blog/build-web-scraping-bot-without-coding.html">https://all-tools.github.io/blog/build-web-scraping-bot-without-coding.html</a></p> <p>Would love to hear your thoughts or if you’ve tried any no-code scraping tools before!<

## Help with Playwright Scraper
 - [https://www.reddit.com/r/webscraping/comments/1fksy83/help_with_playwright_scraper](https://www.reddit.com/r/webscraping/comments/1fksy83/help_with_playwright_scraper)
 - RSS feed: $source
 - date published: 2024-09-19T19:14:41+00:00

<!-- SC_OFF --><div class="md"><p>Hey,</p> <p>I’ve been building a web scraper using Playwright, but I’m running into several technical issues when trying to scrape nested comments and load more data through scrolling. I could really use some help from anyone with experience in Playwright.</p> <h1>Problem:</h1> <ol> <li><strong>Nested Comments and Replies</strong>: <ul> <li>I can successfully navigate to the post, but Playwright fails to extract all nested comments and replies. It struggles to load replies under comments, and only the top-level comments are visible.</li> </ul></li> <li><strong>Scrolling Issues</strong>: <ul> <li>My scrolling function doesn’t seem to work well. It either stops after a few attempts or fails to trigger the loading of more comments.</li> </ul></li> </ol> <h1>What I&#39;ve Tried:</h1> <ul> <li>Using <code>scrollIntoView()</code> to try scrolling to the bottom of the page.</li> <li>Different delays and wait conditions to allow content to load.</li> <li>Expe

## How many of you pick up jobs specifically around scraping sites?
 - [https://www.reddit.com/r/webscraping/comments/1fks0id/how_many_of_you_pick_up_jobs_specifically_around](https://www.reddit.com/r/webscraping/comments/1fks0id/how_many_of_you_pick_up_jobs_specifically_around)
 - RSS feed: $source
 - date published: 2024-09-19T18:30:54+00:00

<!-- SC_OFF --><div class="md"><p>Do many here find jobs/gigs where people are looking to scrape websites? Thinking about offering services as a web scraper but not sure how much demand there is out there.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/DuffyBravo"> /u/DuffyBravo </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fks0id/how_many_of_you_pick_up_jobs_specifically_around/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fks0id/how_many_of_you_pick_up_jobs_specifically_around/">[comments]</a></span>

## The Best Scrapers on GitHub
 - [https://www.reddit.com/r/webscraping/comments/1fknxa7/the_best_scrapers_on_github](https://www.reddit.com/r/webscraping/comments/1fknxa7/the_best_scrapers_on_github)
 - RSS feed: $source
 - date published: 2024-09-19T15:41:14+00:00

<!-- SC_OFF --><div class="md"><p>Hey, </p> <p>Starting my web scraping journey. Watching all the videos, reading all the things... </p> <p>Do y&#39;all follow any pros on GitHub who have sophisticated scraping logic/really good code I could learn from? Tutorials are great but looking for a resource with more complex real-world examples to emulate.</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/windowwiper96"> /u/windowwiper96 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fknxa7/the_best_scrapers_on_github/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fknxa7/the_best_scrapers_on_github/">[comments]</a></span>

## So close to getting my code to work - GoodReads Scraper
 - [https://www.reddit.com/r/webscraping/comments/1fknk8b/so_close_to_getting_my_code_to_work_goodreads](https://www.reddit.com/r/webscraping/comments/1fknk8b/so_close_to_getting_my_code_to_work_goodreads)
 - RSS feed: $source
 - date published: 2024-09-19T15:25:41+00:00

<!-- SC_OFF --><div class="md"><p>Hello, I apologize for being so annoying over the last few days. My code is so close to being done. I can almost taste the finish line. Ive created a scraper for goodreads that uses a keyword to scrape authors names, titles, average reviews, and total number of reviews. I also want it to scrape the top three reviews and I have code that should do it but when I run it, the top reviews section is blank. Just shows me [ ]. Please, I need help. </p> <pre><code>from urllib.parse import urljoin import requests from bs4 import BeautifulSoup import json import argparse from datetime import datetime # Function to grab a page and return the parsed BeautifulSoup object def fetch_page(url): headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#39; } response = requests.get(url, headers=headers, proxies=proxy, timeout=10) if response.status_code == 200: return Bea

## Anyone experienced with Outscraper and marrying it to Apollo data?
 - [https://www.reddit.com/r/webscraping/comments/1fkkxlx/anyone_experienced_with_outscraper_and_marrying](https://www.reddit.com/r/webscraping/comments/1fkkxlx/anyone_experienced_with_outscraper_and_marrying)
 - RSS feed: $source
 - date published: 2024-09-19T13:32:08+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;m looking to pull a specific data set about Companies using outscraper (also open to other tools if theres an alternative recommendation), then marrying that dataset to a contact/persona list using Apollo, with the end goal having a very clean list to prospect/email with. My challenge is marrying the two different data sets. </p> <p>Is anyone experienced in webscraping and list building? If so, would love to hear from you! Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/d4crx90"> /u/d4crx90 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fkkxlx/anyone_experienced_with_outscraper_and_marrying/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fkkxlx/anyone_experienced_with_outscraper_and_marrying/">[comments]</a></span>

## Finding Yandex cached pages
 - [https://www.reddit.com/r/webscraping/comments/1fkewkh/finding_yandex_cached_pages](https://www.reddit.com/r/webscraping/comments/1fkewkh/finding_yandex_cached_pages)
 - RSS feed: $source
 - date published: 2024-09-19T07:05:09+00:00

<!-- SC_OFF --><div class="md"><p>Yandex cache can be years out of date, this is actually very useful for archival purposes. I need to find the urls for cached pages, let&#39;s say on the scale of 1 million. I&#39;ll leave retrieving the pages from cache out of the scope of the question.</p> <p>The primary issue is finding urls that are cached. The cache itself has no search function, it seems the only way is finding the page mentioned through Yandex search, or brute-forcing/stuffing (though they give false cache 404s sometimes). A cursory look through the search engine with a <code>site:</code> query shows that each page returns 10 results, and you can go 25 pages deep. This is not very practical, Because the search query does not allow many parameters and generating enough queries to give broad and different results seems difficult.</p> <p>It seems their official api access is currently closed. I tried free trials for 3 sites claiming to be able to scrape yandex for me, and only 1 a

## How to scrape sec filings of jp Morgan to make financial models
 - [https://www.reddit.com/r/webscraping/comments/1fkdlop/how_to_scrape_sec_filings_of_jp_morgan_to_make](https://www.reddit.com/r/webscraping/comments/1fkdlop/how_to_scrape_sec_filings_of_jp_morgan_to_make)
 - RSS feed: $source
 - date published: 2024-09-19T05:31:27+00:00

<!-- SC_OFF --><div class="md"><p>I’ve never used python before but I watched YouTube videos and took the help of chat gpt also to scrape, on YouTube they install stuff really quick and switch the tabs and I don’t even know what app they are on and it’s been very confusing and chat gpt is giving me unclear instructions and lots of errors.</p> <p>Is this supposed to be easy ? I installed pip package like beautifulsoup4 , pandas , edgar tools and Jupyter notebook but idk what to next , one guy on yt opened edgar tools on git or something and on cgpt it’s giving me commands to run on Jupyter notebook that has the 403 and 200 error .</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Swimming_Cloud_4761"> /u/Swimming_Cloud_4761 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1fkdlop/how_to_scrape_sec_filings_of_jp_morgan_to_make/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1fkdlop/how_to_

