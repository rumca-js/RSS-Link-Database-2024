[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T23:29:15+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>I am currently trying to generate large lead lists for clients by scraping <a href=\"http://maps.google.com\">maps.google.com</a> . The information I want is the business name, their number (if available), and their address. However, I am curious if anyone knows of a query to gather all the businesses in a region? Also if anyone knows of a way to bypass the fact that google maps only surfaces a max of 120 results per query?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/youngkilog\"> /u/youngkilog </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffgkqs/need_to_gather_all_brick_and_mortar_businesses_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffgkqs/need_to_gather_all_brick_and_mortar_businesses_in/\">[comments]</a></span>", "id": 1136070, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ffgkqs/need_to_gather_all_brick_and_mortar_businesses_in", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Need to gather all Brick and Mortar businesses in a region from google maps", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T23:22:07+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone! I&#39;ve been scraping data from the internet for a while now, but I&#39;ve never come across this issue. I am trying to scrape data from &quot;Chalkboard&quot;, which is a fantasy sports betting app only available on iPhone and android. To do this, I set up fiddler as a proxy on my laptop and have been routing all traffic through the proxy to monitor any http/https traffic and look for Chalkboard&#39;s api endpoints. However, I don&#39;t think any of the data being sent to the app from their servers uses HTTPS! None of the responses contain relevant json data for the betting data. The only responses that contain some information are when I select a few players to make a bet--Chalkboard will send a request to their servers to determine if the selection is valid, and their servers will respond with json data that answers the app&#39;s request. Also, images for the players are sent through the app (and maybe the data could be encoded in ", "id": 1136071, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ffgfgj/webscraping_of_an_iphone_app", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Webscraping of an iPhone app", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T21:36:44+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey guys,</p> <p>I am currently tasked with gathering all the brick and mortar businesses in a particular region (Could be California or New York state). What is the best way to do this?</p> <p>I was thinking about scraping google maps, but I&#39;m not sure there&#39;s an easy query to gather all the businesses on google maps. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/youngkilog\"> /u/youngkilog </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffe5oe/need_to_gather_all_brick_and_mortar_businesses_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffe5oe/need_to_gather_all_brick_and_mortar_businesses_in/\">[comments]</a></span>", "id": 1135430, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ffe5oe/need_to_gather_all_brick_and_mortar_businesses_in", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Need to gather all Brick and Mortar businesses in a region", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T20:57:12+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Any way to scrape directly from a normal google chrome instance? I tried playwright for python but I think the page managed to detect that, so if I can listen to the actual google chrome app, that would be the best solution.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Competitive-Fun-5969\"> /u/Competitive-Fun-5969 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffd8fm/how_to_scrape_while_browsing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffd8fm/how_to_scrape_while_browsing/\">[comments]</a></span>", "id": 1135431, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ffd8fm/how_to_scrape_while_browsing", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "How to scrape while browsing", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T18:59:55+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;d love to create a website similar to <a href=\"https://steamdb.info/\">https://steamdb.info/</a> where the output of my scraping can exist and be periodically refreshed. Does anyone know where I can start? Maybe a template? I&#39;m not against hiring a developer for something like this too.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/NoDeparture7996\"> /u/NoDeparture7996 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffafl8/creating_a_website_with_scrape_api/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ffafl8/creating_a_website_with_scrape_api/\">[comments]</a></span>", "id": 1134711, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ffafl8/creating_a_website_with_scrape_api", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Creating a website with scrape API?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T15:19:12+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a python script that scrapes data for 100 players in a day from a tennis website if I run it on 5 tabs. There are 3500 players in total..how can I make this process faster without using multiple PCs.</p> <p>( Multithreading, asynchronous requests are not speeding up the process )</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ChemistryOrdinary860\"> /u/ChemistryOrdinary860 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ff5445/speed_up_scraping_tennis_website/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1ff5445/speed_up_scraping_tennis_website/\">[comments]</a></span>", "id": 1132908, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ff5445/speed_up_scraping_tennis_website", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Speed up scraping ( tennis website )", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T14:10:20+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>hey yall, Im new to web scraping and I need a bit of help. Ive decided to scrape goodreads, an authors page and the top 4 reviews of each book, just to try something out. But ive come at a crossroads with my code. Ive written everything (think im missing a few things) but im not sure how to like... implement it. get it to start. get it to do said scraping. Ive attached the code in this post, any help would be highly appreciated:</p> <pre><code>import requests import json from bs4 import BeautifulSoup import selenium import pandas as pd import time from datetime import datetime def get_timestamp(): return datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;) url = &#39;https://www.goodreads.com/search?utf8=%E2%9C%93&amp;query=tom+king&#39; page = requests.get(url) soup = BeautifulSoup(page.content, &#39;html.parser&#39;) with open(&#39;Authorpage.html&#39;, &#39;w&#39;) as f: f.write(&#39;titles, authors, avg_rating, numb_rating&#39;) page = requests.ge", "id": 1132909, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ff3i5m/newbie_asking_for_help", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Newbie asking for help", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T13:59:41+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone, </p> <p>I am working on a <strong>webscraping framework</strong>(named <strong>Goscrapy</strong>) of my own in my free time. </p> <p>Goscrapy is a Scrapy-inspired web scraping framework in Golang. The primary objective is to <strong>reduce the learning curve for developers looking to migrate from Python (Scrapy) to Golan</strong>g for their web scraping projects, while taking advantage of Golang&#39;s built-in concurrency and generally low resource requirements. </p> <p>Additionally, Goscrapy aims to <strong>provide an interface similar to the popular Scrapy framework in Python</strong>, making Scrapy developers feel at home. </p> <p>It&#39;s still in it&#39;s early stage and is not stable. I am aware that there are a lot of things to be done and is far from complete. Just trying to create a POC atm.</p> <p><strong>Repo</strong>: <a href=\"https://github.com/tech-engine/goscrapy\">https://github.com/tech-engine/goscrapy</a></p> </div><!-- S", "id": 1132258, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1ff394r/goscrapy_harnessing_gos_power_for", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "GoScrapy: Harnessing Go's power for blazzzzzzzzingly fast web scraping, inspired by Python's Scrapy framework", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T10:52:31+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I would like to get address, number and web site data from my saved places in google maps the saved as a .csv file. How can I do that without Google API? (Ex places list: <a href=\"https://maps.app.goo.gl/bsxbhgW9zvXzSa8n9\">https://maps.app.goo.gl/bsxbhgW9zvXzSa8n9</a>)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lilgrsl\"> /u/lilgrsl </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fezmmv/extract_data_from_google_maps_saved_list/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fezmmv/extract_data_from_google_maps_saved_list/\">[comments]</a></span>", "id": 1131327, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fezmmv/extract_data_from_google_maps_saved_list", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Extract data from google maps saved list", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T09:14:25+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I was trying to scrape this website by copying the request using the developer tools in the network tab. I copied it as cURL and posted it to Postman. In Postman I always got the 403 forbidden error. But for some reason removing the User-Agent from the request fixed it. Why?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rp407\"> /u/rp407 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fey75w/why_does_removing_useragent_make_the_request_work/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fey75w/why_does_removing_useragent_make_the_request_work/\">[comments]</a></span>", "id": 1130746, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fey75w/why_does_removing_useragent_make_the_request_work", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Why does removing User-Agent make the request work?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T08:27:55+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all!</p> <p>I am trying to scrape some pages using undetected chromedriver and proxy use. I&#39;ve seen through some analytics that I made 14 requests for my target site. But for these requests I had the following numbers :</p> <p>site_to_scrape 14 requests, usage 1 MB</p> <p><a href=\"http://clients2.googleusercontent.com\">clients2.googleusercontent.com</a> 7 requests, 11 MB (!!)</p> <p><a href=\"http://optimizationguide-pa.googleapis.com\">optimizationguide-pa.googleapis.com</a> 16 requests, 4 MB</p> <p>so for 1 needed MB of info, I also got 15 Mb of useless data.<br/> why the browser even gets those? I tried version_main and driver scopes just in case but nothing. Is there something I can do by my side or these links are possibly triggered by the targeted site per se? Novice scraper here, sorry for any bad English.</p> <p>relevant code</p> <pre><code>options = uc.ChromeOptions() proxy_options = { &#39;proxy&#39;: { &#39;http&#39;: &#39;something&#3", "id": 1130747, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fexkpu/undetected_chromedriver_and", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "undetected chromedriver and clients2.googleusercontent.com", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-12T03:55:20+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello, is there a way to extract google reviews of a franchise with multiple locations?</p> <p>For instance, can I extract the reviews of every McDonald\u2019s store in my city, with the rating, text, store location coordinates etc?</p> <p>Every solution I\u2019ve seen requires a paid platform- thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Old_Ad_4538\"> /u/Old_Ad_4538 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fetkg6/how_to_extract_google_reviews_of_a_franchise/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fetkg6/how_to_extract_google_reviews_of_a_franchise/\">[comments]</a></span>", "id": 1129572, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fetkg6/how_to_extract_google_reviews_of_a_franchise", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "How to extract google reviews of a franchise?", "user": null, "vote": 0}]