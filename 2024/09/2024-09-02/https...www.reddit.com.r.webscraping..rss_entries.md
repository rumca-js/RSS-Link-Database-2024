# Source:webscraping, URL:https://www.reddit.com/r/webscraping/.rss, language:en

## Can you scrape a website if the Terms of Service don‚Äôt allow automated requests?
 - [https://www.reddit.com/r/webscraping/comments/1f7f3gl/can_you_scrape_a_website_if_the_terms_of_service](https://www.reddit.com/r/webscraping/comments/1f7f3gl/can_you_scrape_a_website_if_the_terms_of_service)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T19:09:47+00:00

<!-- SC_OFF --><div class="md"><p>As title says, I want to scrape a few websites where they‚Äôre pretty protective of the information being scraped. Is there any way around this, or is just going ahead with scraping something I could do? </p> <p>Would a VPN help avoid a ban?</p> <p>Thanks</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/3leavclova"> /u/3leavclova </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f7f3gl/can_you_scrape_a_website_if_the_terms_of_service/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f7f3gl/can_you_scrape_a_website_if_the_terms_of_service/">[comments]</a></span>

## How do you find Playwright.js jobs and not poetry jobs
 - [https://www.reddit.com/r/webscraping/comments/1f7dxq3/how_do_you_find_playwrightjs_jobs_and_not_poetry](https://www.reddit.com/r/webscraping/comments/1f7dxq3/how_do_you_find_playwrightjs_jobs_and_not_poetry)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T18:23:41+00:00

<!-- SC_OFF --><div class="md"><p>I swear everytime I search for Playwright it shows me jobs for <strong>an actual playwright..</strong> Anyone have a semantic way searching around this? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Impressive_Safety_26"> /u/Impressive_Safety_26 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f7dxq3/how_do_you_find_playwrightjs_jobs_and_not_poetry/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f7dxq3/how_do_you_find_playwrightjs_jobs_and_not_poetry/">[comments]</a></span>

## I dont what‚Äôs gonna come up with the view, how do i know and automate things
 - [https://www.reddit.com/r/webscraping/comments/1f7bmkm/i_dont_whats_gonna_come_up_with_the_view_how_do_i](https://www.reddit.com/r/webscraping/comments/1f7bmkm/i_dont_whats_gonna_come_up_with_the_view_how_do_i)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T16:52:28+00:00

<!-- SC_OFF --><div class="md"><p>Hey, i have plan to automate some website using puppeteer, the issue is i dont know what type of inputs coming next ( Input fields, drop downs, checkboxes, radio buttons), there might be only checkboxes or only input fields and dropdowns, even i dont how many would come. I can make sure one thing, that there will only those type of inputs.</p> <p>So how do i make puppeteer know what is there and fill it accordingly.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/LocalConversation850"> /u/LocalConversation850 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f7bmkm/i_dont_whats_gonna_come_up_with_the_view_how_do_i/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f7bmkm/i_dont_whats_gonna_come_up_with_the_view_how_do_i/">[comments]</a></span>

## Most effective method for scraping profiles from LinkedIn Sales Navigator
 - [https://www.reddit.com/r/webscraping/comments/1f79wyt/most_effective_method_for_scraping_profiles_from](https://www.reddit.com/r/webscraping/comments/1f79wyt/most_effective_method_for_scraping_profiles_from)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T15:43:03+00:00

<!-- SC_OFF --><div class="md"><p>I&#39;ve used Dux-Soup in the past to run automated lead generation on LinkedIn (it is a chrome plug-in that automatically visits profiles, scrapes the profile info, and optionally connects to the profile and/or send them a message).</p> <p>I&#39;d like to build my own custom tool for this. However as you probably know, scraping LinkedIn is quite difficult due to CSP restrictions. I&#39;m trying to understand what method tools like Dux-Soup use as it is very efficient at it. </p> <p>From my understanding it operates as a headless browser built with something like Puppeteer or Selenium? Is this the most effective method for bypassing LinkedIn&#39;s CSP restrictions?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/OptimalBarnacle7633"> /u/OptimalBarnacle7633 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f79wyt/most_effective_method_for_scraping_profiles_from/">[link]</a></span> &#32; <spa

## Scraping Instagram
 - [https://www.reddit.com/r/webscraping/comments/1f791t5/scraping_instagram](https://www.reddit.com/r/webscraping/comments/1f791t5/scraping_instagram)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T15:07:18+00:00

<!-- SC_OFF --><div class="md"><p>Can I get a list of Instagram account in a certain Industry (lets say dentists in US). If I want to scrape it manually how do I do that?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/False_Ad4246"> /u/False_Ad4246 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f791t5/scraping_instagram/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f791t5/scraping_instagram/">[comments]</a></span>

## Weekly Discussion - 02 Sep 2024
 - [https://www.reddit.com/r/webscraping/comments/1f78w8c/weekly_discussion_02_sep_2024](https://www.reddit.com/r/webscraping/comments/1f78w8c/weekly_discussion_02_sep_2024)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T15:01:07+00:00

<!-- SC_OFF --><div class="md"><p>Welcome to the weekly discussion thread! Whether you&#39;re a seasoned web scraper or just starting out, this is the perfect place to discuss topics that might not warrant a dedicated post, such as:</p> <ul> <li>Techniques for extracting data from popular sites like LinkedIn, Facebook, etc.</li> <li>Industry news, trends, and insights on the web scraping job market</li> <li>Challenges and strategies in marketing and monetizing your scraping projects</li> </ul> <p>Like our monthly <a href="https://reddit.com/r/webscraping/about/sticky?num=1">self-promotion</a> thread, mentions of paid services and tools are permitted ü§ù. If you&#39;re new to web scraping, be sure to check out the <a href="https://webscraping.fyi">beginners guide</a> üå±</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/AutoModerator"> /u/AutoModerator </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f78w8c/weekly_discussion_02_s

## Recreating get request results in 400 Error
 - [https://www.reddit.com/r/webscraping/comments/1f75i9a/recreating_get_request_results_in_400_error](https://www.reddit.com/r/webscraping/comments/1f75i9a/recreating_get_request_results_in_400_error)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T12:28:43+00:00

<!-- SC_OFF --><div class="md"><p>I am trying to get the data on <a href="https://digitalsky.dgca.gov.in/remote_pilots">https://digitalsky.dgca.gov.in/remote_pilots</a>, in the network tab I found the get request to: <a href="https://digitalsky.dgca.gov.in/digital-sky/public/pilots/certified?pageNo=226&amp;size=50">https://digitalsky.dgca.gov.in/digital-sky/public/pilots/certified?pageNo=226&amp;size=50</a> But when I try accessing the link through python (response = requests.get(url)) it gives a 400 error with the following error:</p> <p>‚Äò‚Äô‚Äô{&#39;message&#39;: &#39;Bad Request&#39;, &#39;_links&#39;: {&#39;self&#39;: {&#39;href&#39;: &#39;/digital-sky/public/pilots/certified?pageNo=1&amp;size=50&#39;, &#39;templated&#39;: False}}, &#39;_embedded&#39;: {&#39;errors&#39;: [{&#39;message&#39;: &#39;Required Header [source] not specified&#39;, &#39;path&#39;: &#39;/source&#39;}]}}‚Äô‚Äô‚Äô</p> <p>I tried the same process for another dataset on the same website (<a href="https://digitalsky.dgca

## Understanding how Freebie Alerts App scrapes marketplace listings
 - [https://www.reddit.com/r/webscraping/comments/1f73uj3/understanding_how_freebie_alerts_app_scrapes](https://www.reddit.com/r/webscraping/comments/1f73uj3/understanding_how_freebie_alerts_app_scrapes)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T10:54:30+00:00

<!-- SC_OFF --><div class="md"><p>I am fascinated by how the Freebie Alerts App scrapes listing, specially from FB marketplace, and I was wondering if anyone has any idea on what languages or approaches it may be using to scrape FB marketplace listings. I would love to try to run something similar with custom searches with different settings. I started using python and playwright, but not sure if this may be a good starting approach.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Last_Choice6947"> /u/Last_Choice6947 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f73uj3/understanding_how_freebie_alerts_app_scrapes/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f73uj3/understanding_how_freebie_alerts_app_scrapes/">[comments]</a></span>

## Getting correct selector using Puppeteer
 - [https://www.reddit.com/r/webscraping/comments/1f71hcz/getting_correct_selector_using_puppeteer](https://www.reddit.com/r/webscraping/comments/1f71hcz/getting_correct_selector_using_puppeteer)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T08:12:36+00:00

<!-- SC_OFF --><div class="md"><p>[ Removed by Reddit on account of violating the <a href="/help/contentpolicy">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Big_Sky5878"> /u/Big_Sky5878 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f71hcz/getting_correct_selector_using_puppeteer/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f71hcz/getting_correct_selector_using_puppeteer/">[comments]</a></span>

## Getting correct selector using Puppeteer
 - [https://www.reddit.com/r/webscraping/comments/1f70na5/getting_correct_selector_using_puppeteer](https://www.reddit.com/r/webscraping/comments/1f70na5/getting_correct_selector_using_puppeteer)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T07:13:34+00:00

<!-- SC_OFF --><div class="md"><p>[ Removed by Reddit on account of violating the <a href="/help/contentpolicy">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Big_Sky5878"> /u/Big_Sky5878 </a> <br/> <span><a href="https://www.reddit.com/r/webscraping/comments/1f70na5/getting_correct_selector_using_puppeteer/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/webscraping/comments/1f70na5/getting_correct_selector_using_puppeteer/">[comments]</a></span>

## Am I onto something
 - [https://www.reddit.com/r/webscraping/comments/1f6xf3n/am_i_onto_something](https://www.reddit.com/r/webscraping/comments/1f6xf3n/am_i_onto_something)
 - RSS feed: https://www.reddit.com/r/webscraping/.rss
 - date published: 2024-09-02T03:53:17+00:00

<!-- SC_OFF --><div class="md"><p>I used to joke that no amount of web scraping protections can defend against an external camera pointed at the screen and a bunch of tiny servos typing keys and moving the mouse. I think I&#39;ve found the program equivalent.</p> <p>Recently, I&#39;ve web scraped a bunch of stuff using the pynput library; I literally just manually do what I want to do, then use pynput and pyautogui to record, and then replicate all of my keyboard inputs and mouse movements however many times I want. To scrape the data, I just set it to take automatic screenshots of certain pixels at certain points in time, and maybe use an ML library to extract the text. Obviously, this method isn&#39;t good for scraping large amounts of data, but here are the things I have been able to do:</p> <ul> <li>scrape pages where you&#39;re more interested in live updates e.g. stock prices or trades</li> <li>scrape google images</li> <li>replace the youtube API by recording and performing the

