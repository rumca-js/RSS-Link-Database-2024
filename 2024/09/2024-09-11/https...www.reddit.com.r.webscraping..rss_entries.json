[{"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T18:04:58+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Also related to web automation</p> <p>We want to sign up for tens of thousands of arbitrary e-commerce brands&#39; newsletters through their website once (not unethical/harmful).</p> <p>Earlier we passed a website&#39;s forms (with an email field) to GPT and had it determine which one was a newsletter, and get it to create a Puppeteer script to sign up for it based on the input / button xpaths. It was a little expensive.</p> <p>Currently we parse and filter forms including emails, and based on keywords like &quot;X% off&quot;, &quot;newsletter&quot;, &quot;sign up&quot; etc. We appear to get a 70% success rate (7/10 websites are successfully signed up for).</p> <p>We currently need some help with confirming whether a form was successfully signed up for. I was thinking about listening for network requests with a payload including the email we signed up with but this could falsely include forms that weren&#39;t a newsletter.</p> <p>Any advice on confirm", "id": 1127200, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fegt4u/signing_up_for_arbitrary_websites_newsletters", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Signing up for arbitrary websites' newsletters forms", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T16:47:52+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello - looking for ideas on how to start this task... how do I get a list of every ASIN from Amazon for a specific vendor. IE: I want every ASIN for every Apple product posted on Amazon. Thank you</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/user20202\"> /u/user20202 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1feewpc/how_to_get_every_asin_on_amazon_from_a_specific/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1feewpc/how_to_get_every_asin_on_amazon_from_a_specific/\">[comments]</a></span>", "id": 1126816, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1feewpc/how_to_get_every_asin_on_amazon_from_a_specific", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "How to get every ASIN on Amazon from a specific vendor?", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T16:15:37+00:00", "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1fee41l/scrape_job_postings_containing_hiring_managers/\"> <img src=\"https://a.thumbs.redditmedia.com/e90-E5LnJDIJc1avXJosOeLlWbs0LG3SnXQIAjbgRS8.jpg\" alt=\"Scrape Job postings containing hiring managers\" title=\"Scrape Job postings containing hiring managers\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>As the title suggests, I\u2019m frustrated with unqualified and international applicants spamming job postings. After being laid off, I experienced firsthand how difficult it is to stand out\u2014one company I interviewed with had over 4,000 applicants!</p> <p>To get around this, I\u2019ve started finding job postings that list the hiring manager and sending them direct messages instead. I\u2019m wondering if anyone else would be interested in something like this?</p> <p>Also, any ideas on how to improve or build this process out would be great! I\u2019m currently using JavaScript to scrape the job listings and output a clean CSV. Wou", "id": 1126247, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fee41l/scrape_job_postings_containing_hiring_managers", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 86, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": "https://a.thumbs.redditmedia.com/e90-E5LnJDIJc1avXJosOeLlWbs0LG3SnXQIAjbgRS8.jpg", "title": "Scrape Job postings containing hiring managers", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T13:28:04+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I just released my new open-source project<a href=\"https://github.com/jpjacobpadilla/Stealth-Requests\"> Stealth-Requests</a>! Stealth-Requests is an all-in-one solution for web scraping that seamlessly mimics a browser&#39;s behavior to help you stay undetected when sending HTTP requests. </p> <p>Here are some of the main features:</p> <ul> <li>Mimics Chrome or Safari headers when scraping websites to stay undetected</li> <li>Keeps tracks of dynamic headers such as Referer and Host</li> <li>Masks the TLS fingerprint of requests to look like a browser</li> <li>Automatically extract metadata from HTML responses including page title, description, author, and more</li> <li>Lets you easily convert HTML-based responses into <a href=\"https://lxml.de/apidoc/lxml.html\">lxml</a> and <a href=\"https://beautiful-soup-4.readthedocs.io/en/latest/\">BeautifulSoup</a> objects</li> </ul> <p>Hopefully some of you find this project helpful. Consider checking", "id": 1125308, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Stay Undetected While Scraping the Web | Open Source Project", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T13:24:52+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all - I&#39;m looking for some help for the best method to scrape data from the website TypeRacer - which is a website that allows users to test their typing skills against random users around the globe. There&#39;s also a practice mode where you&#39;re only racing against yourself. They have thousands of passages, and you type one passage per race. I&#39;m hoping to scrape at least a thousand of those passages for another project. </p> <p>Here&#39;s a link to the text database: <a href=\"https://data.typeracer.com/pit/texts\">https://data.typeracer.com/pit/texts</a></p> <p>And here&#39;s a link to the practice page, where it shows a single passage at a time: <a href=\"https://play.typeracer.com/\">https://play.typeracer.com/</a></p> <p>I&#39;m also interested in the passage source data (e.g. title of book and author of book or title of movie and director of movie). </p> <p>Also, I&#39;m a total noob with python so I&#39;ll need a very ELI5 description", "id": 1125309, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fea1wc/help_with_webscraping_data_from_typeracer", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "Help with webscraping data from TypeRacer", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T10:10:34+00:00", "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1fe6k9v/reddit_automation_login_bot/\"> <img src=\"https://external-preview.redd.it/1y5VY6B4lDa2we2IBzUNLjESMZr15g3jUmT0HvMraFI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdf77e9ae4d0f09c32c389ca9d6f4f2e9efdc908\" alt=\"Reddit Automation Login Bot\" title=\"Reddit Automation Login Bot\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thro-away145\"> /u/thro-away145 </a> <br/> <span><a href=\"https://youtu.be/S2KhAGGpcqQ?si=E9cm3TBMFAOxhIir\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1fe6k9v/reddit_automation_login_bot/\">[comments]</a></span> </td></tr></table>", "id": 1124113, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fe6k9v/reddit_automation_login_bot", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 86, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": "https://external-preview.redd.it/1y5VY6B4lDa2we2IBzUNLjESMZr15g3jUmT0HvMraFI.jpg?width=320&crop=smart&auto=webp&s=bdf77e9ae4d0f09c32c389ca9d6f4f2e9efdc908", "title": "Reddit Automation Login Bot", "user": null, "vote": 0}, {"age": null, "album": "", "artist": null, "bookmarked": false, "comments": [], "date_published": "2024-09-11T08:50:45+00:00", "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to add 1 header to a specific request so that it looked like this: </p> <pre><code>headers = default_headers headers[&#39;csrf-token&#39;] = &#39;123&#39; yield scrapy.Request(url=url, headers=headers, callback=self.parse) </code></pre> <p>I found a class which is responsible for storing the default headers: <a href=\"https://docs.scrapy.org/en/latest/_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware\">source</a>. But I can&#39;t import _headers attribute for some reason.</p> <p>My code looks something like this: </p> <pre><code>from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware headers = DefaultHeadersMiddleware._headers </code></pre> <p>But I get: </p> <pre><code>Traceback (most recent call last): File &quot;&lt;console&gt;&quot;, line 1, in &lt;module&gt; AttributeError: type object &#39;DefaultHeadersMiddleware&#39; has no attribute &#39;_headers&#39; </code></pre> </div><!-- SC_O", "id": 1125310, "language": null, "link": "https://www.reddit.com/r/webscraping/comments/1fe5h03/how_can_i_modify_scrapy_default_headers_inside_of", "manual_status_code": 0, "page_rating": 27, "page_rating_contents": 85, "page_rating_visits": 0, "page_rating_votes": 0, "permanent": false, "source": "https://www.reddit.com/r/webscraping/.rss", "source_obj__id": 467, "status_code": 0, "tags": [], "thumbnail": null, "title": "How can I modify scrapy default_headers inside of a scrapy.request method?", "user": null, "vote": 0}]